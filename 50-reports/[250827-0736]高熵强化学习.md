---
title: 高熵强化学习
created: 2025-09-07 17:02
updated: 2025-09-07
origin: week-35
type: report
status: draft
tags: []
links: []
---


## Raw Notes

下面是一份围绕“三篇论文 + 相关工作”的精炼调研报告，重点交代**token 熵的精确定义与实现细节**，并将各路线的目标函数、训练管线与优缺点对照起来，便于直接落地到你的强化学习后训练流程中。

# 一、核心概念：token 熵是什么、为何要在 RL 后训练中关注它

**token 级生成熵**（generation entropy）刻画模型在**当前位置**对下一个词元的犹豫程度。对输入 $q$ 与已生成前缀 $o_{<t}$，当前策略 $\pi_\theta$ 的词表分布为

$$
p_t = \pi_\theta(\cdot \mid q, o_{<t})=\mathrm{Softmax}(z_t),
$$

其**token 熵**定义为

$$
H_t \;=\; -\sum_{j=1}^{V} p_{t,j}\,\log p_{t,j}.
$$

该定义直接来自论文(等式(1))，强调“**熵属于位置 $t$ 的分布**而不是被采样出来的具体 token 本身”。实现上就是对当前前向得到的 logits $z_t$ 做一次 softmax + 向量级的 $-(p\cdot\log p)$ 规约。([arXiv][1])

关注 token 熵的动机：在 CoT 推理中，**大多数 token 是低熵的“续写/拼写”**，而**少数高熵 token 是“分岔点/承上启下”**，决定思路转折与步骤衔接。RL 的收益几乎都发生在这些“高熵分岔点”上。([arXiv][1])

# 二、三篇论文的要点与它们“怎样用到熵”

## 1) 《Beyond the 80/20 Rule: High-Entropy Minority Tokens…》(高熵少数派)

**思想**：只在**高熵 token**处更新策略梯度，**屏蔽底部 80% 低熵 token** 的梯度——“用 20% token 训练也不掉点，甚至更好”。
**目标函数**（DAPO 框架下的改动，批内 Top-ρ 选取）：
对一个训练批 $B$，求最大化

$$
J^{\text{HighEnt}}_B(\theta)
=\mathbb{E}\!\left[ \frac{1}{\sum_i |o_i|}\sum_{i}\sum_{t=1}^{|o_i|}
\mathbf{1}[H^i_t \ge \tau^{B}_\rho]\cdot 
\min\!\big(r^i_t(\theta)\hat A^i_t,\,\mathrm{clip}(r^i_t(\theta), 1-\epsilon_{\text{low}}, 1+\epsilon_{\text{high}})\hat A^i_t\big)
\right],
$$

其中 $\tau^{B}_\rho$ 是**在该（微）批所有 token 的熵上取 Top-ρ 分位的阈值**，仅保留满足 $H^i_t \ge \tau^{B}_\rho$ 的 token 进入损失与反传；$\epsilon_{\text{high}}$ 采用 **Clip-Higher**（上界放宽，如 0.28）以鼓励探索。实现上只需在构造优势时加一层 **indicator mask**。文中常用 $\rho=20\%$。该做法在 Qwen3-32B/14B 上显著提升，在 8B 上持平。([arXiv][1])

**关键实现细节（必须点）**

* **熵的计算**：用**当前策略**的前向 logits 计算 $H_t$，不需要温度缩放（默认 $T=1$）。（见等式(1)与 5.1 节）([arXiv][1])
* **分位阈值**：在\*\*（微）批维度\*\*上把所有 token 的 $H_t$ 拼起来求分位数 $\tau^{B}_\rho$，得到布尔 mask（True 表示 Top-ρ 的高熵 token）。([arXiv][1])
* **只改动 PG 分量**：把 $\mathbf{1}[\cdot]$ 乘到优势上即可，其余（如 clip-higher、动态采样、overlong 奖励等）与 DAPO 配方一致；**不引入 KL 或额外 entropy bonus**。([arXiv][1])
* **经验观察**：$\rho$ 在 10–50% 区间相对鲁棒，但用 100% 会恶化（因为把大量低熵续写 token 也纳入 PG，等价于“稀释”了有效学习信号）。([arXiv][1])

## 2) 《Reasoning with Exploration: An Entropy Perspective》

**思想**：不是“掩码掉谁”，而是**把熵直接注入优势**，鼓励处于高不确定处的**更深/更长**探索；与一般“熵正则（增大不确定性）”不同，本工作通过**优势塑形**来促使模型在关键处更有把握。
**做法（“一行代码”）**：
计算常规优势 $\text{adv}$（PPO 或 GRPO），再加一项**截断且** **detached** 的熵项：

$$
\tilde{\text{adv}}_t
= \text{adv}_t \;+\; \min\!\big(\alpha\cdot H_t^{\text{detach}},\, |\text{adv}_t|/\kappa\big),
$$

再用 $\tilde{\text{adv}}$ 进入标准 PPO/GRPO 的 clipped-surrogate；这样**不改变梯度方向**（熵项不反传），只是放大高熵位置的步长，并且用 $\kappa$ 防止过度放大/翻转符号。论文明确给出 PyTorch 伪“一行”插入点（veRL 框架的 dp\_actor）。([ar5iv][2])

**关键实现细节（必须点）**

* **$\alpha,\kappa$ 两个超参**：$\alpha$ 控制熵项强度；$\kappa$ 用于截断，保证 $\alpha H_t \le |\text{adv}_t|/\kappa$ 时不会反向或主导更新。([ar5iv][2])
* **为何要 detach**：避免把“增大熵”作为优化目标；这里是**用熵做路标**而非 regularizer。随着训练置信度提升，$H_t$ 下降，熵加成会**自衰减**，从而避免后期过探索。([ar5iv][2])
* **与 GRPO/PPO 的兼容**：仅替换优势，剩下的剪切比、KL（若有）等策略不变；理论与实现都保持最小侵入。([ar5iv][2])

## 3) 《FR3E: First Return, Entropy-Eliciting Explore》

**思想**：把高熵位置当作**锚点**，按“**先回到正确轨迹（First Return）—再从高熵处**做**定向展开**（Explore）”的两阶段结构化探索，构造**局部中间反馈**，改进 credit assignment 与探索稳定性。([arXiv][3])

**做法**：

1. 用当前策略生成**基准轨迹**，沿途计算 $H_t$；选择**全局 Top-K 高熵位置**作为**分块边界**，形成“语义块/中间状态”。
2. 从这些锚点做**局部 rollouts**，用可验证的结果给出**经验价值** $V(\text{prefix})$，再配以**自适应优势调制系数** $\gamma\big(\Delta V\big)$ 来稳住学习（进步小则放大、进步大则收敛时缩小）。([arXiv][3])

**关键实现细节（必须点）**

* **熵的用途**：只用于**定位高不确定决策点**（Top-K）及**分块**；损失里不直接加熵项。等式中给出分布 $p_t$ 与熵 $H_t$ 的标准定义与**Top-K 选点**规则。([arXiv][3])
* **计算开销**：相较普通 RLVR，多了**从若干锚点起的局部扩展**与评估；但这些扩展是“定向”的，采样效率高于盲目的全局探索。([arXiv][3])

# 三、与三篇论文相关的其它路线（聚焦“熵/探索/credit assignment/配方”）

* **Clip-Higher 与 DAPO 配方**：上界放宽（如 $1+\epsilon_{\text{high}}=1.28$）能在不破坏稳定性的前提下**更敢于把低概率“探索 token”抬起来**，是近来 RLVR 成功的关键配方之一（本文 1) 也沿用）。DAPO 公开报告详述了动机与实现。([arXiv][4])
* **GRPO 及其实现要点**：不训练 critic，用同一问题的多样本平均奖励作基线，token-level PPO 损失+可选 KL；veRL 文档提供了工程接口。([Verl][5])
* **熵可控的 DPO（H-DPO）**：在**偏好优化**（无显式 RL）里，通过修改 DPO 的**反向 KL 正则**来**控制策略熵/锐度**，与上面“在 RL 中用熵”形成互补。实现改动仅在损失计算处，实验显示对数学任务的 pass\@k 有提升。([arXiv][6], [ar5iv][7])
* **ETPO（Entropy-Regularized Token-level Policy Optimization）**：把**软 Bellman**与**策略更新**都下沉到**token 粒度**，从理论到算法完整地把“熵正则”引入到 token 级 RL。适合交互式环境（如代码代理），而非典型的离线 RLVR，但在“**token 级 credit assignment + 熵**”上与三篇论文一脉相承。([arXiv][8])
* **过程奖励与中间反馈**（与 FR3E 目标相近）：VinePPO（无价值网络、MC 估计中间价值）、PRIME（只用结果标签学**隐式过程奖励**，得到稠密过程信号）、S-GRPO（串行组+衰减奖励，引导“更早更好”的思考退出），都是**加强中间监督/credit assignment**的代表，与“熵选点/结构化探索”可以组合。([arXiv][9])
* **拒绝采样基线（RAFT / Reinforce-Rej）**：近期工作显示“仅用正样本做再训练”的简单基线已能逼近乃至超过部分 RL 配方，提示我们应谨慎评估熵驱动探索的**相对收益**。([arXiv][10])

# 四、token 熵——工程侧“怎么做才对”

> 下面把“算 $H_t$”“怎么选 Top-ρ/Top-K”“放到损失/优势里”的实现细节一次讲清。

## A. 计算 $H_t$：一行向量规约

* **前向**得到 logits $z_t$（你本来为计算 log-prob/比值 $r_t$ 就会算）。
* **Softmax** 得 $p_t$，常规训练温度 $T=1$（除非你显式做温度缩放）。
* **规约**：`H_t = -(p_t * (p_t.log())).sum(dim=-1)`；注意对 **padding/EOS** 做 mask，不把它们纳入统计/更新。该定义与三篇论文一致（见 1) 的式(1)，2) 的式(4)，3) 的式(5)）。([arXiv][1], [ar5iv][2])

## B. Top-ρ（批内阈值）/Top-K（序列级）选点

* **批内 Top-ρ（高熵少数派）**：把**一个（微）批**中所有 token 的 $H_t$ 拉平成一维，做 `quantile(1-ρ)` 得 $\tau^B_\rho$，得到布尔 mask：`mask = (H >= tau)；adv *= mask`。论文明确写作 $\tau^B_\rho$，是**在批级别**求阈值而非逐序列。([arXiv][1])
* **序列级 Top-K（FR3E）**：对**单条轨迹**取全局 Top-K 的位置（“高不确定决策点”）作为**锚点/分块边界**，后续仅在这些锚点上做局部 rollouts。([arXiv][3])

## C. 把熵“接”进优化

* **“掩码式”接法（高熵少数派）**：在 token-level PPO/DAPO 损失里，把优势乘上 `mask`，实现“**只在高熵处学**”。**其余配方不变**（clip-higher/采样/overlong 奖励等保持一致；论文实验中不使用 KL/entropy bonus）。([arXiv][1])
* **“优势塑形”接法（熵视角）**：

  $$
  \tilde{\text{adv}}=\text{adv}+\min(\alpha\cdot H^{\text{detach}},\, |\text{adv}|/\kappa),
  $$

  其中 `H.detach()` **不回传梯度**，只改变步长大小与优先级；`min` 截断确保不会翻转 $\text{adv}$ 的符号。对应 veRL 的实现点就是**在计算完 adv 后加一行**再入损失。([ar5iv][2])
* **“结构化探索”接法（FR3E）**：熵只用于**定位锚点**；真正进入损失的是锚点处展开得到的**经验价值/优势**，配上**自适应缩放因子**控制学习强度，进而稳定训练而不过度早收敛。([arXiv][3])

## D. 与常见正则/配方的交互

* **不要把“熵优势塑形”与“显式熵正则”混为一谈**：前者是**不回传**的路标信号，后者会直接驱动“变得更不确定”。论文指出后者在 CoT 中会**伤害低熵多数 token 的确定性**，不如 clip-higher 这类“更关注高熵少数”的做法。([arXiv][1])
* **与 KL 正则**：1) 中实验不加 KL 亦可稳定；2) 若保留 KL，建议先只引入**优势塑形**或**Top-ρ 掩码**中的一个，逐步网格 $\alpha,\kappa,\rho$；3) 与 clip-higher 的配合通常更自然（高比值 token 往往也是高熵 token）。([arXiv][1])

# 五、对比与实践建议

| 路线               | 用熵做什么                  | 代价           | 何时更合适                         | 主要风险/调参点                                         |
| ---------------- | ---------------------- | ------------ | ----------------------------- | ------------------------------------------------ |
| **高熵少数派（Top-ρ）** | 只在高熵 token 反传          | 极低（仅一层 mask） | 你已用 DAPO/GRPO，想**减噪提效**       | ρ 过大→“稀释”；过小→样本不足。建议 10–30% 起试。([arXiv][1])      |
| **熵优势塑形**        | 放大高熵处的优势（detach + 截断）  | 极低（一行）       | 想保留**全部 token**训练又突出“分岔点”     | $\alpha,\kappa$ 需网格；$\kappa$ 太小会过放大。([ar5iv][2]) |
| **FR3E**         | 高熵定位锚点 + 定向 rollouts   | 中等（局部展开）     | 更关注**中间反馈/credit assignment** | K 过大开销上升；需要设计锚点数量与展开步数。([arXiv][3])              |
| **H-DPO**        | 在 DPO 中直接控熵/锐度         | 低（改损失）       | 偏好优化场景或想少调 RL 环节              | 与 RLVR 不同范式；正则强度需小心。([ar5iv][7])                 |
| **ETPO**         | token-级软 Bellman + 熵正则 | 较高（算法更重）     | 交互式/在线环境                      | 工程复杂、与 RLVR 评测口径不同。([arXiv][8])                  |

# 六、最小可落地清单（工程角度）

1. **度量熵**：在生成/训练前向里，取 `p = softmax(logits)`，`H = -(p * log(p)).sum(-1)`；屏蔽 PAD/EOS。三篇论文都使用该实现。([arXiv][1], [ar5iv][2])
2. **批内 Top-ρ 掩码**（若采用高熵少数派）：把批内所有 token 的 `H` 拼起来做 `quantile` 得阈值 $\tau^B_\rho$，生成 `mask` 乘到优势或损失上。ρ=0.2 常作为强基线。([arXiv][1])
3. **优势塑形“一行”**（若采用熵视角）：`adv += min(alpha * H.detach(), adv.abs()/kappa)`；从 $\alpha\in[0.05,0.2]$、$\kappa\in[2,8]$ 小步扫描。([ar5iv][2])
4. **结构化探索（FR3E）**：对每条样本**先**生成一条“基准正确轨迹”（或较优轨迹），按 Top-K $H_t$ 定锚、分块；再从各锚点做若干次**局部** rollouts 评估并计算经验价值，最后以自适应优势调制进入策略更新。([arXiv][3])
5. **与配方的搭配**：Clip-Higher（如 $\epsilon_{\text{high}}\approx0.28$）+ token-level PPO/GRPO 是当前社区默认强基线；在此之上引入 **(2) 或 (3)** 的熵机制，通常更稳。([arXiv][4], [Verl][5])

---

# 参考与延伸阅读（精选）

* **高熵少数派（Top-ρ 掩码）**：熵定义、Top-ρ 阈值与目标式(6)，及“不加 KL/entropy bonus”的配方与结果。([arXiv][1])
* **熵优势塑形**：熵定义(式(4))、优势塑形(式(5)(6))、“一行实现”与理论动机。([ar5iv][2])
* **FR3E**：熵计算与 Top-K 选点、分块/中间状态构造、锚点局部 rollouts 与自适应优势调制。([arXiv][3])
* **Clip-Higher / DAPO**：为何放宽上剪切能鼓励探索 token；DAPO 全配方与公开报告。([arXiv][4])
* **GRPO 实现简介**（veRL 文档）：组采样、群内相对优势与无 critic 策略。([Verl][5])
* **H-DPO**：在偏好优化中显式**控制策略熵**（非 RLVR）。([arXiv][6], [ar5iv][7])
* **ETPO**：token-级软 Bellman + 熵正则的理论-实践框架。([arXiv][8])
* **VinePPO / PRIME / S-GRPO**：改进 credit assignment 与中间奖励的代表工作，可与“熵锚点/Top-ρ/优势塑形”互补。([arXiv][9])
* **拒绝采样基线（RAFT / Reinforce-Rej）**：提醒评测要对齐与做充足消融。([arXiv][10])

> 如果你要把这些机制接到现有 veRL/GRPO++ 训练脚本里：
> **Top-ρ** 属于“优势前的掩码”；**熵优势塑形**属于“计算完优势后一行相加（detach+截断）”；**FR3E** 需在采样器里加入“按高熵锚点做局部展开与评估”的流程，随后把得到的“锚点价值/优势”写回策略更新。以上三者均不要求你改动模型架构，只是调整**优势/采样/反传范围**与**采样策略**。

[1]: https://arxiv.org/pdf/2506.01939 "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning"
[2]: https://www.ar5iv.org/pdf/2506.14758v2 "[2506.14758] Reasoning with Exploration: An Entropy Perspective"
[3]: https://arxiv.org/html/2507.07017v1 "First Return, Entropy-Eliciting Explore"
[4]: https://arxiv.org/pdf/2503.14476?utm_source=chatgpt.com "DAPO: An Open-Source LLM Reinforcement Learning ..."
[5]: https://verl.readthedocs.io/en/latest/algo/grpo.html?utm_source=chatgpt.com "Group Relative Policy Optimization (GRPO) - verl documentation"
[6]: https://arxiv.org/abs/2411.07595 "[2411.07595] Entropy Controllable Direct Preference Optimization"
[7]: https://ar5iv.org/pdf/2411.07595 "[2411.07595] Entropy Controllable Direct Preference Optimization"
[8]: https://arxiv.org/html/2402.06700v1 "Entropy-Regularized Token-Level Policy Optimization for Large Language Models"
[9]: https://arxiv.org/html/2410.01679v2?utm_source=chatgpt.com "VinePPO: Refining Credit Assignment in RL Training of LLMs"
[10]: https://arxiv.org/html/2504.11343v1?utm_source=chatgpt.com "A Minimalist Approach to LLM Reasoning: from Rejection ..."
