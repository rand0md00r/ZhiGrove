---
title: 惊讶度
created: 2025-09-07 17:02
updated: 2025-09-07
origin: week-35
type: knowledge
status: draft
tags: []
links: []
---

## TL;DR（≤3点）
- 定义：**惊讶度 / 自信息** $I(x)=-\log p(x)$（底 2→bit，底 $e$→nat）。
- 关系：**熵 = 平均惊讶度**；NLL 就是“真实样本”的惊讶度；交叉熵是用真分布对模型惊讶度求平均。
- 性质：越稀有越“惊”，**可加性**（独立事件相加），序列总惊讶度 = 各 token 惊讶度之和。


## What（是什么）
- **定义**：$I(x)=-\log p(x)$。概率越小，$I$ 越大。
- **单位**：  
  - bit：$I(x)=-\log_2 p(x)$，可解释为“理想前缀码所需的比特数”。  
  - nat：$I(x)=-\ln p(x)$；换算：$1\text{ nat}= \log_2 e \approx 1.4427\text{ bit}$。
- **加性**：独立事件 $x,y$，$I(x,y)=I(x)+I(y)$。


## Why（为什么这么做/何时使用）
- **建模直觉**：它把“罕见性”变成可加的度量，便于**求和、求期望、做梯度优化**。
- **训练指标**：语言建模里，**NLL** 就是对真实 token 的惊讶度；**平均 NLL** 就是**交叉熵**，最小化它等价于最大似然。
- **分析用**：定位**高惊讶度 token**（模型最不确定/最关键的分岔点），做对齐、采样控制或误差分析。  


## How（最小复现配方，≤5步）
1. 给定某位置 $t$ 的 logits $z_t\in\mathbb{R}^V$ 与温度 $T>0$（$\beta=1/T$），得策略分布  
   $p_t(v)=\dfrac{e^{\beta z_{t,v}}}{\sum_j e^{\beta z_{t,j}}}$。
2. 对真实/采样到的 token $y_t$，其**惊讶度**为  
   $I_t(y_t)=-\log p_t(y_t)$（选定对数底）。
3. 序列总惊讶度（链式法则）：$I(y_{1:L})=\sum_{t=1}^L I_t(y_t)$。
4. **平均惊讶度**（=熵）：$H_t=\mathbb{E}_{v\sim p_t}[I_t(v)]=-\sum_v p_t(v)\log p_t(v)$。
5. 数据/批次上取平均：得到**平均 NLL / 交叉熵**；若用 nat，**困惑度**$=\exp(H)$；若用 bit，困惑度 $=2^H$。


## Gotchas（坑点与边界）
- **底数与单位**：别混用 bit 和 nat；换算需乘 $\log_2 e$ 或 $\ln 2$。
- **$p=0$ 的无穷大**：$I=-\log 0=+\infty$；实际需 **加平滑/剪裁**（如 $p\leftarrow\max(p,\varepsilon)$）。
- **训练 vs 推理分布**：教师强制下的 $p_t$ 与 top-$k$/top-$p$/温度重采样后的分布不同；你报告的惊讶度需**说明计算口径**。
- **平均 vs 个体**：熵（平均惊讶度）是**分布属性**；$-\log p(y_t)$ 是**样本属性**。混淆二者会误读指标。
- **尺度直觉**：概率每 **×1/2**，惊讶度 **+1 bit**；极低概率 token 的 $I$ 会主导序列总值。



## Raw Notes

下面把“惊讶度（surprisal / self-information）”说得又直白又准：

## 它是什么

* 一句话：**某件事有多罕见，就有多“惊”**。
* 数学定义：$\text{惊讶度} = I(x) = -\log p(x)$
  概率 $p(x)$ 越小，$I(x)$ 越大；常用底数：

  * 以 2 为底（$\log_2$）：单位是 **bit**（信息位）
  * 以 e 为底（$\ln$）：单位是 **nat**

## 为什么用 “$-\log p$” 表示

* 直觉1（稀有 = 更有料）：小概率事件带来更多“新信息”，所以数值更大。
* 直觉2（可加性）：独立事件 $x,y$ 同时发生的惊讶度

  $$
  I(x,y)=-\log p(x,y)=-\log p(x)-\log p(y)=I(x)+I(y)
  $$

  ——“连续两次巧合”比一次更震惊，数值直接**相加**，很顺手。

## 一个小表（$\log_2$ 单位：bit）

| 概率 $p$ | 惊讶度 $I=-\log_2 p$ | 直觉       |
| ------ | ----------------: | -------- |
| 1（必然）  |             0 bit | 一点不惊讶    |
| 1/2    |             1 bit | 抛硬币出正面   |
| 1/4    |             2 bit | 连续两次正面   |
| 1/8    |             3 bit | 连续三次正面   |
| 1/100  |         ≈6.64 bit | 百分之一的巧合  |
| 1/1000 |         ≈9.97 bit | 千分之一的大巧合 |

> 记忆法：**概率每×1/2，惊讶度 +1 bit**。

## 和“熵”的关系

* **熵 = 平均惊讶度**

  $$
  H = \mathbb{E}_{x\sim p}[-\log p(x)] = \sum_x p(x)\,I(x)
  $$

  ——一个分布整体“有多不确定”，就是把它每个结果的惊讶度按其概率**求平均**。

## 和 NLL/交叉熵的关系

* 训练里常见的 **负对数似然（NLL）** 就是对“真实发生的那个结果”的惊讶度：$-\log p(y)$。
* **交叉熵**是用“真实分布”对**模型分布的惊讶度**做平均。

## 语言模型里的直觉

* “The cat sat on the \_\_\_”

  * “mat”的模型概率高 → **惊讶度低**（很顺）
  * “unicorn”的概率低 → **惊讶度高**（很怪）
* 在一段生成里：**高惊讶度的 token**=模型最犹豫、最关键的分岔点；**低惊讶度的 token**=模板化续写。

## 单位怎么理解（bit 的直观）

* $I(x)=k$ bit 可以理解为：“平均需要 **k 个是/否** 问题才能把这个结果区分出来”。
  例：$p=1/8\Rightarrow I=3$ bit——需要 3 次二分才能定位到它。

---

**一句话总结**：
惊讶度 $I(x)=-\log p(x)$ 是“**稀有度尺子**”：越少见，数值越大；它能相加，平均后就是熵。用它，我们能精确刻画“这一步到底有多让模型犯难”。
