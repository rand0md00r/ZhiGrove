08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.0.block.0.norm2.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.0.block.0.norm2.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.0.block.0.conv2.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.0.block.0.conv2.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.0.block.0.nin_shortcut.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.0.block.0.nin_shortcut.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.0.block.1.norm1.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.0.block.1.norm1.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.0.block.1.conv1.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.0.block.1.conv1.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.0.block.1.norm2.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.0.block.1.norm2.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.0.block.1.conv2.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.0.block.1.conv2.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.0.block.2.norm1.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.0.block.2.norm1.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.0.block.2.conv1.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.0.block.2.conv1.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.0.block.2.norm2.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.0.block.2.norm2.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.0.block.2.conv2.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.0.block.2.conv2.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.0.norm1.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.0.norm1.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.0.conv1.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.0.conv1.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.0.norm2.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.0.norm2.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.0.conv2.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.0.conv2.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.0.nin_shortcut.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.0.nin_shortcut.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.1.norm1.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.1.norm1.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.1.conv1.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.1.conv1.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.1.norm2.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.1.norm2.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.1.conv2.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.1.conv2.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.2.norm1.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.2.norm1.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.2.conv1.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.2.conv1.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.2.norm2.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.2.norm2.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.2.conv2.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.block.2.conv2.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.upsample.conv.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.1.upsample.conv.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.block.0.norm1.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.block.0.norm1.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.block.0.conv1.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.block.0.conv1.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.block.0.norm2.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.block.0.norm2.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.block.0.conv2.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.block.0.conv2.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.block.1.norm1.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.block.1.norm1.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.block.1.conv1.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.block.1.conv1.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.block.1.norm2.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.block.1.norm2.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.block.1.conv2.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.block.1.conv2.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.block.2.norm1.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.block.2.norm1.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.block.2.conv1.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.block.2.conv1.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.block.2.norm2.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.block.2.norm2.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.block.2.conv2.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.block.2.conv2.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.upsample.conv.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.2.upsample.conv.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.block.0.norm1.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.block.0.norm1.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.block.0.conv1.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.block.0.conv1.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.block.0.norm2.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.block.0.norm2.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.block.0.conv2.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.block.0.conv2.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.block.1.norm1.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.block.1.norm1.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.block.1.conv1.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.block.1.conv1.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.block.1.norm2.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.block.1.norm2.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.block.1.conv2.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.block.1.conv2.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.block.2.norm1.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.block.2.norm1.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.block.2.conv1.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.block.2.conv1.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.block.2.norm2.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.block.2.norm2.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.block.2.conv2.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.block.2.conv2.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.upsample.conv.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.up.3.upsample.conv.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.norm_out.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.norm_out.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.conv_out.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.decoder.conv_out.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.quant_conv.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.quant_conv.bias is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.post_quant_conv.weight is skipped since its requires_grad=False
08/12 10:15:32 - mmengine - WARNING - autoencoder.post_quant_conv.bias is skipped since its requires_grad=False
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/site-packages/mmengine/config/config.py", line 109, in __getattr__
    value = super().__getattr__(name)
  File "/usr/local/lib/python3.10/site-packages/addict/addict.py", line 67, in __getattr__
    return self.__getitem__(item)
  File "/usr/local/lib/python3.10/site-packages/mmengine/config/config.py", line 138, in __getitem__
    return self.build_lazy(super().__getitem__(key))
  File "/usr/local/lib/python3.10/site-packages/mmengine/config/config.py", line 105, in __missing__
    raise KeyError(name)
KeyError: 'requires_grad'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/usr/local/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/root/.vscode-server/extensions/ms-python.debugpy-2025.10.0/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py", line 71, in <module>
    cli.main()
  File "/root/.vscode-server/extensions/ms-python.debugpy-2025.10.0/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py", line 501, in main
    run()
  File "/root/.vscode-server/extensions/ms-python.debugpy-2025.10.0/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py", line 351, in run_file
    runpy.run_path(target, run_name="__main__")
  File "/root/.vscode-server/extensions/ms-python.debugpy-2025.10.0/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 310, in run_path
    return _run_module_code(code, init_globals, run_name, pkg_name=pkg_name, script_name=fname)
  File "/root/.vscode-server/extensions/ms-python.debugpy-2025.10.0/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 127, in _run_module_code
    _run_code(code, mod_globals, init_globals, mod_name, mod_spec, pkg_name, script_name)
  File "/root/.vscode-server/extensions/ms-python.debugpy-2025.10.0/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py", line 118, in _run_code
    exec(code, run_globals)
  File "/vepfs/DI/yaqi/understand_gen/CrossUni-do/scripts/train.py", line 357, in <module>
    main()
  File "/vepfs/DI/yaqi/understand_gen/CrossUni-do/scripts/train.py", line 353, in main
    runner.train()
  File "/usr/local/lib/python3.10/site-packages/mmengine/runner/_flexible_runner.py", line 1182, in train
    self.strategy.prepare(
  File "/usr/local/lib/python3.10/site-packages/mmengine/_strategy/deepspeed.py", line 385, in prepare
    self.optim_wrapper = self.build_optim_wrapper(optim_wrapper, model)
  File "/usr/local/lib/python3.10/site-packages/mmengine/_strategy/base.py", line 485, in build_optim_wrapper
    return build_optim_wrapper(model, optim_wrapper)
  File "/usr/local/lib/python3.10/site-packages/mmengine/optim/optimizer/builder.py", line 214, in build_optim_wrapper
    optim_wrapper = optim_wrapper_constructor(model)
  File "/usr/local/lib/python3.10/site-packages/mmengine/optim/optimizer/default_constructor.py", line 318, in __call__
    optimizer = OPTIMIZERS.build(optimizer_cfg)
  File "/usr/local/lib/python3.10/site-packages/mmengine/registry/registry.py", line 570, in build
    return self.build_func(cfg, *args, **kwargs, registry=self)
  File "/usr/local/lib/python3.10/site-packages/mmengine/registry/build_functions.py", line 247, in build_optimizer_from_cfg
    return build_from_cfg(cfg, registry, default_args)
  File "/usr/local/lib/python3.10/site-packages/mmengine/registry/build_functions.py", line 121, in build_from_cfg
    obj = obj_cls(**args)  # type: ignore
  File "/vepfs/DI/yaqi/understand_gen/CrossUni-do/src/optimisers/custom_adamw.py", line 10, in __init__
    params = [p for p in params if p.requires_grad]
  File "/vepfs/DI/yaqi/understand_gen/CrossUni-do/src/optimisers/custom_adamw.py", line 10, in <listcomp>
    params = [p for p in params if p.requires_grad]
  File "/usr/local/lib/python3.10/site-packages/mmengine/config/config.py", line 113, in __getattr__
    raise AttributeError(f"'{self.__class__.__name__}' object has no "
AttributeError: 'ConfigDict' object has no attribute 'requires_grad'



from torch.optim import AdamW


class CustomAdamW(AdamW):
    def __init__(self, params, weight_decay, *args, **kwargs):
        # import pdb; pdb.set_trace()
        if isinstance(params, dict):
            params = [p for p in params.values() if p.requires_grad]
        else:
            params = [p for p in params if p.requires_grad]

        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.
        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.
        decay_params = [p for p in params if p.dim() >= 2]
        nodecay_params = [p for p in params if p.dim() < 2]
        optim_groups = [
            {'params': decay_params, 'weight_decay': weight_decay},
            {'params': nodecay_params, 'weight_decay': 0.0}
        ]
        num_decay_params = sum(p.numel() for p in decay_params)
        num_nodecay_params = sum(p.numel() for p in nodecay_params)
        print(f"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters")
        print(f"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters")
        # Create AdamW optimizer and use the fused version if it is available
        # fused_available = 'fused' in inspect.signature(AdamW).parameters
        # extra_args = dict(fused=True) if fused_available else dict()
        # print(f"using fused AdamW: {fused_available}")

        # kwargs.update(extra_args)

        super().__init__(params=optim_groups, *args, **kwargs)


class ParamWiseAdamW(AdamW):
    def __init__(self, params, *args, **kwargs):
        assert isinstance(params, list)
        for param in params:
            assert isinstance(param, dict)
            assert isinstance(param['params'], list)
            assert len(param['params']) == 1

            if param['params'][0].ndim == 1:
                param['weight_decay'] = 0.0

        super().__init__(params=params, *args, **kwargs)



产生了一些报错，代码中子自定义的优化器类如上，帮我解决一下
