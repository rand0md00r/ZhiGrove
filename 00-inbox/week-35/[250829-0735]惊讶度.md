下面把“惊讶度（surprisal / self-information）”说得又直白又准：

## 它是什么

* 一句话：**某件事有多罕见，就有多“惊”**。
* 数学定义：$\text{惊讶度} = I(x) = -\log p(x)$
  概率 $p(x)$ 越小，$I(x)$ 越大；常用底数：

  * 以 2 为底（$\log_2$）：单位是 **bit**（信息位）
  * 以 e 为底（$\ln$）：单位是 **nat**

## 为什么用 “$-\log p$” 表示

* 直觉1（稀有 = 更有料）：小概率事件带来更多“新信息”，所以数值更大。
* 直觉2（可加性）：独立事件 $x,y$ 同时发生的惊讶度

  $$
  I(x,y)=-\log p(x,y)=-\log p(x)-\log p(y)=I(x)+I(y)
  $$

  ——“连续两次巧合”比一次更震惊，数值直接**相加**，很顺手。

## 一个小表（$\log_2$ 单位：bit）

| 概率 $p$ | 惊讶度 $I=-\log_2 p$ | 直觉       |
| ------ | ----------------: | -------- |
| 1（必然）  |             0 bit | 一点不惊讶    |
| 1/2    |             1 bit | 抛硬币出正面   |
| 1/4    |             2 bit | 连续两次正面   |
| 1/8    |             3 bit | 连续三次正面   |
| 1/100  |         ≈6.64 bit | 百分之一的巧合  |
| 1/1000 |         ≈9.97 bit | 千分之一的大巧合 |

> 记忆法：**概率每×1/2，惊讶度 +1 bit**。

## 和“熵”的关系

* **熵 = 平均惊讶度**

  $$
  H = \mathbb{E}_{x\sim p}[-\log p(x)] = \sum_x p(x)\,I(x)
  $$

  ——一个分布整体“有多不确定”，就是把它每个结果的惊讶度按其概率**求平均**。

## 和 NLL/交叉熵的关系

* 训练里常见的 **负对数似然（NLL）** 就是对“真实发生的那个结果”的惊讶度：$-\log p(y)$。
* **交叉熵**是用“真实分布”对**模型分布的惊讶度**做平均。

## 语言模型里的直觉

* “The cat sat on the \_\_\_”

  * “mat”的模型概率高 → **惊讶度低**（很顺）
  * “unicorn”的概率低 → **惊讶度高**（很怪）
* 在一段生成里：**高惊讶度的 token**=模型最犹豫、最关键的分岔点；**低惊讶度的 token**=模板化续写。

## 单位怎么理解（bit 的直观）

* $I(x)=k$ bit 可以理解为：“平均需要 **k 个是/否** 问题才能把这个结果区分出来”。
  例：$p=1/8\Rightarrow I=3$ bit——需要 3 次二分才能定位到它。

---

**一句话总结**：
惊讶度 $I(x)=-\log p(x)$ 是“**稀有度尺子**”：越少见，数值越大；它能相加，平均后就是熵。用它，我们能精确刻画“这一步到底有多让模型犯难”。
