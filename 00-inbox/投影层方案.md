**投影层是指 LLM 的last hidden state（MetaQuery对应的部分）**

方案一：

1. TransEncoder：

组成结构：
- N 层 Encoder layer（多头注意力层
- 3层Encoder Reduction layer（多头注意力层 + 降维线性层）
- 线性层 indim = d_model(降维后) * num_token; outdim = latten_size;
- 

2. MLP




DINO v2，v3进行监督；

使用pytorch 的平滑操作进行loss 平滑。


import torch.nn.functional as F
F.smooth_l1_loss