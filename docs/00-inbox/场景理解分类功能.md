# 1. 功能理解

1. 核心功能满足对于特定环境的不同情景的分类，例如4s店中的迎宾、主动介绍、引导、欢送等情景；
2. 精细能力是对人物的非语言信息进行提取和理解，包括表情、语气、动作；
3. 场景变更需求，能够实时变更场景（情景），实现例如中断对话、修正行为等；

# 2. 现有实现方案

1. 以开源VLM + 多system prompt + vLLM部署的方式实现了场景理解；
2. 端侧客户端以固定频率发送场景图像，

# 3. 缺陷分析

1. 丢失时序信息。
    - 动作连贯性缺失，单帧图像无法分辨连续动作信息，很难识别例如”抬手“的目的是什么；
    - 无法识别语气，没有音频模态的输入；
    - 表情细节难以捕捉，固定频率采样图像容易错过微表情变化；

2. 相应延迟与资源浪费
    - 固定采样频率在没有人的时候会产生GPU空转；
    - 延迟高，VLM推理通常需要几百ms到几秒，对于场景变更这种需要毫秒级反应的需求，难以实现；
    - 多 system prompt 无法很好利用 vLLM 的 KV cache（系统消息变动多，缓存复用差）。

3. 标签建模粗糙
    - 软标签 + 自然语言输出，没有一个稳定的、可控的离散状态空间；
    - 场景一多就会出现prompt 爆炸、不同 prompt 对同一画面的判断可能矛盾，缺乏统一仲裁逻辑

4. 场景切换逻辑缺失
    - 缺乏显式的场景状态机/事件层进行平滑过滤（例如需要连续 N 秒判断为“顾客在离开”才切换到欢送）、禁止不合理跳转；
    - 状态层缺失，容易导致模型误判、多轮对话场景不一致导致的体验割裂；

# 4. 改进方案概述

1. 端云协同 + 多模态融合 + 快慢系统
    - 音频流处理：
        - ASR（语音转文字） + SER（语音情感识别）
    - 数据融合：
        - 早融合方案：image + SER标签 + ASR文本 -> VLM -> 场景理解结果
        - 晚融合方案：VLM caption + SER 标签 + ASR 文本 -> Tiny LLM -> 场景理解结果
    - 场景状态层：建立显式的“情景状态机 + 事件触发”


2. Video-LLM/Omni-LLM
