# 今日排查日志（2025-09-19，Asia/Taipei）

## 背景
- **现象**：训练到 **26k iters**（此前 3k 等阶段也测过），推理端仍呈现“彩色纹理/油画感”，缺乏清晰语义。
- **目标**：先**锁定推理链路正确性**，再评估训练是否学到语义。

---

## 关键代码调整
- 在 `generate()` 内实现**极简欧拉采样**（均匀 `dt`，`t∈[0,1]`，`log_snr=4-8t`，先**关闭 CFG**），并加入探针输出 `[A]/[B]/[C]`。
- **统一 VAE 尺度**：依据 `FrozenAutoencoderKL`（`sample()` 内 *sf、`decode()` 内 /sf），**外部不再额外乘/除 `scale_factor`**。
- 增加调试旋钮：时间方向 `time_dir`、步长放大 `dt_scale`、FM 前**方差对齐**、以及（候选）FM 前向 **fp32**。

---

## 实验与证据

### 1) VAE 编解码冒烟（通过 ✅）
- `pixels_to_latents / latents_to_pixels` 自测：
  - `z` 统计：**range [-4.500, 5.562], mean -0.014, var 1.414**
  - 重建像素：**range [0.000, 0.992]，PSNR=29.08 dB**
- **结论**：VAE 尺度/前后处理正常。

### 2) `scale_factor` 行为确认（通过 ✅）
- 现象：`decode(z)` 清晰，`decode(z * sf)` 变糊。
- 结合实现（`decode()` 内部已 `/ sf`）→ 外部**不要再**乘/除 `sf`。
- **结论**：推理端直接 `decode(z)`。

### 3) 生成端探针（样例）
- `[A] decode(z_init)`：范围合理，但对比度偏低。
- `[B] first |Δz|_rms ≈ 2.6e-3 ~ 7.8e-3`：**速度场在动**。
- `[C] decode(z_end)`：范围 [0,1]，但画面仍“油画/纹理感”。

### 4) 文本通路有效性（通过 ✅）
- `cos(z_text)`：
  - `cos(cat, car)=0.098`
  - `cos(cat, apple)=0.114`
  - `cos(car, apple)=0.108`
- **结论**：不同 prompt 的文本嵌入可区分，文本通路正常。

### 5) FM 单步对齐（薄弱 ❌）
- 在 `t=0.10/0.37/0.63/0.90`：
  - `cos(v_pred, v_tgt)=0.050/0.045/0.036/0.015`（方向弱）
  - `||pred||/||tgt||≈0.28~0.30`（**幅值约小 3.4×**）
- **结论**：FM 速度场偏弱且幅值系统性偏小。

### 6) “对着真图射击”（x0 → x1，失败 ❌）
- 用**学到的**速度场积分：`MSE(z_end, x1)=4.531`（误差大）。

### 7) 教师场欧拉验证（通过 ✅）
- 使用 `Dt_psi` 欧拉积分（0→1）：
  - `N=100 → 1.51e-02`
  - `N=50  → 3.41e-03`
  - `N=20  → 5.84e-04`
  - `N=10  → 1.54e-04`
  - `N=5   → 5.42e-05`
- **结论**：`psi/Dt_psi` 与欧拉调度实现正确，采样调度 OK。

---

## 诊断结论
- **VAE 正常**、**调度正确**、**文本条件有效**。
- **瓶颈在 FM**：预测速度场方向弱、幅值偏小（≈0.29× 目标），导致采样“走不动”，出图只剩纹理。

---

## 临时修复（推理侧）
- 在 `generate()`：
  - **FM 前向用 fp32**（避免 bf16 压幅值）。
  - 对 `v_pred` 乘全局增益 **`fm_gain ≈ 3.5`**（由 `1/0.29` 估计）。
  - 可选扫 `dt_scale ∈ {1, 2, 3}` 和 `time_dir ∈ {0->1, 1->0}`，选最清晰者。
- 目的：**先让链路跑起来**，观察训练进度对生成质量的影响。

---

## 后续计划（建议按序）
1. **推理验证（短期）**
   - 用 `fm_gain=3.5, use_fp32_fm=True, num_steps=150, 256px, 无CFG` 出一批固定 prompt 图。
   - 再跑“射击到 x1”（同样乘 `fm_gain` + fp32），期望 `MSE` 明显小于 **4.53**。
2. **自动标定 fm_gain（可选）**
   - 用少量 (x_img, prompt) 估计 `median(||v_tgt||/||v_pred||)`，固定为部署常数。
3. **训练侧修复（中期）**
   - 提高 **FM 损失权重** 或提升 **FM 子网 LR**；
   - 加 **norm 对齐损失**：`λ * (||v_pred|| - ||v_tgt||)^2`；
   - 让 **FM 模块用 fp32** 训练（或在混精度中强制 FM 层 fp32）；
   - 继续训练并复测单步对齐/射击 MSE。

---

## 产物/记录
- VAE 回环图：`orig.png / recon_wrap.png / recon_direct.png`
- 生成探针截图与读数（`[A]/[B]/[C]`）
- 文本 cos、FM 单步对齐 cos/ratio、射击 MSE、Teacher-Euler MSE 曲线

> 当前状态：**推理链路（VAE/调度/文本）可信**，**FM 需要“打气”**。先用推理侧增益 + fp32 验证可行，再把修复回灌训练。


---
> 下面是相关debug 代码块

# 1) 文本通路是否真的在起作用

不同 prompt 的 `z_text` 如果几乎一致，后面再怎么采样都出不来语义。

```python
@torch.no_grad()
def dbg_text_cos(model, prompts=("a cat","a car","a red apple")):
    zs=[]
    for p in prompts:
        tok = model.tokenizer(p, return_tensors="pt").to(model.device)
        q_meta = model.meta_queries[None].expand(1, model.num_queries, -1).to(model.device, model.dtype)
        llm_in = model.prepare_forward_input(x=q_meta, input_ids=tok.input_ids, attention_mask=torch.ones_like(tok.input_ids))
        hs = model.llm.model(**llm_in, return_dict=True).last_hidden_state
        q_tokens = hs[:, -model.num_queries:]
        q_mask = torch.ones(q_tokens.shape[:2], device=model.device, dtype=torch.bool)
        z_text, _, _ = model.text_ve_encoder(q_tokens, q_mask)   # (1,4096)
        zs.append(z_text)
    import torch.nn.functional as F
    for i in range(len(zs)):
        for j in range(i+1,len(zs)):
            c = F.cosine_similarity(zs[i], zs[j], dim=-1).mean().item()
            print(f"[text] cos({prompts[i]}, {prompts[j]}) = {c:.3f}")
```

**判读**

* cos < 0.9：区分度还可以 ✅
* cos ≈ 1：文本几乎没进来（检查 LLM 拼接/TransEncoder/重参数化是否过强噪声）。

---

# 2) 单步对齐测试：`v_pred` vs `v_target`（和训练完全同形）

用一张真实图（与其文本）做单步“对齐度”测试，直接告诉你 **FM 模型有没有学到正确方向**。

```python
@torch.no_grad()
def dbg_fm_single_step(model, pixel_values, input_ids, attention_mask, n_t=4):
    device, dtype = model.device, model.dtype
    # x1: 图像latent（与训练一致 256×256）
    x_img = F.interpolate(pixel_values.to(device, dtype), size=(256,256), mode='bilinear', align_corners=False)
    x1 = model.pixels_to_latents(x_img)                # (B,4,32,32)
    # x0: 文本latent
    q_meta = model.meta_queries[None].expand(input_ids.size(0), model.num_queries, -1).to(device, dtype)
    llm_in = model.prepare_forward_input(x=q_meta, input_ids=input_ids.to(device), attention_mask=attention_mask.to(device))
    hs = model.llm.model(**llm_in, return_dict=True).last_hidden_state
    q_tokens = hs[:, -model.num_queries:]
    q_mask = torch.ones(q_tokens.shape[:2], device=device, dtype=torch.bool)
    z_text, _, _ = model.text_ve_encoder(q_tokens, q_mask)      # (B,4096)
    x0 = einops.rearrange(z_text, 'b (c h w)-> b c h w', c=4,h=x1.shape[-2], w=x1.shape[-1]).to(dtype)

    # 采样几个t
    import torch
    from src.models.openuni.fm_utils import psi, Dt_psi
    sigma_min, sigma_max = model.sigma_min, model.sigma_max
    B = x1.size(0)
    ts = torch.linspace(0.1, 0.9, n_t, device=device, dtype=dtype)

    for t in ts:
        t_vec = torch.full((B,), float(t), device=device, dtype=dtype)
        x_t = psi(t_vec, x0, x1, sigma_min, sigma_max).to(dtype)
        v_tgt = Dt_psi(t_vec, x0, x1, sigma_min, sigma_max)      # 目标速度
        log_snr = 4.0 - 8.0 * t_vec
        v_pred = model.fm_transformers(x_t, log_snr=log_snr, null_indicator=torch.zeros(B, dtype=torch.bool, device=device))[-1]

        # 计算对齐度（cos & 比例）
        a = v_pred.flatten(1).float(); b = v_tgt.flatten(1).float()
        cos = (F.normalize(a,dim=1) * F.normalize(b,dim=1)).sum(1).mean().item()
        ratio = (a.norm(dim=1) / (b.norm(dim=1)+1e-8)).mean().item()
        print(f"[fm] t={float(t):.2f}  cos(v_pred, v_tgt)={cos:.3f}  ||pred||/||tgt||={ratio:.3f}")
```

**判读**

* cos 明显 > 0（理想 ≥0.5）且 ratio 在 0.5\~2 之间 → 学到方向 ✅
* cos ≈ 0 或负数、ratio 极小/极大 → **FM 模型没学到**（训练不够/训练设置或 dtype/forward 有误）。

---

# 3) “对着真图射击”：从 x0 积分到 x1（教师驱动版）

看能否把同一张图从文本 latent 积到该图 latent（不是生成，只是验证 ODE/调度是否工作）。

```python
@torch.no_grad()
def dbg_shoot_to_x1(model, pixel_values, input_ids, attention_mask, num_steps=100, dt_scale=1.0):
    device, dtype = model.device, model.dtype
    x_img = F.interpolate(pixel_values.to(device, dtype), size=(256,256), mode='bilinear', align_corners=False)
    x1 = model.pixels_to_latents(x_img)  # target
    # x0
    q_meta = model.meta_queries[None].expand(input_ids.size(0), model.num_queries, -1).to(device, dtype)
    llm_in = model.prepare_forward_input(x=q_meta, input_ids=input_ids.to(device), attention_mask=attention_mask.to(device))
    hs = model.llm.model(**llm_in, return_dict=True).last_hidden_state
    q_tokens = hs[:, -model.num_queries:]; q_mask = torch.ones(q_tokens.shape[:2], device=device, dtype=torch.bool)
    z_text,_,_ = model.text_ve_encoder(q_tokens, q_mask)
    z = einops.rearrange(z_text, 'b (c h w)-> b c h w', c=4,h=x1.shape[-2], w=x1.shape[-1]).to(dtype)

    # 最简欧拉（与训练同：log_snr=4-8t，0->1）
    N=max(1,int(num_steps)); dt=(1.0/N)*float(dt_scale)
    B=z.size(0); null=torch.zeros(B, dtype=torch.bool, device=device)
    for i in range(N):
        t = torch.full((B,), (i+0.5)/N, device=device, dtype=dtype)
        log_snr = 4.0 - 8.0 * t
        v = model.fm_transformers(z, log_snr=log_snr, null_indicator=null)[-1]
        z = z + v * dt
    # 看和 x1 的距离
    err = F.mse_loss(z.float(), x1.float()).item()
    print(f"[shoot] MSE(z_end, x1) = {err:.6f}")
```

**判读**

* `MSE(z_end,x1)` 随步数或 `dt_scale` 能明显下降（哪怕到 1e-2 \~ 1e-3）→ ODE 链路 OK ✅
* 基本不降 → 还是 **FM 场学得不对/力度太小/方向错**。

# 调试结果：
``` bash
1）文本通路结果：
[text] cos(a cat, a car) = 0.098
[text] cos(a cat, a red apple) = 0.114
[text] cos(a car, a red apple) = 0.108

2）单步对齐测试：
[fm] t=0.10  cos(v_pred, v_tgt)=0.050  ||pred||/||tgt||=0.287
[fm] t=0.37  cos(v_pred, v_tgt)=0.045  ||pred||/||tgt||=0.296
[fm] t=0.63  cos(v_pred, v_tgt)=0.036  ||pred||/||tgt||=0.286
[fm] t=0.90  cos(v_pred, v_tgt)=0.015  ||pred||/||tgt||=0.282

3）“对着真图射击”：从 x0 积分到 x1（教师驱动版）
[shoot] MSE(z_end, x1) = 4.531314


```

