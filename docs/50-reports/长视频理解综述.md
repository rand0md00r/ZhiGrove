# MA-LMM 中的 Query-Former 与长时记忆模块

## 方法流程
1. **视觉特征提取**
   - 视频逐帧输入视觉编码器，提取帧特征并加入时间位置编码。
   - 特征存入长时记忆模块。

2. **长时记忆建模**
   - 使用 Query-Former 作为桥梁，对齐视觉特征与文本空间。
   - 引入两个记忆库：
     - **视觉记忆库（Visual Memory Bank）**：保存所有历史帧特征，用于 cross-attention。
     - **查询记忆库（Query Memory Bank）**：保存历次查询结果（learned queries），用于 self-attention。

3. **文本解码**
   - Q-Former 输出的最终表示输入 LLM，生成回答或描述。

## 输入输出
- **输入**：视频帧序列 + 文本提示  
- **输出**：自然语言回答、视频描述或分类标签  

## 记忆更新机制
1. **视觉记忆库更新**
   - 每新一帧加入库中，作为 cross-attention 的 Key/Value。

2. **查询记忆库更新**
   - 每个时间步的查询 \(z_t\) 累积存入库中，作为 self-attention 的 Key/Value。

3. **记忆压缩（MBC）**
   - 计算相邻帧特征余弦相似度，合并最相似的一对特征。  
   - 保留判别性特征并减少冗余，避免显存线性增长。



# 长视频理解方法对比：MA-LMM vs. Streaming-LVU

| 方法                                                         | 对已有记忆库方法的批评                                       | 改进思路                                                     | 特点                                               |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | -------------------------------------------------- |
| **MA-LMM** (Memory-Augmented LMM for Long-Term Video Understanding) | 记忆库在召回历史信息时依赖**显式时间戳**，如果没有时间指引，难以生成全面的回答 | 引入 **Query-Former + 双记忆库（视觉记忆库 + 查询记忆库）**，通过语义查询与历史特征交互进行检索 | 解决了对时间戳的依赖，支持语义驱动的历史信息回忆   |
| **Streaming-LVU** (Streaming Long Video Understanding with LLMs) | 在长视频场景中，传统记忆库容易出现 **检索效率低**，且依赖时间片段索引，难以进行流式理解 | 提出 **Streaming 机制**，动态维护上下文记忆，支持流式输入和压缩更新 | 强调连续视频流处理与高效记忆管理，适合超长视频场景 |



# VideoStreaming (Memory-Propagated Streaming Encoding + Adaptive Memory Selection)

## 方法核心思想
- **目标**：在长视频理解中保留关键的空间信息与时间动态，同时减少冗余。  
- **核心机制**：
  1. **Memory-Propagated Streaming Encoding**：逐段编码视频，并将前一段的记忆传递给后一段，形成连续表示。  
  2. **Adaptive Memory Selection**：针对具体问题，自适应地选择相关历史记忆，提升回答的针对性和精确性。  

---

## 方法流程
1. **视频分段与逐段编码**
   - 将长视频划分为多个短片段（clip）。  
   - 编码当前片段时，会先参考前一个片段的记忆，再与当前特征拼接后输入到 **小型 Decoder-only LM**。  
   - 由于自回归特性，序列信息逐渐累积到最后几个 token，因此使用最后几个 token 作为更新后的记忆，代表当前时间点之前的全部视频信息。  

2. **固定长度记忆**
   - 模型在编码过程中始终保持固定长度的记忆，用以表示任意长的视频。  
   - 但这会导致早期细节被压缩或丢失。  

3. **历史记忆保存与问题驱动的记忆选择**
   - 为弥补固定长度记忆的不足，方法会保存 **所有片段的历史记忆**。  
   - 在编码每个片段时，附加一个 **summary token**（片段指示符），用于概括该片段的语义信息。  
   - 在回答具体问题时：  
     1. 将最终迭代的 condensed memory 与问题拼接，输入同一个小型 LM。  
     2. 获取问题指示符（question indicator）。  
     3. 计算问题指示符与所有片段指示符的相似度。  
     4. 选取相似度最高的若干 clip memories，作为与问题最相关的历史信息。  
   - 最终将这些 **自适应选择的记忆** 输入 LLM，进行详细问答。  

---

## 输入与输出
- **输入**：长视频（切分为多个 clip）+ 问题（question）。  
- **输出**：对问题的自然语言回答，具备明确的时间指向性（temporal grounding）。  

---

## 训练策略
- **两阶段训练**：  
  1. **单片段预训练**：通过 prefix 任务，增强小型 LM 的单片段编码能力。  
  2. **流式训练**：让其作为 streaming encoder，并与 LLM 联合训练，实现长视频理解。  

- **数据构建**：  
  - 将短视频拼接成长视频，并保留原有问题。  
  - 使用 Panda-70M 的长视频及分段描述，构建多轮长视频 QA 数据（带显式时间戳），指导模型学习准确的记忆选择。  

---

## 方法贡献
1. 分析了长视频理解中的挑战，指出现有方法存在编码效率低的问题。  
2. 提出了两大关键设计：  
   - **Memory-Propagated Streaming Encoding**  
   - **Adaptive Memory Selection**  
3. 实验表明：VideoStreaming 能在长视频基准测试中实现更精确的时间定位、更优性能和更高推理效率。  



## 流式编码流程

第一阶段

1. 将一段视频分割为多个clip(剪辑)。对每一个clip提取均匀采样16帧。通过小语言模型和mlp投影层将每一帧蒸馏为4个token，最终使得每一个每个clip具有64个token。

第二阶段



# An Exploration of Video Understanding in Large Multimodal Models

# Apollo: An Exploration of Video Understanding in Large Multimodal Models

## 论文贡献

1. **系统性探索设计空间**
   - 全面研究视频大规模多模态模型（video-LMMs）的关键要素：  
     视频采样、编码器选择、token 重采样、token 融合、训练调度、数据构成等。  
   - 揭示了真正驱动视频理解性能的关键因素，并提供了可操作的设计建议。

2. **提出 “Scaling Consistency”**
   - 发现小规模模型（2B–4B 参数以上）和小数据集上的设计选择可以可靠迁移到大模型。  
   - 显著降低研究成本，使研究者可以在低资源条件下高效实验。

3. **改进评测方式：ApolloBench**
   - 指出现有基准测试中大量问题无需视频信息即可解答，存在冗余。  
   - 提出 **ApolloBench**：更高效、更紧凑的基准，评测速度快 41 倍，并更能区分视频感知与推理能力。

4. **推出 Apollo 模型家族**
   - 基于研究洞察，训练了 **Apollo-1.5B, Apollo-3B, Apollo-7B** 三类模型。  
   - **Apollo-3B 超越大多数 7B 模型**，而 **Apollo-7B 在同类中达到 SOTA**，甚至超过部分 30B 模型。  
   - 展现了通过合理设计与训练，中等规模模型也能实现先进的视频理解能力。

---

## 论文结论

- 本研究填补了视频 LMM 缺乏系统性探索的空白，提供了从 **设计 → 训练 → 评测 → 模型实现** 的完整指南。  
- **Scaling Consistency** 的提出，使得研究者无需依赖超大模型，也能获得可靠的实验结论，加快研究迭代。  
- **ApolloBench** 提供了一个更高质量的评测工具，避免了现有基准中的“假视频理解”现象。  
- **Apollo 模型家族** 的结果证明：通过精心设计，中小规模模型也能匹敌甚至超越更大规模模型。  
- 整体上，Apollo 工作为视频 LMM 的未来发展提供了 **理论洞察、实证结果与实用工具**，推动该领域向更高效、更可解释的方向前进。  

# OVO-Bench: 在线视频理解基准

## 研究背景
- **时间感知（Temporal Awareness）** 是在线视频大模型（Video-LLMs）的关键能力。  
- 离线模型：依赖完整视频，做静态、事后分析。  
- 在线模型：需在视频流过程中，随时间戳动态推理与回答。  
- **现有基准不足**：未能充分评估“时间感知”能力。  

---

## 三大评测场景
1. **Backward tracing（回溯推理）**  
   - 追溯过去事件来回答问题。  
2. **Real-time understanding（实时理解）**  
   - 在当前时间戳即时理解并回答。  
3. **Forward active responding（前瞻响应）**  
   - 等待未来信息足够时，再准确作答。  

---

## 数据与任务设计
- 共 **12 个任务**，**644 个视频**。  
- 含 **约 2800 条人工精细标注**，均带有 **精确时间戳**。  
- 结合 **自动生成管线** 与 **人工标注**，保证数据高质量。  

---

## 评测方法
- 设计了系统化评估流程。  
- 沿视频时间轴动态提问，模拟真实使用场景。  

---

## 实验结果
- 评估了 **11 个现有 Video-LLMs**。  
- 结果显示：  
  - 在传统基准上表现良好。  
  - 在 **在线视频理解** 上，显著落后于人类表现。  

---

## 贡献与意义
- **填补空白**：首次系统性评估视频 LLM 的时间感知能力。  
- **推动研究**：促进模型在 **在线动态推理** 方向的进步。  
- **开源资源**：[GitHub - OVO-Bench](https://github.com/JoeLeelyf/OVO-Bench)  



# Temporal Preference Optimization for Long-Form Video Understanding

## 核心问题
- 当前视频大模型（video-LMMs）在长视频理解上表现欠佳，尤其是 **时间定位 (temporal grounding)** 能力不足。
- 现有方法依赖大规模合成/标注数据，且缺乏显式的时间优化信号。

## 方法论：Temporal Preference Optimization (TPO)
TPO 是一个 **后训练(post-training)** 框架，结合 **自动偏好数据构造** 与 **Direct Preference Optimization (DPO)**，提升模型的长时序理解能力。

### 1. 偏好数据构造 (Self-Training)
TPO 自动生成 **优选 (preferred)** 与 **劣选 (dis-preferred)** 响应对，形成偏好数据：
- **Localized Temporal Preference**
  - 针对视频子片段生成问题。
  - r⁺：基于相关片段的回答。
  - r⁻：基于去掉该片段的其他部分生成的回答。
- **Comprehensive Temporal Preference**
  - 针对全局长时序问题。
  - r⁺：基于完整视频的回答。
  - r⁻：基于下采样/缺失片段的视频回答。
- **Post-filtering**：利用 LLM（GPT-4o-mini）过滤掉错误或质量不佳的pair。

### 2. 偏好优化 (Direct Preference Optimization)
- 使用 DPO 训练 video-LMM，使模型更倾向输出 r⁺ 而非 r⁻。
- 同时结合少量 **SFT Loss**，保证训练稳定性。  
- 优化目标：
  
  $\pi_\theta(r^+|V,q) > \pi_\theta(r^-|V,q)$

---

## 贡献
1. **提出 TPO 框架**：一种轻量、可扩展的后训练方法，显式增强长视频的时间理解能力。
2. **双粒度时间偏好建模**：
   - Localized（细粒度）+ Comprehensive（全局依赖），实现互补效果。
3. **无需大规模人工标注**：通过自训练自动生成偏好对，降低了数据成本。

---

## 总结
TPO 通过 **自动构造多粒度时间偏好数据 + 偏好优化训练**，显著提升视频大模型的**长时序理解与时间定位能力**，为长视频理解提供了一种高效的**后训练解决方案**。



### 方法论总结：Temporal Preference Optimization (TPO)

- **核心思想**：通过构造 **优选 (preferred)** 与 **劣选 (dis-preferred)** 响应对，引导模型学习时间定位能力。  

#### 两类时间偏好数据
1. **局部时间定位 (Localized Temporal Grounding)**
   - 问题针对特定片段。
   - r⁺：来自对应片段的回答。
   - r⁻：来自无关片段的回答。  

2. **全局时间定位 (Comprehensive Temporal Grounding)**
   - 问题涉及更长时序依赖。
   - r⁺：基于完整视频的回答。
   - r⁻：基于下采样、缺失关键信息的视频回答。  

#### 优化方式
- 利用 **Direct Preference Optimization (DPO)**，让模型更倾向输出 r⁺。  
- 简单的视频输入变换即可自动生成偏好数据。  

#### 效果
- 在 **细粒度 (局部)** 与 **长上下文 (全局)** 场景下均能提升时间推理能力。  
- 为长视频理解提供稳健的后训练解决方案。  



# Video-XL:  Extra-Long Vision Language Model for Hour-Scale Video Understanding



1. 输入与特征抽取

- 输入：长视频（小时级别，可包含 2k+ 帧）
- 处理：
  - 从视频中抽帧
  - 每帧送入 **CLIP ViT-L/14**，得到帧级视觉 token（包括 [CLS] 表示）

---

2. 语义一致性分段（Semantic Consistency Segmentation）

- 目标：避免固定长度切片，做到“语义稳定 → 粗切，语义突变 → 细切”
- 方法：
  1. 使用 CLIP [CLS] embedding 表征每帧
  2. 计算 **Depth Score**：
     -  $D_t = 1 - \cos(f_t, f_{t+1})$ 
  3. 在窗口内找到局部最大值作为 **分段边界**
- 输出：动态长度的片段 $$\{X_1, X_2, \ldots, X_i\}$$

---

3. 动态压缩率分配

- 每个片段 $X_i$$  分配一个压缩率 $   $\alpha_i $ ：
  - 语义复杂 (Depth Score 高) → 小 $\alpha$ → 更多 VST
  - 语义平稳 (Depth Score 低) → 大 $\alpha$ → 更少 VST

---

4. 插入 Visual Summarization Token (VST)

- 在片段 $X_i$ 中，每隔 $\alpha_i$ 个视觉 token 插入一个 VST：
  - 公式：$ k_i = |X_i| / \alpha_i $$
  - 插入后得到：  
    $$X'_i = [x_{i1}, ..., x_{i\alpha_i}, \langle vs \rangle_{i1}, ..., x_{i|X_i|}, \langle vs \rangle_{ik_i}]$$

---

5. LLM 编码 + KV 压缩

- 基础模型：**Qwen2-7B**
- 编码流程：
  1. 将片段 $X'_i$ 送入 LLM，所有 token（包括 VST）参与多头自注意力
  2. 片段结束后：
     - **丢弃普通视觉 token 的 K,V**
     - **保留 VST 的 K,V** 作为片段的压缩记忆
  3. 编码下一个片段 $X_{i+1}$ 时：
     - 仅能 attend 到之前所有 VST 的 KV（作为代理）

---

6. 累积记忆 & 长视频建模

- VST KV-cache 在不同片段间累积，形成“压缩版的全局记忆”
- 优点：
  - 上下文长度消耗 ~ 与 VST 数量线性相关
  - 避免原始视觉 token 的二次复杂度 & 巨大显存开销

---

7. 训练策略

- **指令微调**：统一图像/多图/视频任务
- **课程学习**：从小压缩比 (2×/4×) → 大压缩比 (8×/16×)，逐步适应
- **合成数据 VICO**：基于视频分段字幕生成跨段 QA，用于训练长视频推理能力

---

8. 整体特点

- 基于 LLaVA-like MLLM，而非专门的 Video-Encoder
- 创新点：**LLM 内部的 KV 压缩 (VST) + 动态自适应分段**
- 能在单卡上处理 **小时级视频 (2000+ 帧)**，同时保持细粒度推理能力



# StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding



| 模块                    | 内容                                                         |
| ----------------------- | ------------------------------------------------------------ |
| **输入**                | 流式视频（逐段到达的帧序列）                                 |
| **输出**                | 视频问答 / 推理结果（QA、检索、排序等任务）                  |
| **主干网络 (Backbone)** | 现有 MLLMs（如 **LLaVA-OneVision, Qwen2-VL, Qwen2.5-VL**），**StreamMem 作为插件，不需再训练** |
| **流程概述**            | 1. 视频帧流式输入   2. 输入帧压缩（减少冗余）   3. 送入视觉编码器 → 投影到 MLLM   4. 每段计算 KV-cache → 通过 **Saliency Metric** 选择重要 KV   5. **Frame-wise KV merging** 构建帧级 prototype 表示   6. 累积形成 **紧凑 KV memory**   7. QA 阶段利用 KV memory 进行推理 |
| **KV 压缩机制**         | - **显著性筛选 (Saliency Metric)**：基于 visual token 与 chat template token 的 cross-attention 分数，选出重要 KV   - **输入压缩**：减少重复帧输入   - **KV 合并**：相似 KV 合并为 prototype，保持多样性同时节省内存 |
| **特点**                | - **Query-agnostic**：不依赖提前知道问题，压缩时通用保留信息   - **Training-free**：无需微调，可直接接入任意 MLLM   - **Bounded Memory**：内存占用随视频长度不增长，避免 OOM |
| **应用场景**            | - 实时流式视频理解   - 内存受限设备上的长视频推理   - 开放世界的连续视频 Agent |

| 方面         | **Video-XL: 动态压缩率分配**                                 | **StreamMem: Saliency Metric**                               |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **触发时机** | 在 **视频分段阶段** 就决定压缩率                             | 在 **KV 压缩阶段** 选择哪些 KV 保留                          |
| **依据信号** | - **Depth Score**：基于相邻帧的 CLIP [CLS] 余弦相似度差  - 捕捉语义突变（动作切换点） | - **Cross-attention 分数**：视觉 token 与 chat template token 的注意力强度  - 捕捉哪些视觉 token 对语言理解更有用 |
| **压缩方式** | - 动态设置 **插入 VST 的频率**  - 语义复杂 → 更多 VST，保留更细信息  - 语义平稳 → 更少 VST，节省内存 | - 从已有的 KV-cache 中 **选择/合并重要 token**  - 保留显著 token 的 KV  - 合并冗余 KV 为 prototype |
| **目标**     | - 自适应分配压缩比，保证语义边界处信息密集  - **“结构级别”压缩**（按片段分配 VST 数量） | - 无需 query，也能在 KV-cache 中挑选关键信息  - **“内容级别”压缩**（按 token 显著性筛选） |
| **优势**     | - 对“语义边界”敏感，能在动作切换点保留更多细节  - 易解释（Depth Score 可视化清晰） | - 与 query 无关，支持 **流式输入**  - 不需重新切片，能在线动态选择 KV |
| **局限**     | - 需要提前做分段（offline）  - 偏离线场景，不适合纯 streaming | - 注意力显著性可能不等于任务显著性  - 可能保留了一些“假重要”的 token |

# Qwen2.5-Omni: End-to-End Multimodal Model

## 模型特点
- **输入模态**：文本、图像、音频、视频  
- **输出模态**：文本 + 语音（自然语音，支持流式生成）  
- **流式处理**：支持输入和输出的实时流式处理  

## 核心方法
1. **Block-wise Processing**
   - 音频和视频编码器采用分块处理
   - 编码器负责感知 → LLM 负责长序列建模  
   - 实现高效解耦和跨模态融合  

2. **TMRoPE (Time-aligned Multimodal RoPE)**
   - 新的时间对齐位置编码
   - 将音频与视频帧按时间顺序交错排列，实现跨模态时间同步  

3. **Thinker–Talker 架构**
   - **Thinker**：LLM，负责文本生成  
   - **Talker**：双轨自回归模型，直接基于 Thinker 的隐表示生成音频 token  
   - 两者端到端联合训练与推理，避免模态间干扰  

4. **Streaming Audio Decoding**
   - 使用滑动窗口 DiT（Diffusion Transformer）
   - 限制感受野以降低初始延迟，提高流式语音生成体验  

## 实验结果
- 与 **Qwen2.5-VL** 相当，优于 **Qwen2-Audio**  
- 在 **Omni-Bench** 等多模态基准上达到 SOTA  
- 语音指令跟随任务效果接近文本输入能力（MMLU, GSM8K）  
- 流式语音生成在稳健性与自然度上超越大多数现有方法（包括非流式）  

## 关键贡献
- 首个端到端 **流式多模态输入–输出** 模型  
- 提出 **TMRoPE** 解决跨模态时间同步  
- 提出 **Thinker–Talker** 架构实现文本与语音的并行生成  
- 推出高效 **流式语音解码** 方法，降低延迟，提升交互体验



**不同模态的处理方式**

- **Text 文本**
  - 只用 1D 的位置 ID（等价于普通的 RoPE）。
- **Audio 音频**
  - 每 40ms 对应一个 temporal ID。
  - 相当于：音频序列直接按时间轴对齐。
- **Image 图像**
  - 每个图像 token 的 temporal ID 相同（因为一张图像就是一个时间点）。
  - 高度、宽度的 ID 则根据在图像中的位置分配（2D RoPE）。
- **Video 视频**
  - 视为多张图像序列。
  - 每帧视频有一个**独立的 temporal ID**（随时间推移递增）。
  - **高度、宽度依然按图像 token 的 2D 位置分配**。
  - **帧率不固定时**：temporal ID 会根据实际时间调整，**确保 40ms 对应 1 个 ID，与音频对齐。**

在 TMRoPE 中，视频被视作一系列图像，每一帧除了**分配高度和宽度位置 ID** 外，还会根据**时间顺序获得递增的时间 ID**。由于视频帧率并不固定，TMRoPE 会依据帧的实际时间戳**动态调整时间 ID**，确保始终遵循“一个时间 ID 对应 40ms”的规则。这样，**视频帧的时间 ID 能与音频的时间 ID 对齐**，**实现跨模态的精确时序同步**，从而保证模型在多模态输入下既能**捕捉空间结构**，又能保持**时间一致性**。

| 阶段                       | 训练策略                                                   | 数据规模 & 类型                                              | 目标                                                         |
| -------------------------- | ---------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **阶段一**：模态编码器适配 | - 冻结 LLM- 仅训练视觉/音频编码器（先 adapter 后 encoder） | - 图文对- 音频-文本对                                        | - 建立视觉-文本、音频-文本的语义对齐- 为 LLM 融合多模态奠定基础 |
| **阶段二**：多模态联合训练 | - 解冻所有参数，全模型训练                                 | - 图像/视频相关 **8000 亿 tokens**- 音频相关 **3000 亿 tokens**- 音视频联合 **1000 亿 tokens**- 纯文本数据 | - 跨模态交互与理解- 多任务学习能力- 保持/提升语言能力        |
| **阶段三**：长序列训练     | - 扩展序列长度到 32,768 tokens                             | - 长文本- 长音频- 长视频- 长图像序列                         | - 增强复杂长序列建模能力- 适应真实场景下的长时依赖           |



## Qwen-VL 视频能力分析

| 方面           | 优势                                                         | 缺陷                                                         |
| -------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **时间建模**   | - 动态 FPS 采样，避免冗余帧- MRoPE 与绝对时间对齐，可进行秒级事件定位 | - 没有显式记忆机制，无法保持跨片段的长时依赖- 跨场景因果推理能力有限 |
| **Token 处理** | - 14×14 patch 切分 + 2×2 merge + MLP 压缩，显著减少 token 数- 支持更长视频输入（最高 32k tokens） | - 压缩导致细节丢失（如小目标跟踪、动作细节理解）- 对高精度需求的视频任务可能性能下降 |
| **注意力机制** | - 窗口注意力降低计算复杂度，仅少数层全局 self-attention- 能处理分钟级视频而保持效率 | - 全局一致性弱，长视频中不同片段之间的语义连接不稳固         |
| **长序列支持** | - 最终阶段训练扩展至 32k tokens，覆盖长视频、长音频数据- 在 LongVideoBench、LVBench 等基准超越 GPT-4o | - 32k tokens 仍有限制，无法端到端覆盖小时级超长视频- 实际需依赖分段/滑动窗口处理 |
| **整体表现**   | - 在分钟级长视频（几千帧）中具备强时序理解和事件定位能力     | - 超长视频（>30 分钟至小时级）理解力不足，易片段化、遗忘全局语义 |

### 边界与缺陷

- **记忆机制缺失**：依赖 **token 压缩 + 窗口 attention**，没有引入 memory bank / 分层时序建模 → 在跨场景、长时依赖视频任务上仍会失效。
- **小时级视频 ≠ 全局建模**：虽然可以输入小时级视频，但受限于 **32k token 上限**，无法在端到端条件下完整编码小时级视频（需要分块/滑动窗口）。
- **细粒度信息丢失**：2×2 patch merge 等压缩方式减少了时空分辨率 → 对精细动作、细微物体变化不够敏感。
- **跨片段推理能力有限**：在需要全局语义一致性、因果链路建模的任务（如电影剧情问答）上存在弱点。







