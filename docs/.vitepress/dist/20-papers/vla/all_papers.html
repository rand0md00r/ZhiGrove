<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>论文记录模板（复制用） | ZhiGrove</title>
    <meta name="description" content="Wang Yaqi's Knowledge Base">
    <meta name="generator" content="VitePress v1.6.4">
    <link rel="preload stylesheet" href="/ZhiGrove/assets/style.C8nuTVcb.css" as="style">
    <link rel="preload stylesheet" href="/ZhiGrove/vp-icons.css" as="style">
    
    <script type="module" src="/ZhiGrove/assets/app.CXuj0tw-.js"></script>
    <link rel="preload" href="/ZhiGrove/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/ZhiGrove/assets/chunks/theme.DdaDK2L0.js">
    <link rel="modulepreload" href="/ZhiGrove/assets/chunks/framework.OaOo95RB.js">
    <link rel="modulepreload" href="/ZhiGrove/assets/20-papers_vla_all_papers.md.CYgyoP_l.lean.js">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-5d98c3a5><!--[--><!--]--><!--[--><span tabindex="-1" data-v-0b0ada53></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0b0ada53>Skip to content</a><!--]--><!----><header class="VPNav" data-v-5d98c3a5 data-v-ae24b3ad><div class="VPNavBar" data-v-ae24b3ad data-v-6aa21345><div class="wrapper" data-v-6aa21345><div class="container" data-v-6aa21345><div class="title" data-v-6aa21345><div class="VPNavBarTitle has-sidebar" data-v-6aa21345 data-v-1168a8e4><a class="title" href="/ZhiGrove/" data-v-1168a8e4><!--[--><!--]--><!----><span data-v-1168a8e4>ZhiGrove</span><!--[--><!--]--></a></div></div><div class="content" data-v-6aa21345><div class="content-body" data-v-6aa21345><!--[--><!--]--><div class="VPNavBarSearch search" data-v-6aa21345><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-6aa21345 data-v-dc692963><span id="main-nav-aria-label" class="visually-hidden" data-v-dc692963> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/ZhiGrove/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>首页</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/ZhiGrove/00-inbox/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>收件箱</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/ZhiGrove/10-knowledge/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>知识库</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/ZhiGrove/20-papers/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>论文</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/ZhiGrove/30-ideas/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>灵感</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/ZhiGrove/40-experiments/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>实验</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/ZhiGrove/50-reports/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>报告</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-6aa21345 data-v-6c893767><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-6c893767 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-6aa21345 data-v-0394ad82 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/wangyaqi/ZhiGrove" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-6aa21345 data-v-bb2aa2f0 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-cf11d7a2><span class="vpi-more-horizontal icon" data-v-cf11d7a2></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><!----><!--[--><!--[--><!----><div class="group" data-v-bb2aa2f0><div class="item appearance" data-v-bb2aa2f0><p class="label" data-v-bb2aa2f0>Appearance</p><div class="appearance-action" data-v-bb2aa2f0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-bb2aa2f0 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div></div></div><div class="group" data-v-bb2aa2f0><div class="item social-links" data-v-bb2aa2f0><div class="VPSocialLinks social-links-list" data-v-bb2aa2f0 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/wangyaqi/ZhiGrove" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-6aa21345 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><div class="divider" data-v-6aa21345><div class="divider-line" data-v-6aa21345></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-5d98c3a5 data-v-a6f0e41e><div class="container" data-v-a6f0e41e><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-a6f0e41e><span class="vpi-align-left menu-icon" data-v-a6f0e41e></span><span class="menu-text" data-v-a6f0e41e>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-a6f0e41e data-v-8a42e2b4><button data-v-8a42e2b4>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-5d98c3a5 data-v-319d5ca6><div class="curtain" data-v-319d5ca6></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-319d5ca6><span class="visually-hidden" id="sidebar-aria-label" data-v-319d5ca6> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0 has-active" data-v-c40bc020 data-v-b3fd67f8><div class="item" role="button" tabindex="0" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><h2 class="text" data-v-b3fd67f8>Papers</h2><!----></div><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/ZhiGrove/20-papers/" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>README</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/ZhiGrove/20-papers/250829-0735-Qwen2.5-Omni%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>250829-0735-Qwen2.5-Omni论文阅读</p><!--]--></a><!----></div><!----></div><section class="VPSidebarItem level-1 collapsible collapsed" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" role="button" tabindex="0" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><h3 class="text" data-v-b3fd67f8>2025</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b3fd67f8><span class="vpi-chevron-right caret-icon" data-v-b3fd67f8></span></div></div><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-2 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/ZhiGrove/20-papers/2025/2025-08-20-.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>2025-08-20-</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-2 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/ZhiGrove/20-papers/2025/2025-08-20-vtla-preference-learning.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>2025-08-20-vtla-preference-learning</p><!--]--></a><!----></div><!----></div><!--]--></div></section><section class="VPSidebarItem level-1 collapsible collapsed has-active" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" role="button" tabindex="0" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><h3 class="text" data-v-b3fd67f8>vla</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b3fd67f8><span class="vpi-chevron-right caret-icon" data-v-b3fd67f8></span></div></div><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-2 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/ZhiGrove/20-papers/vla/all_papers.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>all_papers</p><!--]--></a><!----></div><!----></div><!--]--></div></section><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-5d98c3a5 data-v-1428d186><div class="VPDoc has-sidebar has-aside" data-v-1428d186 data-v-39a288b8><!--[--><!--]--><div class="container" data-v-39a288b8><div class="aside" data-v-39a288b8><div class="aside-curtain" data-v-39a288b8></div><div class="aside-container" data-v-39a288b8><div class="aside-content" data-v-39a288b8><div class="VPDocAside" data-v-39a288b8 data-v-3f215769><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3f215769 data-v-a5bbad30><div class="content" data-v-a5bbad30><div class="outline-marker" data-v-a5bbad30></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-a5bbad30>On this page</div><ul class="VPDocOutlineItem root" data-v-a5bbad30 data-v-b933a997><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-39a288b8><div class="content-container" data-v-39a288b8><!--[--><!--]--><main class="main" data-v-39a288b8><div style="position:relative;" class="vp-doc _ZhiGrove_20-papers_vla_all_papers" data-v-39a288b8><div><h1 id="论文记录模板-复制用" tabindex="-1">论文记录模板（复制用） <a class="header-anchor" href="#论文记录模板-复制用" aria-label="Permalink to &quot;论文记录模板（复制用）&quot;">​</a></h1><blockquote><p>每篇论文按此模板填写，并放到合适分区（可在多个分区留“短条目”交叉引用）</p></blockquote><h2 id="标题-年份-会议-期刊" tabindex="-1">标题（年份，会议/期刊） <a class="header-anchor" href="#标题-年份-会议-期刊" aria-label="Permalink to &quot;标题（年份，会议/期刊）&quot;">​</a></h2><ul><li><strong>Citation</strong>：作者，题目，会议/期刊，年份</li><li><strong>链接</strong>：论文 | 代码 | 项目页（若有）</li><li><strong>任务</strong>：操作 / 导航 / 混合；场景（家庭/工业/车载…）</li><li><strong>架构</strong>：Planner-Executor / 端到端；是否工具调用；是否记忆模块</li><li><strong>动作范式</strong>：AR / 扩散 / Flow Matching / 混合；动作空间（离散/连续；关节/技能/程序）</li><li><strong>感知输入</strong>：RGB / 深度 / 语言 / 多视角 / 视频长度</li><li><strong>训练</strong>：预训练数据（规模/来源）、SFT、偏好/强化（DPO/RLAIF/RLHF）、奖励设计</li><li><strong>数据</strong>：自建 / 公共；真机/仿真；合成/蒸馏</li><li><strong>评测</strong>：基准与指标；关键结果（可列 1–3 个数字）</li><li><strong>开源程度</strong>：权重 / 训练代码 / 推理代码 / 数据（许可证）</li><li><strong>部署</strong>：推理硬件、时延、吞吐、边缘可行性</li><li><strong>亮点</strong>：3–5 条要点</li><li><strong>局限</strong>：2–3 条要点</li><li><strong>个人笔记</strong>：你的理解、与现有系统的可复用性、TODO</li></ul><hr><h2 id="π0-5-a-vision-language-action-model-with-open-world-generalization-2025-corl" tabindex="-1">π₀․₅: a Vision-Language-Action Model with Open-World Generalization（2025，CoRL） <a class="header-anchor" href="#π0-5-a-vision-language-action-model-with-open-world-generalization-2025-corl" aria-label="Permalink to &quot;π₀․₅: a Vision-Language-Action Model with Open-World Generalization（2025，CoRL）&quot;">​</a></h2><ul><li><p><strong>Citation</strong>：Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, <em>et al.</em> “π₀․₅: a Vision-Language-Action Model with Open-World Generalization.” <em>Proceedings of the 9th Conference on Robot Learning (CoRL)</em>, PMLR 305:17–40, 2025. (<a href="https://proceedings.mlr.press/v305/black25a.html" title="$\pi_0.5$: a Vision-Language-Action Model with Open-World Generalization" target="_blank" rel="noreferrer">Proceedings of Machine Learning Research</a>)</p></li><li><p><strong>链接</strong>：论文（arXiv | PMLR）| 代码（openpi）| 项目页/博文</p><ul><li>arXiv：(<a href="https://arxiv.org/abs/2504.16054?utm_source=chatgpt.com" title="[2504.16054] $π_{0.5}$: a Vision-Language-Action Model ..." target="_blank" rel="noreferrer">arXiv</a>)</li><li>PMLR页面与PDF：(<a href="https://proceedings.mlr.press/v305/black25a.html" title="$\pi_0.5$: a Vision-Language-Action Model with Open-World Generalization" target="_blank" rel="noreferrer">Proceedings of Machine Learning Research</a>)</li><li>博文解读：(<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</li><li>开源仓库（Apache-2.0）：(<a href="https://github.com/Physical-Intelligence/openpi" title="GitHub - Physical-Intelligence/openpi" target="_blank" rel="noreferrer">GitHub</a>)</li><li>HF 权重（LeRobot 转换）：(<a href="https://huggingface.co/lerobot/pi05_base?utm_source=chatgpt.com" title="lerobot/pi05_base" target="_blank" rel="noreferrer">Hugging Face</a>)</li></ul></li><li><p><strong>任务</strong>：操作（家居场景的长任务，如收拾餐具、擦除污渍、整理卧室）；强调“陌生家庭”零样本泛化。 (<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</p></li><li><p><strong>架构</strong>：单一 VLA 模型同时做高层语义决策与低层连续控制；推理时先自述式高层子任务（AR 文本），再生成低层“动作块”（Flow Matching 连续控制）。非工具调用范式；无专门记忆模块描述。 (<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</p></li><li><p><strong>动作范式</strong>：<strong>混合</strong>＝高层<strong>自回归</strong>（文本子任务）+ 低层<strong>Flow Matching</strong>（连续关节控制）；低层以约 1s/50 步的“action chunk”输出连续关节指令；动作空间为<strong>连续</strong>（关节级）。另含约 3e8 参数的“action expert”。 (<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</p></li><li><p><strong>感知输入</strong>：RGB 视觉 + 语言指令；训练期采用混合多模态信号（检测框、子任务标签、网页多模态等）。(<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</p></li><li><p><strong>训练</strong>：核心是**异构协同训练（co-training）**与混合监督：</p><ul><li>来源：移动操作真机数据、跨环境静态/移动机器人数据、跨载体（cross-embodiment）数据、网页多模态任务（VQA/Caption/Detection）、“口头教练”逐步指令等；文中对<strong>取消某一来源</strong>的消融。(<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</li><li>规模：开源仓库给出 openpi 基座模型“<strong>10k+ 小时机器人数据</strong>”预训练（π₀/π₀-FAST/π₀․₅均提供 base ckpt）；文中移动操作自采数据在某些消融中约 <strong>400 小时</strong>。(<a href="https://github.com/Physical-Intelligence/openpi" title="GitHub - Physical-Intelligence/openpi" target="_blank" rel="noreferrer">GitHub</a>)</li><li>相关技术：知识绝缘（Knowledge Insulation）作为 π₀․₅ 训练配方升级的一部分在 repo/白皮书中提及。(<a href="https://github.com/Physical-Intelligence/openpi" title="GitHub - Physical-Intelligence/openpi" target="_blank" rel="noreferrer">GitHub</a>)</li></ul></li><li><p><strong>数据</strong>：<strong>真机</strong>为主，家庭/办公室等多环境；<strong>跨载体/跨环境</strong>整合 + <strong>网页多模态</strong>；并在开源模型中提供与 <strong>DROID/LIBERO</strong> 等<strong>公共数据</strong>相关的变体与微调示例。(<a href="https://github.com/Physical-Intelligence/openpi" title="GitHub - Physical-Intelligence/openpi" target="_blank" rel="noreferrer">GitHub</a>)</p></li><li><p><strong>评测</strong>：两大设置——<strong>完整清洁任务</strong>与 <strong>OOD 指定物体入抽屉</strong>；指标为<strong>语言跟随率</strong>与<strong>成功率</strong>。关键数字（博文给出）：</p><ul><li><strong>IID 成功率</strong>：83%（π₀․₅） vs 57%（无多环境数据 ME）/67%（无跨载体 CE）。</li><li><strong>OOD 成功率</strong>：<strong>94%</strong>（π₀․₅） vs 31%（无 ME）/49%（无 CE）/74%（无网页数据 WD）。</li><li><strong>结论</strong>：WD 对 OOD 类别识别助益大；ME/CE 对整体泛化关键。(<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</li></ul></li><li><p><strong>开源程度</strong>：<strong>权重</strong>（π₀․₅ base 与若干任务专家）、<strong>训练与推理代码</strong>（现已含 PyTorch 训练流程）、<strong>数据接口/示例</strong>均开放（Apache-2.0）；HF 提供 PyTorch safetensors 转换。(<a href="https://github.com/Physical-Intelligence/openpi" title="GitHub - Physical-Intelligence/openpi" target="_blank" rel="noreferrer">GitHub</a>)</p></li><li><p><strong>部署</strong>：官方 openpi README 建议<strong>单卡 RTX 4090（&gt;8GB）可推理</strong>；LoRA 微调约 &gt;22.5GB；全参微调需 A100/H100 级显存。已给出 Docker/UV 环境与多 GPU FSDP 选项。(<a href="https://github.com/Physical-Intelligence/openpi" title="GitHub - Physical-Intelligence/openpi" target="_blank" rel="noreferrer">GitHub</a>)</p></li><li><p><strong>亮点</strong>：</p><ol><li><strong>高/低层一体化</strong>：同一模型先“想”（AR 文本）再“做”（Flow连续控制），贴近“内在推理→动作”的链式流程。(<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</li><li><strong>协同训练配方</strong>显著提升陌生环境泛化，<strong>WD/ME/CE</strong> 各司其职。(<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</li><li><strong>开放生态</strong>：代码、基座与专家权重、训练脚本、与 DROID/LIBERO 的适配齐备，易于再训练与复现。(<a href="https://github.com/Physical-Intelligence/openpi" title="GitHub - Physical-Intelligence/openpi" target="_blank" rel="noreferrer">GitHub</a>)</li><li><strong>连续动作的 Flow Matching</strong> + <strong>动作专家</strong>，在灵活性与稳定性间取得平衡。(<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</li><li><strong>可扩展数据配方</strong>与环境数缩放实验，显示“100+ 环境”后接近在测试环境训练的上限。(<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</li></ol></li><li><p><strong>局限</strong>：</p><ol><li>官方明确<strong>非追求极致灵巧度</strong>，在新家务场景仍有失败案例；成功并非稳定 100%。(<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</li><li>训练依赖<strong>大规模多源数据</strong>与较重 GPU 资源；低资源/新平台迁移仍需微调与工程适配。(<a href="https://github.com/Physical-Intelligence/openpi" title="GitHub - Physical-Intelligence/openpi" target="_blank" rel="noreferrer">GitHub</a>)</li><li>文中未强调外部工具/记忆模块，<strong>长时任务的显式记忆与可解释规划</strong>仍有提升空间（研究社区正在探索）。〔基于文献对比推断；论文未主打该点〕</li></ol></li><li><p><strong>个人笔记</strong>：</p><ul><li><p><strong>与现有系统复用性</strong>：对车内/家居“多阶段流程”的<strong>高层语言-子任务</strong>再到<strong>低层控制</strong>非常契合“Planner（文本）→ Executor（连续动作）”的工程拆分；你现有 VLA-Cabin/工业巡检可复用其<strong>协同训练配方</strong>与<strong>Flow 低层头</strong>，并将**WD（网页/合成）+ CE/ME（跨载体/跨环境）**纳入数据管线。</p></li><li><p><strong>落地建议/TODO</strong>：</p><ol><li>以 <strong>π₀․₅ base</strong> 为起点，在自家数据上做 <strong>KI（知识绝缘）配置 + LoRA</strong> 微调；优先接入 <strong>DROID/LIBERO</strong> 公共数据做对齐；桌面→移动基座分阶段蒸馏。(<a href="https://github.com/Physical-Intelligence/openpi" title="GitHub - Physical-Intelligence/openpi" target="_blank" rel="noreferrer">GitHub</a>)</li><li>保持<strong>高层 AR 子任务链</strong>可观测（日志化），同时评估低层 <strong>1s/50步动作块</strong>的时延与稳定性，必要时在仿真中做 action-chunk horizon 的灵敏度实验。(<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</li><li>建立<strong>WD/ME/CE</strong> 的<strong>可插拔数据开关</strong>，复现实验室里的 ablation 曲线，作为泛化“健康度”回归测试。(<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</li></ol></li></ul></li></ul><hr><h2 id="roboomni-proactive-robot-manipulation-in-omni-modal-context-2025-arxiv" tabindex="-1">RoboOmni: Proactive Robot Manipulation in Omni-modal Context（2025，arXiv） <a class="header-anchor" href="#roboomni-proactive-robot-manipulation-in-omni-modal-context-2025-arxiv" aria-label="Permalink to &quot;RoboOmni: Proactive Robot Manipulation in Omni-modal Context（2025，arXiv）&quot;">​</a></h2><ul><li><strong>Citation</strong>：Siyin Wang, Jinlan Fu, Feihong Liu, et al. RoboOmni: Proactive Robot Manipulation in Omni-modal Context. arXiv:2510.23763v3 [cs.RO], 2025</li><li><strong>链接</strong>：论文 <a href="https://arxiv.org/pdf/2510.23763" target="_blank" rel="noreferrer">https://arxiv.org/pdf/2510.23763</a> | 代码 <a href="https://github.com/OpenMOSS/RoboOmni" target="_blank" rel="noreferrer">https://github.com/OpenMOSS/RoboOmni</a> | 项目页 <a href="https://OpenMOSS.github.io/RoboOmni" target="_blank" rel="noreferrer">https://OpenMOSS.github.io/RoboOmni</a> | Hugging Face <a href="https://huggingface.co/collections/fnlp/roboomni" target="_blank" rel="noreferrer">https://huggingface.co/collections/fnlp/roboomni</a></li><li><strong>任务</strong>：操作；场景（家庭）</li><li><strong>架构</strong>：Perceiver-Thinker-Talker-Executor 端到端；无工具调用；无记忆模块</li><li><strong>动作范式</strong>：离散令牌（FAST+ tokenizer）；动作空间（离散2048个令牌，映射7-DoF连续关节级控制向量）</li><li><strong>感知输入</strong>：RGB、语言（文本）、音频（语音+环境声音）；多模态时序输入</li><li><strong>训练</strong>：预训练数据（OmniAction数据集141k episodes + Open-X Embodiment子集；规模140k+ episodes、来源合成构建）、SFT（下游任务微调）、无偏好/强化训练、奖励设计（自回归最大似然目标，融合对话与动作生成损失）</li><li><strong>数据</strong>：自建（OmniAction）+ 扩展公共（LIBERO）；真机+仿真；合成（文本脚本+听觉实现+验证）</li><li><strong>评测</strong>：基准（OmniAction-LIBERO-TTS、OmniAction-LIBERO-Real）；指标（成功率、意图识别准确率、推理时延）；关键结果（平均成功率85.6%、意图识别准确率88.9%、推理时延为ASR+OpenVLA的0.49倍）</li><li><strong>开源程度</strong>：权重（Hugging Face） / 训练代码（开源） / 推理代码（开源） / 数据（开源，许可证未知）</li><li><strong>部署</strong>：推理硬件（RTX 4090）、时延（0.49×相对基线）、吞吐（未明确）、边缘可行性（潜力较高，端到端效率优）</li><li><strong>亮点</strong>： <ol><li>提出跨模态上下文指令新范式，无需显式指令，从视、听、文本多模态推断用户意图</li><li>端到端框架统一意图识别、交互确认、动作执行，支持直接语音交互（无需ASR）</li><li>自建大规模OmniAction数据集，覆盖6类上下文指令、5k+说话人、多场景声学环境</li><li>推理速度远超基线，时延仅为传统ASR+VLA pipeline的一半</li><li>真机与仿真实验均验证优势，意图识别与主动协助能力突出</li></ol></li><li><strong>局限</strong>： <ol><li>非语言指令任务成功率相对较低（~82%），仍是核心挑战</li><li>预训练依赖大规模硬件资源（64 A100 GPUs训练10天），训练成本高</li><li>真机实验仅基于WidowX 250S机械臂，复杂场景与多机器人适配性待验证</li></ol></li><li><strong>个人笔记</strong>：RoboOmni的核心突破是打破了机器人对显式指令的依赖，多模态端到端设计既保留了细粒度语义信息，又提升了执行效率，OmniAction数据集的构建流程（文本脚本→听觉实现→验证）可复用在多模态机器人任务中。其主动交互逻辑（推断意图→确认→执行）贴近真实人机协作场景，可复用至家庭服务机器人系统。TODO：关注其在工业场景的适配方案，以及轻量化模型的部署可能性。</li></ul><hr></div></div></main><footer class="VPDocFooter" data-v-39a288b8 data-v-e257564d><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-e257564d><span class="visually-hidden" id="doc-footer-aria-label" data-v-e257564d>Pager</span><div class="pager" data-v-e257564d><a class="VPLink link pager-link prev" href="/ZhiGrove/20-papers/2025/2025-08-20-vtla-preference-learning.html" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Previous page</span><span class="title" data-v-e257564d>2025-08-20-vtla-preference-learning</span><!--]--></a></div><div class="pager" data-v-e257564d><!----></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-5d98c3a5 data-v-e315a0ad><div class="container" data-v-e315a0ad><p class="message" data-v-e315a0ad>Released under the MIT License.</p><p class="copyright" data-v-e315a0ad>Copyright © 2025 Wang Yaqi</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"00-inbox_index.md\":\"DLcZKgze\",\"00-inbox_readme.md\":\"CROMh2To\",\"00-inbox_week-34_triage.md\":\"Bd21h7VP\",\"00-inbox_week-35_triage.md\":\"vLr3Y60-\",\"00-inbox_week-36_triage.md\":\"D2WG-JFq\",\"00-inbox_week-37_250908-0020-clip损失.md\":\"BF9Ao-im\",\"00-inbox_week-37_250908-0020-moe llama结构.md\":\"DoV69DcU\",\"00-inbox_week-37_250908-0020-siglip损失.md\":\"tNmo36ye\",\"00-inbox_week-37_250909-0023-dino语义监督.md\":\"COxAmtT6\",\"00-inbox_week-37_250910-0024-clip-siglip-vicreg损失区别与适用场景.md\":\"DiCvqywx\",\"00-inbox_week-37_250910-0024-训练loss调试思路.md\":\"av_Unh76\",\"00-inbox_week-38_250918-0024-0916talk.md\":\"CAT-H946\",\"00-inbox_week-38_250920-0022-check_code.md\":\"BLX1z2V6\",\"00-inbox_week-39_250919-0024-0919-代码框架问题排查.md\":\"B6Wv4tLA\",\"00-inbox_week-39_250924-0024-0922-ocr-todolist.md\":\"OJ38MYqH\",\"00-inbox_week-39_250925-0025-0923-复现crossflow.md\":\"i5f0OFHX\",\"00-inbox_week-39_250925-0025-语义调音.md\":\"DITAIE7L\",\"00-inbox_week-41_251010-0025-0922-ocr-todolist.md\":\"DZPDtC9H\",\"00-inbox_week-41_251010-0025-0923-复现crossflow.md\":\"BFR0ZHnn\",\"00-inbox_week-41_251010-0025-dots_ocr服务部署 _ 调用.md\":\"zVyuBHXn\",\"00-inbox_week-41_251010-0025-laion400m训练数据处理.md\":\"1RHkz-kk\",\"00-inbox_week-41_251010-0025-语义调音.md\":\"lKF60PSI\",\"00-inbox_week-45_251027-1431-vla-vla调研分享.md\":\"iFv3PXRc\",\"00-inbox_week-45_251104-1431-ocr任务-docx等格式文档检测.md\":\"YRWX1WUF\",\"00-inbox_week-45_251104-1431-vla-扩散生成调研.md\":\"CMpVtTg7\",\"00-inbox_week-45_251104-1431-vla-综述论文节选翻译-0925发布.md\":\"DU4Vlch_\",\"00-inbox_week-46_251111-0027-k线记忆卡.md\":\"DuXtCsiw\",\"00-inbox_week-50_251113-1650-自动交易策略实现log.md\":\"DjqNSPf3\",\"00-inbox_week-50_251211-1650-deepseekv3_2.md\":\"Bko8VItX\",\"00-inbox_week-50_251211-1650-jit论文-视频要点.md\":\"DHEX-o3m\",\"00-inbox_week-50_251211-1650-rustdesk安装及配置.md\":\"w8RvI8-8\",\"00-inbox_week-50_251211-1650-trm应用于vla.md\":\"8TG5-tPZ\",\"00-inbox_week-50_251211-1650-trm应用大纲.md\":\"DiOjYmi3\",\"00-inbox_week-50_251211-1650-trm论文精读笔记.md\":\"T6iy4inR\",\"00-inbox_week-50_251211-1650-vae原理.md\":\"tn1tCE3Z\",\"00-inbox_week-50_251211-1650-图像生成基座模型调研.md\":\"C3_ev7jC\",\"00-inbox_week-50_251211-1650-场景理解分类功能.md\":\"D6A0V50r\",\"00-inbox_week-50__251212-0031_index.md\":\"C3GQgXuF\",\"00-inbox_测试.md\":\"C7fhBSuK\",\"10-knowledge_250822-0737-重启训练一个半小时内不开始训-overview.md\":\"DTlEsSVk\",\"10-knowledge_250826-0737-i2i训练方案-overview.md\":\"BWqUgHb_\",\"10-knowledge_250826-0737-openuni训练方法-overview.md\":\"D5Pf2zFA\",\"10-knowledge_250827-0736-token熵-overview.md\":\"yMmqgiKV\",\"10-knowledge_250827-0736-扩散过程中打印数值-overview.md\":\"vw7Gz5_4\",\"10-knowledge_250828-0735-fm_transformers模型参数-overview.md\":\"wqj-DMMn\",\"10-knowledge_250829-0735-困惑度-overview.md\":\"msel50i0\",\"10-knowledge_250829-0735-惊讶度-overview.md\":\"DRmLapWg\",\"10-knowledge_250830-0735-pipeline检查-overview.md\":\"Dm1xkb3f\",\"10-knowledge_250903-0735-macos快捷操作.md\":\"D3c37vqB\",\"10-knowledge_250906-0735-vicreg监督.md\":\"Dd57c5Wc\",\"10-knowledge_250906-0735-投影层方案.md\":\"C22clrJL\",\"10-knowledge_250907-0735-特征解耦方案gpt参考.md\":\"QE9RuHLF\",\"10-knowledge_example-knowledge.md\":\"DWSfhqqG\",\"10-knowledge_index.md\":\"D7YErs8N\",\"10-knowledge_readme.md\":\"DX_wp92N\",\"20-papers_2025_2025-08-20-.md\":\"DCTyHO3m\",\"20-papers_2025_2025-08-20-vtla-preference-learning.md\":\"BizrrC37\",\"20-papers_250829-0735-qwen2.5-omni论文阅读.md\":\"Cx8gSJFV\",\"20-papers_index.md\":\"Rx3N4286\",\"20-papers_vla_all_papers.md\":\"CYgyoP_l\",\"30-ideas_2025_9月_250826-0737-与弘扬讨论-overview.md\":\"C6iq2em0\",\"30-ideas_2025_9月_250826-0737-现有模型结构可改进点-overview.md\":\"CR_pnMmR\",\"30-ideas_2025_9月_250827-0736-0826-与弘扬沟通-overview.md\":\"C2BHfEZx\",\"30-ideas_2025_9月_250829-0735-中间层模型方案梳理-overview.md\":\"QONbFOex\",\"30-ideas_2025_9月_250829-0735-论文理论包装方案.md\":\"D5ny1W3u\",\"30-ideas_2025_9月_250904-0735-多轮对话拓展idea-overview.md\":\"DogjKQ42\",\"30-ideas_2025_9月_250906-0735-0905_今日晨思.md\":\"CpoE1vCq\",\"30-ideas_2025_9月_250907-0735-梦梦的建议.md\":\"DPat3o-T\",\"30-ideas_2025_9月_week36-论文实验idea汇总.md\":\"DklZ5tYa\",\"30-ideas_index.md\":\"vC2LKruP\",\"40-experiments_250826-0736-0826测评日志.md\":\"DcuRq2z5\",\"40-experiments_250902-0735-0901实验分析.md\":\"CJ-n6KHc\",\"40-experiments_250904-0735-exp_card_0903_transencodder.md\":\"DngFnrmi\",\"40-experiments_250904-0735-实验日志模板.md\":\"z_bUPbrc\",\"40-experiments_exp_card_0908_dinov2.md\":\"aIgTjKuw\",\"40-experiments_index.md\":\"OjZdn4zK\",\"50-reports_250820-0754-ocr调研.md\":\"76hfXUhd\",\"50-reports_250827-0736-高熵强化学习.md\":\"BGjTIXbQ\",\"50-reports_250901-0735-resume.md\":\"Cht0KdOP\",\"50-reports_250903-0735-座舱vla端云协同方案.md\":\"BUGw-tzo\",\"50-reports_index.md\":\"Dd1uw3NR\",\"50-reports_resume_250901-0735-resume_network.md\":\"DkL0TB9W\",\"50-reports_座舱vla端云协同方案.md\":\"BoR0GFx-\",\"50-reports_文本扩散模型调研.md\":\"C_U4opUc\",\"50-reports_训练与部署资源申请书.md\":\"D_ztKl30\",\"50-reports_近期发布模型调研-0908.md\":\"bm38CHwU\",\"50-reports_长视频理解综述.md\":\"CEqqO3ga\",\"50-reports_高熵rl - 数据构建增益.md\":\"L_PhsjTX\",\"index.md\":\"DZVZkJJg\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"ZhiGrove\",\"description\":\"Wang Yaqi's Knowledge Base\",\"base\":\"/ZhiGrove/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"收件箱\",\"link\":\"/00-inbox/\"},{\"text\":\"知识库\",\"link\":\"/10-knowledge/\"},{\"text\":\"论文\",\"link\":\"/20-papers/\"},{\"text\":\"灵感\",\"link\":\"/30-ideas/\"},{\"text\":\"实验\",\"link\":\"/40-experiments/\"},{\"text\":\"报告\",\"link\":\"/50-reports/\"}],\"sidebar\":{\"/00-inbox/\":[{\"text\":\"Inbox\",\"items\":[{\"text\":\"README\",\"link\":\"/00-inbox/\"},{\"text\":\"测试\",\"link\":\"/00-inbox/测试\"},{\"text\":\"week-34\",\"collapsed\":true,\"items\":[{\"text\":\"TRIAGE\",\"link\":\"/00-inbox/week-34/TRIAGE\"}]},{\"text\":\"week-35\",\"collapsed\":true,\"items\":[{\"text\":\"TRIAGE\",\"link\":\"/00-inbox/week-35/TRIAGE\"}]},{\"text\":\"week-36\",\"collapsed\":true,\"items\":[{\"text\":\"TRIAGE\",\"link\":\"/00-inbox/week-36/TRIAGE\"}]},{\"text\":\"week-37\",\"collapsed\":true,\"items\":[{\"text\":\"250908-0020-MoE llama结构\",\"link\":\"/00-inbox/week-37/250908-0020-MoE llama结构\"},{\"text\":\"250908-0020-clip损失\",\"link\":\"/00-inbox/week-37/250908-0020-clip损失\"},{\"text\":\"250908-0020-siglip损失\",\"link\":\"/00-inbox/week-37/250908-0020-siglip损失\"},{\"text\":\"250909-0023-dino语义监督\",\"link\":\"/00-inbox/week-37/250909-0023-dino语义监督\"},{\"text\":\"250910-0024-clip-siglip-vicreg损失区别与适用场景\",\"link\":\"/00-inbox/week-37/250910-0024-clip-siglip-vicreg损失区别与适用场景\"},{\"text\":\"250910-0024-训练loss调试思路\",\"link\":\"/00-inbox/week-37/250910-0024-训练loss调试思路\"}]},{\"text\":\"week-38\",\"collapsed\":true,\"items\":[{\"text\":\"250918-0024-0916talk\",\"link\":\"/00-inbox/week-38/250918-0024-0916talk\"},{\"text\":\"250920-0022-check_code\",\"link\":\"/00-inbox/week-38/250920-0022-check_code\"}]},{\"text\":\"week-39\",\"collapsed\":true,\"items\":[{\"text\":\"250919-0024-0919-代码框架问题排查\",\"link\":\"/00-inbox/week-39/250919-0024-0919-代码框架问题排查\"},{\"text\":\"250924-0024-0922-ocr-TODOlist\",\"link\":\"/00-inbox/week-39/250924-0024-0922-ocr-TODOlist\"},{\"text\":\"250925-0025-0923-复现crossflow\",\"link\":\"/00-inbox/week-39/250925-0025-0923-复现crossflow\"},{\"text\":\"250925-0025-语义调音\",\"link\":\"/00-inbox/week-39/250925-0025-语义调音\"}]},{\"text\":\"week-41\",\"collapsed\":true,\"items\":[{\"text\":\"251010-0025-0922-ocr-TODOlist\",\"link\":\"/00-inbox/week-41/251010-0025-0922-ocr-TODOlist\"},{\"text\":\"251010-0025-0923-复现crossflow\",\"link\":\"/00-inbox/week-41/251010-0025-0923-复现crossflow\"},{\"text\":\"251010-0025-dots_ocr服务部署 & 调用\",\"link\":\"/00-inbox/week-41/251010-0025-dots_ocr服务部署 & 调用\"},{\"text\":\"251010-0025-laion400m训练数据处理\",\"link\":\"/00-inbox/week-41/251010-0025-laion400m训练数据处理\"},{\"text\":\"251010-0025-语义调音\",\"link\":\"/00-inbox/week-41/251010-0025-语义调音\"}]},{\"text\":\"week-45\",\"collapsed\":true,\"items\":[{\"text\":\"251027-1431-VLA-VLA调研分享\",\"link\":\"/00-inbox/week-45/251027-1431-VLA-VLA调研分享\"},{\"text\":\"251104-1431-OCR任务-docx等格式文档检测\",\"link\":\"/00-inbox/week-45/251104-1431-OCR任务-docx等格式文档检测\"},{\"text\":\"251104-1431-VLA-扩散生成调研\",\"link\":\"/00-inbox/week-45/251104-1431-VLA-扩散生成调研\"},{\"text\":\"251104-1431-VLA-综述论文节选翻译-0925发布\",\"link\":\"/00-inbox/week-45/251104-1431-VLA-综述论文节选翻译-0925发布\"}]},{\"text\":\"week-46\",\"collapsed\":true,\"items\":[{\"text\":\"251111-0027-k线记忆卡\",\"link\":\"/00-inbox/week-46/251111-0027-k线记忆卡\"}]},{\"text\":\"week-50\",\"collapsed\":true,\"items\":[{\"text\":\"251113-1650-自动交易策略实现log\",\"link\":\"/00-inbox/week-50/251113-1650-自动交易策略实现log\"},{\"text\":\"251211-1650-JiT论文-视频要点\",\"link\":\"/00-inbox/week-50/251211-1650-JiT论文-视频要点\"},{\"text\":\"251211-1650-TRM应用于VLA\",\"link\":\"/00-inbox/week-50/251211-1650-TRM应用于VLA\"},{\"text\":\"251211-1650-TRM应用大纲\",\"link\":\"/00-inbox/week-50/251211-1650-TRM应用大纲\"},{\"text\":\"251211-1650-TRM论文精读笔记\",\"link\":\"/00-inbox/week-50/251211-1650-TRM论文精读笔记\"},{\"text\":\"251211-1650-VAE原理\",\"link\":\"/00-inbox/week-50/251211-1650-VAE原理\"},{\"text\":\"251211-1650-deepseekV3_2\",\"link\":\"/00-inbox/week-50/251211-1650-deepseekV3_2\"},{\"text\":\"251211-1650-rustdesk安装及配置\",\"link\":\"/00-inbox/week-50/251211-1650-rustdesk安装及配置\"},{\"text\":\"251211-1650-图像生成基座模型调研\",\"link\":\"/00-inbox/week-50/251211-1650-图像生成基座模型调研\"},{\"text\":\"251211-1650-场景理解分类功能\",\"link\":\"/00-inbox/week-50/251211-1650-场景理解分类功能\"},{\"text\":\"[251212-0031]index\",\"link\":\"/00-inbox/week-50/[251212-0031]index\"}]}]}],\"/10-knowledge/\":[{\"text\":\"Knowledge\",\"items\":[{\"text\":\"README\",\"link\":\"/10-knowledge/\"},{\"text\":\"250822-0737-重启训练一个半小时内不开始训-overview\",\"link\":\"/10-knowledge/250822-0737-重启训练一个半小时内不开始训-overview\"},{\"text\":\"250826-0737-OpenUni训练方法-overview\",\"link\":\"/10-knowledge/250826-0737-OpenUni训练方法-overview\"},{\"text\":\"250826-0737-i2i训练方案-overview\",\"link\":\"/10-knowledge/250826-0737-i2i训练方案-overview\"},{\"text\":\"250827-0736-token熵-overview\",\"link\":\"/10-knowledge/250827-0736-token熵-overview\"},{\"text\":\"250827-0736-扩散过程中打印数值-overview\",\"link\":\"/10-knowledge/250827-0736-扩散过程中打印数值-overview\"},{\"text\":\"250828-0735-fm_transformers模型参数-overview\",\"link\":\"/10-knowledge/250828-0735-fm_transformers模型参数-overview\"},{\"text\":\"250829-0735-困惑度-overview\",\"link\":\"/10-knowledge/250829-0735-困惑度-overview\"},{\"text\":\"250829-0735-惊讶度-overview\",\"link\":\"/10-knowledge/250829-0735-惊讶度-overview\"},{\"text\":\"250830-0735-pipeline检查-overview\",\"link\":\"/10-knowledge/250830-0735-pipeline检查-overview\"},{\"text\":\"250903-0735-macos快捷操作\",\"link\":\"/10-knowledge/250903-0735-macos快捷操作\"},{\"text\":\"250906-0735-VICReg监督\",\"link\":\"/10-knowledge/250906-0735-VICReg监督\"},{\"text\":\"250906-0735-投影层方案\",\"link\":\"/10-knowledge/250906-0735-投影层方案\"},{\"text\":\"250907-0735-特征解耦方案GPT参考\",\"link\":\"/10-knowledge/250907-0735-特征解耦方案GPT参考\"},{\"text\":\"example-knowledge\",\"link\":\"/10-knowledge/example-knowledge\"}]}],\"/20-papers/\":[{\"text\":\"Papers\",\"items\":[{\"text\":\"README\",\"link\":\"/20-papers/\"},{\"text\":\"250829-0735-Qwen2.5-Omni论文阅读\",\"link\":\"/20-papers/250829-0735-Qwen2.5-Omni论文阅读\"},{\"text\":\"2025\",\"collapsed\":true,\"items\":[{\"text\":\"2025-08-20-\",\"link\":\"/20-papers/2025/2025-08-20-\"},{\"text\":\"2025-08-20-vtla-preference-learning\",\"link\":\"/20-papers/2025/2025-08-20-vtla-preference-learning\"}]},{\"text\":\"vla\",\"collapsed\":true,\"items\":[{\"text\":\"all_papers\",\"link\":\"/20-papers/vla/all_papers\"}]}]}],\"/30-ideas/\":[{\"text\":\"Ideas\",\"items\":[{\"text\":\"README\",\"link\":\"/30-ideas/\"},{\"text\":\"2025\",\"collapsed\":true,\"items\":[{\"text\":\"9月\",\"collapsed\":true,\"items\":[{\"text\":\"250826-0737-与弘扬讨论-overview\",\"link\":\"/30-ideas/2025/9月/250826-0737-与弘扬讨论-overview\"},{\"text\":\"250826-0737-现有模型结构可改进点-overview\",\"link\":\"/30-ideas/2025/9月/250826-0737-现有模型结构可改进点-overview\"},{\"text\":\"250827-0736-0826-与弘扬沟通-overview\",\"link\":\"/30-ideas/2025/9月/250827-0736-0826-与弘扬沟通-overview\"},{\"text\":\"250829-0735-中间层模型方案梳理-overview\",\"link\":\"/30-ideas/2025/9月/250829-0735-中间层模型方案梳理-overview\"},{\"text\":\"250829-0735-论文理论包装方案\",\"link\":\"/30-ideas/2025/9月/250829-0735-论文理论包装方案\"},{\"text\":\"250904-0735-多轮对话拓展idea-overview\",\"link\":\"/30-ideas/2025/9月/250904-0735-多轮对话拓展idea-overview\"},{\"text\":\"250906-0735-0905_今日晨思\",\"link\":\"/30-ideas/2025/9月/250906-0735-0905_今日晨思\"},{\"text\":\"250907-0735-梦梦的建议\",\"link\":\"/30-ideas/2025/9月/250907-0735-梦梦的建议\"},{\"text\":\"week36-论文实验idea汇总\",\"link\":\"/30-ideas/2025/9月/week36-论文实验idea汇总\"}]}]}]}],\"/40-experiments/\":[{\"text\":\"Experiments\",\"items\":[{\"text\":\"README\",\"link\":\"/40-experiments/\"},{\"text\":\"250826-0736-0826测评日志\",\"link\":\"/40-experiments/250826-0736-0826测评日志\"},{\"text\":\"250902-0735-0901实验分析\",\"link\":\"/40-experiments/250902-0735-0901实验分析\"},{\"text\":\"250904-0735-exp_card_0903_transencodder\",\"link\":\"/40-experiments/250904-0735-exp_card_0903_transencodder\"},{\"text\":\"250904-0735-实验日志模板\",\"link\":\"/40-experiments/250904-0735-实验日志模板\"},{\"text\":\"exp_card_0908_dinov2\",\"link\":\"/40-experiments/exp_card_0908_dinov2\"}]}],\"/50-reports/\":[{\"text\":\"Reports\",\"items\":[{\"text\":\"README\",\"link\":\"/50-reports/\"},{\"text\":\"250820-0754-OCR调研\",\"link\":\"/50-reports/250820-0754-OCR调研\"},{\"text\":\"250827-0736-高熵强化学习\",\"link\":\"/50-reports/250827-0736-高熵强化学习\"},{\"text\":\"250901-0735-resume\",\"link\":\"/50-reports/250901-0735-resume\"},{\"text\":\"250903-0735-座舱VLA端云协同方案\",\"link\":\"/50-reports/250903-0735-座舱VLA端云协同方案\"},{\"text\":\"座舱VLA端云协同方案\",\"link\":\"/50-reports/座舱VLA端云协同方案\"},{\"text\":\"文本扩散模型调研\",\"link\":\"/50-reports/文本扩散模型调研\"},{\"text\":\"训练与部署资源申请书\",\"link\":\"/50-reports/训练与部署资源申请书\"},{\"text\":\"近期发布模型调研-0908\",\"link\":\"/50-reports/近期发布模型调研-0908\"},{\"text\":\"长视频理解综述\",\"link\":\"/50-reports/长视频理解综述\"},{\"text\":\"高熵RL - 数据构建增益\",\"link\":\"/50-reports/高熵RL - 数据构建增益\"},{\"text\":\"resume\",\"collapsed\":true,\"items\":[{\"text\":\"250901-0735-resume_network\",\"link\":\"/50-reports/resume/250901-0735-resume_network\"}]}]}]},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/wangyaqi/ZhiGrove\"}],\"search\":{\"provider\":\"local\"},\"footer\":{\"message\":\"Released under the MIT License.\",\"copyright\":\"Copyright © 2025 Wang Yaqi\"}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>