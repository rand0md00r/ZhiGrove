<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>高熵强化学习 | ZhiGrove</title>
    <meta name="description" content="Wang Yaqi's Knowledge Base">
    <meta name="generator" content="VitePress v1.6.4">
    <link rel="preload stylesheet" href="/ZhiGrove/assets/style.C8nuTVcb.css" as="style">
    <link rel="preload stylesheet" href="/ZhiGrove/vp-icons.css" as="style">
    
    <script type="module" src="/ZhiGrove/assets/app.CXuj0tw-.js"></script>
    <link rel="preload" href="/ZhiGrove/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/ZhiGrove/assets/chunks/theme.DdaDK2L0.js">
    <link rel="modulepreload" href="/ZhiGrove/assets/chunks/framework.OaOo95RB.js">
    <link rel="modulepreload" href="/ZhiGrove/assets/50-reports_250827-0736-高熵强化学习.md.BGjTIXbQ.lean.js">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-5d98c3a5><!--[--><!--]--><!--[--><span tabindex="-1" data-v-0b0ada53></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0b0ada53>Skip to content</a><!--]--><!----><header class="VPNav" data-v-5d98c3a5 data-v-ae24b3ad><div class="VPNavBar" data-v-ae24b3ad data-v-6aa21345><div class="wrapper" data-v-6aa21345><div class="container" data-v-6aa21345><div class="title" data-v-6aa21345><div class="VPNavBarTitle has-sidebar" data-v-6aa21345 data-v-1168a8e4><a class="title" href="/ZhiGrove/" data-v-1168a8e4><!--[--><!--]--><!----><span data-v-1168a8e4>ZhiGrove</span><!--[--><!--]--></a></div></div><div class="content" data-v-6aa21345><div class="content-body" data-v-6aa21345><!--[--><!--]--><div class="VPNavBarSearch search" data-v-6aa21345><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-6aa21345 data-v-dc692963><span id="main-nav-aria-label" class="visually-hidden" data-v-dc692963> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/ZhiGrove/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>首页</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/ZhiGrove/00-inbox/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>收件箱</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/ZhiGrove/10-knowledge/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>知识库</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/ZhiGrove/20-papers/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>论文</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/ZhiGrove/30-ideas/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>灵感</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/ZhiGrove/40-experiments/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>实验</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/ZhiGrove/50-reports/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>报告</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-6aa21345 data-v-6c893767><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-6c893767 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-6aa21345 data-v-0394ad82 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/wangyaqi/ZhiGrove" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-6aa21345 data-v-bb2aa2f0 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-cf11d7a2><span class="vpi-more-horizontal icon" data-v-cf11d7a2></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><!----><!--[--><!--[--><!----><div class="group" data-v-bb2aa2f0><div class="item appearance" data-v-bb2aa2f0><p class="label" data-v-bb2aa2f0>Appearance</p><div class="appearance-action" data-v-bb2aa2f0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-bb2aa2f0 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div></div></div><div class="group" data-v-bb2aa2f0><div class="item social-links" data-v-bb2aa2f0><div class="VPSocialLinks social-links-list" data-v-bb2aa2f0 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/wangyaqi/ZhiGrove" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-6aa21345 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><div class="divider" data-v-6aa21345><div class="divider-line" data-v-6aa21345></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-5d98c3a5 data-v-a6f0e41e><div class="container" data-v-a6f0e41e><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-a6f0e41e><span class="vpi-align-left menu-icon" data-v-a6f0e41e></span><span class="menu-text" data-v-a6f0e41e>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-a6f0e41e data-v-8a42e2b4><button data-v-8a42e2b4>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-5d98c3a5 data-v-319d5ca6><div class="curtain" data-v-319d5ca6></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-319d5ca6><span class="visually-hidden" id="sidebar-aria-label" data-v-319d5ca6> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0 has-active" data-v-c40bc020 data-v-b3fd67f8><div class="item" role="button" tabindex="0" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><h2 class="text" data-v-b3fd67f8>Reports</h2><!----></div><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/ZhiGrove/50-reports/" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>README</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/ZhiGrove/50-reports/250820-0754-OCR%E8%B0%83%E7%A0%94.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>250820-0754-OCR调研</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/ZhiGrove/50-reports/250827-0736-%E9%AB%98%E7%86%B5%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>250827-0736-高熵强化学习</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/ZhiGrove/50-reports/250901-0735-resume.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>250901-0735-resume</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/ZhiGrove/50-reports/250903-0735-%E5%BA%A7%E8%88%B1VLA%E7%AB%AF%E4%BA%91%E5%8D%8F%E5%90%8C%E6%96%B9%E6%A1%88.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>250903-0735-座舱VLA端云协同方案</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/ZhiGrove/50-reports/%E5%BA%A7%E8%88%B1VLA%E7%AB%AF%E4%BA%91%E5%8D%8F%E5%90%8C%E6%96%B9%E6%A1%88.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>座舱VLA端云协同方案</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/ZhiGrove/50-reports/%E6%96%87%E6%9C%AC%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%A0%94.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>文本扩散模型调研</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/ZhiGrove/50-reports/%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%83%A8%E7%BD%B2%E8%B5%84%E6%BA%90%E7%94%B3%E8%AF%B7%E4%B9%A6.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>训练与部署资源申请书</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/ZhiGrove/50-reports/%E8%BF%91%E6%9C%9F%E5%8F%91%E5%B8%83%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%A0%94-0908.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>近期发布模型调研-0908</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/ZhiGrove/50-reports/%E9%95%BF%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3%E7%BB%BC%E8%BF%B0.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>长视频理解综述</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/ZhiGrove/50-reports/%E9%AB%98%E7%86%B5RL%20-%20%E6%95%B0%E6%8D%AE%E6%9E%84%E5%BB%BA%E5%A2%9E%E7%9B%8A.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>高熵RL - 数据构建增益</p><!--]--></a><!----></div><!----></div><section class="VPSidebarItem level-1 collapsible collapsed" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" role="button" tabindex="0" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><h3 class="text" data-v-b3fd67f8>resume</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b3fd67f8><span class="vpi-chevron-right caret-icon" data-v-b3fd67f8></span></div></div><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-2 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/ZhiGrove/50-reports/resume/250901-0735-resume_network.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>250901-0735-resume_network</p><!--]--></a><!----></div><!----></div><!--]--></div></section><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-5d98c3a5 data-v-1428d186><div class="VPDoc has-sidebar has-aside" data-v-1428d186 data-v-39a288b8><!--[--><!--]--><div class="container" data-v-39a288b8><div class="aside" data-v-39a288b8><div class="aside-curtain" data-v-39a288b8></div><div class="aside-container" data-v-39a288b8><div class="aside-content" data-v-39a288b8><div class="VPDocAside" data-v-39a288b8 data-v-3f215769><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3f215769 data-v-a5bbad30><div class="content" data-v-a5bbad30><div class="outline-marker" data-v-a5bbad30></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-a5bbad30>On this page</div><ul class="VPDocOutlineItem root" data-v-a5bbad30 data-v-b933a997><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-39a288b8><div class="content-container" data-v-39a288b8><!--[--><!--]--><main class="main" data-v-39a288b8><div style="position:relative;" class="vp-doc _ZhiGrove_50-reports_250827-0736-%E9%AB%98%E7%86%B5%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0" data-v-39a288b8><div><h2 id="raw-notes" tabindex="-1">Raw Notes <a class="header-anchor" href="#raw-notes" aria-label="Permalink to &quot;Raw Notes&quot;">​</a></h2><p>下面是一份围绕“三篇论文 + 相关工作”的精炼调研报告，重点交代<strong>token 熵的精确定义与实现细节</strong>，并将各路线的目标函数、训练管线与优缺点对照起来，便于直接落地到你的强化学习后训练流程中。</p><h1 id="一、核心概念-token-熵是什么、为何要在-rl-后训练中关注它" tabindex="-1">一、核心概念：token 熵是什么、为何要在 RL 后训练中关注它 <a class="header-anchor" href="#一、核心概念-token-熵是什么、为何要在-rl-后训练中关注它" aria-label="Permalink to &quot;一、核心概念：token 熵是什么、为何要在 RL 后训练中关注它&quot;">​</a></h1><p><strong>token 级生成熵</strong>（generation entropy）刻画模型在<strong>当前位置</strong>对下一个词元的犹豫程度。对输入 $q$ 与已生成前缀 $o_{&lt;t}$，当前策略 $\pi_\theta$ 的词表分布为</p><p>$$ p_t = \pi_\theta(\cdot \mid q, o_{&lt;t})=\mathrm{Softmax}(z_t), $$</p><p>其<strong>token 熵</strong>定义为</p><p>$$ H_t ;=; -\sum_{j=1}^{V} p_{t,j},\log p_{t,j}. $$</p><p>该定义直接来自论文(等式(1))，强调“<strong>熵属于位置 $t$ 的分布</strong>而不是被采样出来的具体 token 本身”。实现上就是对当前前向得到的 logits $z_t$ 做一次 softmax + 向量级的 $-(p\cdot\log p)$ 规约。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</p><p>关注 token 熵的动机：在 CoT 推理中，<strong>大多数 token 是低熵的“续写/拼写”</strong>，而<strong>少数高熵 token 是“分岔点/承上启下”</strong>，决定思路转折与步骤衔接。RL 的收益几乎都发生在这些“高熵分岔点”上。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</p><h1 id="二、三篇论文的要点与它们-怎样用到熵" tabindex="-1">二、三篇论文的要点与它们“怎样用到熵” <a class="header-anchor" href="#二、三篇论文的要点与它们-怎样用到熵" aria-label="Permalink to &quot;二、三篇论文的要点与它们“怎样用到熵”&quot;">​</a></h1><h2 id="_1-《beyond-the-80-20-rule-high-entropy-minority-tokens-》-高熵少数派" tabindex="-1">1) 《Beyond the 80/20 Rule: High-Entropy Minority Tokens…》(高熵少数派) <a class="header-anchor" href="#_1-《beyond-the-80-20-rule-high-entropy-minority-tokens-》-高熵少数派" aria-label="Permalink to &quot;1) 《Beyond the 80/20 Rule: High-Entropy Minority Tokens…》(高熵少数派)&quot;">​</a></h2><p><strong>思想</strong>：只在<strong>高熵 token</strong>处更新策略梯度，<strong>屏蔽底部 80% 低熵 token</strong> 的梯度——“用 20% token 训练也不掉点，甚至更好”。 <strong>目标函数</strong>（DAPO 框架下的改动，批内 Top-ρ 选取）： 对一个训练批 $B$，求最大化</p><p>$$ J^{\text{HighEnt}}<em i="">B(\theta) =\mathbb{E}!\left[ \frac{1}{\sum_i |o_i|}\sum</em>\sum_{t=1}^{|o_i|} \mathbf{1}[H^i_t \ge \tau^{B}<em>\rho]\cdot \min!\big(r^i_t(\theta)\hat A^i_t,,\mathrm{clip}(r^i_t(\theta), 1-\epsilon</em>{\text{low}}, 1+\epsilon_{\text{high}})\hat A^i_t\big) \right], $$</p><p>其中 $\tau^{B}<em>\rho$ 是<strong>在该（微）批所有 token 的熵上取 Top-ρ 分位的阈值</strong>，仅保留满足 $H^i_t \ge \tau^{B}</em>\rho$ 的 token 进入损失与反传；$\epsilon_{\text{high}}$ 采用 <strong>Clip-Higher</strong>（上界放宽，如 0.28）以鼓励探索。实现上只需在构造优势时加一层 <strong>indicator mask</strong>。文中常用 $\rho=20%$。该做法在 Qwen3-32B/14B 上显著提升，在 8B 上持平。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</p><p><strong>关键实现细节（必须点）</strong></p><ul><li><strong>熵的计算</strong>：用<strong>当前策略</strong>的前向 logits 计算 $H_t$，不需要温度缩放（默认 $T=1$）。（见等式(1)与 5.1 节）(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>分位阈值</strong>：在**（微）批维度**上把所有 token 的 $H_t$ 拼起来求分位数 $\tau^{B}_\rho$，得到布尔 mask（True 表示 Top-ρ 的高熵 token）。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>只改动 PG 分量</strong>：把 $\mathbf{1}[\cdot]$ 乘到优势上即可，其余（如 clip-higher、动态采样、overlong 奖励等）与 DAPO 配方一致；<strong>不引入 KL 或额外 entropy bonus</strong>。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>经验观察</strong>：$\rho$ 在 10–50% 区间相对鲁棒，但用 100% 会恶化（因为把大量低熵续写 token 也纳入 PG，等价于“稀释”了有效学习信号）。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</li></ul><h2 id="_2-《reasoning-with-exploration-an-entropy-perspective》" tabindex="-1">2) 《Reasoning with Exploration: An Entropy Perspective》 <a class="header-anchor" href="#_2-《reasoning-with-exploration-an-entropy-perspective》" aria-label="Permalink to &quot;2) 《Reasoning with Exploration: An Entropy Perspective》&quot;">​</a></h2><p><strong>思想</strong>：不是“掩码掉谁”，而是<strong>把熵直接注入优势</strong>，鼓励处于高不确定处的<strong>更深/更长</strong>探索；与一般“熵正则（增大不确定性）”不同，本工作通过<strong>优势塑形</strong>来促使模型在关键处更有把握。 <strong>做法（“一行代码”）</strong>： 计算常规优势 $\text{adv}$（PPO 或 GRPO），再加一项<strong>截断且</strong> <strong>detached</strong> 的熵项：</p><p>$$ \tilde{\text{adv}}_t = \text{adv}_t ;+; \min!\big(\alpha\cdot H_t^{\text{detach}},, |\text{adv}_t|/\kappa\big), $$</p><p>再用 $\tilde{\text{adv}}$ 进入标准 PPO/GRPO 的 clipped-surrogate；这样<strong>不改变梯度方向</strong>（熵项不反传），只是放大高熵位置的步长，并且用 $\kappa$ 防止过度放大/翻转符号。论文明确给出 PyTorch 伪“一行”插入点（veRL 框架的 dp_actor）。(<a href="https://www.ar5iv.org/pdf/2506.14758v2" title="[2506.14758] Reasoning with Exploration: An Entropy Perspective" target="_blank" rel="noreferrer">ar5iv</a>)</p><p><strong>关键实现细节（必须点）</strong></p><ul><li><strong>$\alpha,\kappa$ 两个超参</strong>：$\alpha$ 控制熵项强度；$\kappa$ 用于截断，保证 $\alpha H_t \le |\text{adv}_t|/\kappa$ 时不会反向或主导更新。(<a href="https://www.ar5iv.org/pdf/2506.14758v2" title="[2506.14758] Reasoning with Exploration: An Entropy Perspective" target="_blank" rel="noreferrer">ar5iv</a>)</li><li><strong>为何要 detach</strong>：避免把“增大熵”作为优化目标；这里是<strong>用熵做路标</strong>而非 regularizer。随着训练置信度提升，$H_t$ 下降，熵加成会<strong>自衰减</strong>，从而避免后期过探索。(<a href="https://www.ar5iv.org/pdf/2506.14758v2" title="[2506.14758] Reasoning with Exploration: An Entropy Perspective" target="_blank" rel="noreferrer">ar5iv</a>)</li><li><strong>与 GRPO/PPO 的兼容</strong>：仅替换优势，剩下的剪切比、KL（若有）等策略不变；理论与实现都保持最小侵入。(<a href="https://www.ar5iv.org/pdf/2506.14758v2" title="[2506.14758] Reasoning with Exploration: An Entropy Perspective" target="_blank" rel="noreferrer">ar5iv</a>)</li></ul><h2 id="_3-《fr3e-first-return-entropy-eliciting-explore》" tabindex="-1">3) 《FR3E: First Return, Entropy-Eliciting Explore》 <a class="header-anchor" href="#_3-《fr3e-first-return-entropy-eliciting-explore》" aria-label="Permalink to &quot;3) 《FR3E: First Return, Entropy-Eliciting Explore》&quot;">​</a></h2><p><strong>思想</strong>：把高熵位置当作<strong>锚点</strong>，按“<strong>先回到正确轨迹（First Return）—再从高熵处</strong>做<strong>定向展开</strong>（Explore）”的两阶段结构化探索，构造<strong>局部中间反馈</strong>，改进 credit assignment 与探索稳定性。(<a href="https://arxiv.org/html/2507.07017v1" title="First Return, Entropy-Eliciting Explore" target="_blank" rel="noreferrer">arXiv</a>)</p><p><strong>做法</strong>：</p><ol><li>用当前策略生成<strong>基准轨迹</strong>，沿途计算 $H_t$；选择<strong>全局 Top-K 高熵位置</strong>作为<strong>分块边界</strong>，形成“语义块/中间状态”。</li><li>从这些锚点做<strong>局部 rollouts</strong>，用可验证的结果给出<strong>经验价值</strong> $V(\text{prefix})$，再配以<strong>自适应优势调制系数</strong> $\gamma\big(\Delta V\big)$ 来稳住学习（进步小则放大、进步大则收敛时缩小）。(<a href="https://arxiv.org/html/2507.07017v1" title="First Return, Entropy-Eliciting Explore" target="_blank" rel="noreferrer">arXiv</a>)</li></ol><p><strong>关键实现细节（必须点）</strong></p><ul><li><strong>熵的用途</strong>：只用于<strong>定位高不确定决策点</strong>（Top-K）及<strong>分块</strong>；损失里不直接加熵项。等式中给出分布 $p_t$ 与熵 $H_t$ 的标准定义与<strong>Top-K 选点</strong>规则。(<a href="https://arxiv.org/html/2507.07017v1" title="First Return, Entropy-Eliciting Explore" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>计算开销</strong>：相较普通 RLVR，多了<strong>从若干锚点起的局部扩展</strong>与评估；但这些扩展是“定向”的，采样效率高于盲目的全局探索。(<a href="https://arxiv.org/html/2507.07017v1" title="First Return, Entropy-Eliciting Explore" target="_blank" rel="noreferrer">arXiv</a>)</li></ul><h1 id="三、与三篇论文相关的其它路线-聚焦-熵-探索-credit-assignment-配方" tabindex="-1">三、与三篇论文相关的其它路线（聚焦“熵/探索/credit assignment/配方”） <a class="header-anchor" href="#三、与三篇论文相关的其它路线-聚焦-熵-探索-credit-assignment-配方" aria-label="Permalink to &quot;三、与三篇论文相关的其它路线（聚焦“熵/探索/credit assignment/配方”）&quot;">​</a></h1><ul><li><strong>Clip-Higher 与 DAPO 配方</strong>：上界放宽（如 $1+\epsilon_{\text{high}}=1.28$）能在不破坏稳定性的前提下<strong>更敢于把低概率“探索 token”抬起来</strong>，是近来 RLVR 成功的关键配方之一（本文 1) 也沿用）。DAPO 公开报告详述了动机与实现。(<a href="https://arxiv.org/pdf/2503.14476?utm_source=chatgpt.com" title="DAPO: An Open-Source LLM Reinforcement Learning ..." target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>GRPO 及其实现要点</strong>：不训练 critic，用同一问题的多样本平均奖励作基线，token-level PPO 损失+可选 KL；veRL 文档提供了工程接口。(<a href="https://verl.readthedocs.io/en/latest/algo/grpo.html?utm_source=chatgpt.com" title="Group Relative Policy Optimization (GRPO) - verl documentation" target="_blank" rel="noreferrer">Verl</a>)</li><li><strong>熵可控的 DPO（H-DPO）</strong>：在<strong>偏好优化</strong>（无显式 RL）里，通过修改 DPO 的<strong>反向 KL 正则</strong>来<strong>控制策略熵/锐度</strong>，与上面“在 RL 中用熵”形成互补。实现改动仅在损失计算处，实验显示对数学任务的 pass@k 有提升。(<a href="https://arxiv.org/abs/2411.07595" title="[2411.07595] Entropy Controllable Direct Preference Optimization" target="_blank" rel="noreferrer">arXiv</a>, <a href="https://ar5iv.org/pdf/2411.07595" title="[2411.07595] Entropy Controllable Direct Preference Optimization" target="_blank" rel="noreferrer">ar5iv</a>)</li><li><strong>ETPO（Entropy-Regularized Token-level Policy Optimization）</strong>：把<strong>软 Bellman</strong>与<strong>策略更新</strong>都下沉到<strong>token 粒度</strong>，从理论到算法完整地把“熵正则”引入到 token 级 RL。适合交互式环境（如代码代理），而非典型的离线 RLVR，但在“<strong>token 级 credit assignment + 熵</strong>”上与三篇论文一脉相承。(<a href="https://arxiv.org/html/2402.06700v1" title="Entropy-Regularized Token-Level Policy Optimization for Large Language Models" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>过程奖励与中间反馈</strong>（与 FR3E 目标相近）：VinePPO（无价值网络、MC 估计中间价值）、PRIME（只用结果标签学<strong>隐式过程奖励</strong>，得到稠密过程信号）、S-GRPO（串行组+衰减奖励，引导“更早更好”的思考退出），都是<strong>加强中间监督/credit assignment</strong>的代表，与“熵选点/结构化探索”可以组合。(<a href="https://arxiv.org/html/2410.01679v2?utm_source=chatgpt.com" title="VinePPO: Refining Credit Assignment in RL Training of LLMs" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>拒绝采样基线（RAFT / Reinforce-Rej）</strong>：近期工作显示“仅用正样本做再训练”的简单基线已能逼近乃至超过部分 RL 配方，提示我们应谨慎评估熵驱动探索的<strong>相对收益</strong>。(<a href="https://arxiv.org/html/2504.11343v1?utm_source=chatgpt.com" title="A Minimalist Approach to LLM Reasoning: from Rejection ..." target="_blank" rel="noreferrer">arXiv</a>)</li></ul><h1 id="四、token-熵——工程侧-怎么做才对" tabindex="-1">四、token 熵——工程侧“怎么做才对” <a class="header-anchor" href="#四、token-熵——工程侧-怎么做才对" aria-label="Permalink to &quot;四、token 熵——工程侧“怎么做才对”&quot;">​</a></h1><blockquote><p>下面把“算 $H_t$”“怎么选 Top-ρ/Top-K”“放到损失/优势里”的实现细节一次讲清。</p></blockquote><h2 id="a-计算-h-t-一行向量规约" tabindex="-1">A. 计算 $H_t$：一行向量规约 <a class="header-anchor" href="#a-计算-h-t-一行向量规约" aria-label="Permalink to &quot;A. 计算 $H_t$：一行向量规约&quot;">​</a></h2><ul><li><strong>前向</strong>得到 logits $z_t$（你本来为计算 log-prob/比值 $r_t$ 就会算）。</li><li><strong>Softmax</strong> 得 $p_t$，常规训练温度 $T=1$（除非你显式做温度缩放）。</li><li><strong>规约</strong>：<code>H_t = -(p_t * (p_t.log())).sum(dim=-1)</code>；注意对 <strong>padding/EOS</strong> 做 mask，不把它们纳入统计/更新。该定义与三篇论文一致（见 1) 的式(1)，2) 的式(4)，3) 的式(5)）。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>, <a href="https://www.ar5iv.org/pdf/2506.14758v2" title="[2506.14758] Reasoning with Exploration: An Entropy Perspective" target="_blank" rel="noreferrer">ar5iv</a>)</li></ul><h2 id="b-top-ρ-批内阈值-top-k-序列级-选点" tabindex="-1">B. Top-ρ（批内阈值）/Top-K（序列级）选点 <a class="header-anchor" href="#b-top-ρ-批内阈值-top-k-序列级-选点" aria-label="Permalink to &quot;B. Top-ρ（批内阈值）/Top-K（序列级）选点&quot;">​</a></h2><ul><li><strong>批内 Top-ρ（高熵少数派）</strong>：把<strong>一个（微）批</strong>中所有 token 的 $H_t$ 拉平成一维，做 <code>quantile(1-ρ)</code> 得 $\tau^B_\rho$，得到布尔 mask：<code>mask = (H &gt;= tau)；adv *= mask</code>。论文明确写作 $\tau^B_\rho$，是<strong>在批级别</strong>求阈值而非逐序列。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>序列级 Top-K（FR3E）</strong>：对<strong>单条轨迹</strong>取全局 Top-K 的位置（“高不确定决策点”）作为<strong>锚点/分块边界</strong>，后续仅在这些锚点上做局部 rollouts。(<a href="https://arxiv.org/html/2507.07017v1" title="First Return, Entropy-Eliciting Explore" target="_blank" rel="noreferrer">arXiv</a>)</li></ul><h2 id="c-把熵-接-进优化" tabindex="-1">C. 把熵“接”进优化 <a class="header-anchor" href="#c-把熵-接-进优化" aria-label="Permalink to &quot;C. 把熵“接”进优化&quot;">​</a></h2><ul><li><p><strong>“掩码式”接法（高熵少数派）</strong>：在 token-level PPO/DAPO 损失里，把优势乘上 <code>mask</code>，实现“<strong>只在高熵处学</strong>”。<strong>其余配方不变</strong>（clip-higher/采样/overlong 奖励等保持一致；论文实验中不使用 KL/entropy bonus）。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</p></li><li><p><strong>“优势塑形”接法（熵视角）</strong>：</p><p>$$ \tilde{\text{adv}}=\text{adv}+\min(\alpha\cdot H^{\text{detach}},, |\text{adv}|/\kappa), $$</p><p>其中 <code>H.detach()</code> <strong>不回传梯度</strong>，只改变步长大小与优先级；<code>min</code> 截断确保不会翻转 $\text{adv}$ 的符号。对应 veRL 的实现点就是<strong>在计算完 adv 后加一行</strong>再入损失。(<a href="https://www.ar5iv.org/pdf/2506.14758v2" title="[2506.14758] Reasoning with Exploration: An Entropy Perspective" target="_blank" rel="noreferrer">ar5iv</a>)</p></li><li><p><strong>“结构化探索”接法（FR3E）</strong>：熵只用于<strong>定位锚点</strong>；真正进入损失的是锚点处展开得到的<strong>经验价值/优势</strong>，配上<strong>自适应缩放因子</strong>控制学习强度，进而稳定训练而不过度早收敛。(<a href="https://arxiv.org/html/2507.07017v1" title="First Return, Entropy-Eliciting Explore" target="_blank" rel="noreferrer">arXiv</a>)</p></li></ul><h2 id="d-与常见正则-配方的交互" tabindex="-1">D. 与常见正则/配方的交互 <a class="header-anchor" href="#d-与常见正则-配方的交互" aria-label="Permalink to &quot;D. 与常见正则/配方的交互&quot;">​</a></h2><ul><li><strong>不要把“熵优势塑形”与“显式熵正则”混为一谈</strong>：前者是<strong>不回传</strong>的路标信号，后者会直接驱动“变得更不确定”。论文指出后者在 CoT 中会<strong>伤害低熵多数 token 的确定性</strong>，不如 clip-higher 这类“更关注高熵少数”的做法。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>与 KL 正则</strong>：1) 中实验不加 KL 亦可稳定；2) 若保留 KL，建议先只引入<strong>优势塑形</strong>或<strong>Top-ρ 掩码</strong>中的一个，逐步网格 $\alpha,\kappa,\rho$；3) 与 clip-higher 的配合通常更自然（高比值 token 往往也是高熵 token）。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</li></ul><h1 id="五、对比与实践建议" tabindex="-1">五、对比与实践建议 <a class="header-anchor" href="#五、对比与实践建议" aria-label="Permalink to &quot;五、对比与实践建议&quot;">​</a></h1><table tabindex="0"><thead><tr><th>路线</th><th>用熵做什么</th><th>代价</th><th>何时更合适</th><th>主要风险/调参点</th></tr></thead><tbody><tr><td><strong>高熵少数派（Top-ρ）</strong></td><td>只在高熵 token 反传</td><td>极低（仅一层 mask）</td><td>你已用 DAPO/GRPO，想<strong>减噪提效</strong></td><td>ρ 过大→“稀释”；过小→样本不足。建议 10–30% 起试。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</td></tr><tr><td><strong>熵优势塑形</strong></td><td>放大高熵处的优势（detach + 截断）</td><td>极低（一行）</td><td>想保留<strong>全部 token</strong>训练又突出“分岔点”</td><td>$\alpha,\kappa$ 需网格；$\kappa$ 太小会过放大。(<a href="https://www.ar5iv.org/pdf/2506.14758v2" title="[2506.14758] Reasoning with Exploration: An Entropy Perspective" target="_blank" rel="noreferrer">ar5iv</a>)</td></tr><tr><td><strong>FR3E</strong></td><td>高熵定位锚点 + 定向 rollouts</td><td>中等（局部展开）</td><td>更关注<strong>中间反馈/credit assignment</strong></td><td>K 过大开销上升；需要设计锚点数量与展开步数。(<a href="https://arxiv.org/html/2507.07017v1" title="First Return, Entropy-Eliciting Explore" target="_blank" rel="noreferrer">arXiv</a>)</td></tr><tr><td><strong>H-DPO</strong></td><td>在 DPO 中直接控熵/锐度</td><td>低（改损失）</td><td>偏好优化场景或想少调 RL 环节</td><td>与 RLVR 不同范式；正则强度需小心。(<a href="https://ar5iv.org/pdf/2411.07595" title="[2411.07595] Entropy Controllable Direct Preference Optimization" target="_blank" rel="noreferrer">ar5iv</a>)</td></tr><tr><td><strong>ETPO</strong></td><td>token-级软 Bellman + 熵正则</td><td>较高（算法更重）</td><td>交互式/在线环境</td><td>工程复杂、与 RLVR 评测口径不同。(<a href="https://arxiv.org/html/2402.06700v1" title="Entropy-Regularized Token-Level Policy Optimization for Large Language Models" target="_blank" rel="noreferrer">arXiv</a>)</td></tr></tbody></table><h1 id="六、最小可落地清单-工程角度" tabindex="-1">六、最小可落地清单（工程角度） <a class="header-anchor" href="#六、最小可落地清单-工程角度" aria-label="Permalink to &quot;六、最小可落地清单（工程角度）&quot;">​</a></h1><ol><li><strong>度量熵</strong>：在生成/训练前向里，取 <code>p = softmax(logits)</code>，<code>H = -(p * log(p)).sum(-1)</code>；屏蔽 PAD/EOS。三篇论文都使用该实现。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>, <a href="https://www.ar5iv.org/pdf/2506.14758v2" title="[2506.14758] Reasoning with Exploration: An Entropy Perspective" target="_blank" rel="noreferrer">ar5iv</a>)</li><li><strong>批内 Top-ρ 掩码</strong>（若采用高熵少数派）：把批内所有 token 的 <code>H</code> 拼起来做 <code>quantile</code> 得阈值 $\tau^B_\rho$，生成 <code>mask</code> 乘到优势或损失上。ρ=0.2 常作为强基线。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>优势塑形“一行”</strong>（若采用熵视角）：<code>adv += min(alpha * H.detach(), adv.abs()/kappa)</code>；从 $\alpha\in[0.05,0.2]$、$\kappa\in[2,8]$ 小步扫描。(<a href="https://www.ar5iv.org/pdf/2506.14758v2" title="[2506.14758] Reasoning with Exploration: An Entropy Perspective" target="_blank" rel="noreferrer">ar5iv</a>)</li><li><strong>结构化探索（FR3E）</strong>：对每条样本<strong>先</strong>生成一条“基准正确轨迹”（或较优轨迹），按 Top-K $H_t$ 定锚、分块；再从各锚点做若干次<strong>局部</strong> rollouts 评估并计算经验价值，最后以自适应优势调制进入策略更新。(<a href="https://arxiv.org/html/2507.07017v1" title="First Return, Entropy-Eliciting Explore" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>与配方的搭配</strong>：Clip-Higher（如 $\epsilon_{\text{high}}\approx0.28$）+ token-level PPO/GRPO 是当前社区默认强基线；在此之上引入 <strong>(2) 或 (3)</strong> 的熵机制，通常更稳。(<a href="https://arxiv.org/pdf/2503.14476?utm_source=chatgpt.com" title="DAPO: An Open-Source LLM Reinforcement Learning ..." target="_blank" rel="noreferrer">arXiv</a>, <a href="https://verl.readthedocs.io/en/latest/algo/grpo.html?utm_source=chatgpt.com" title="Group Relative Policy Optimization (GRPO) - verl documentation" target="_blank" rel="noreferrer">Verl</a>)</li></ol><hr><h1 id="参考与延伸阅读-精选" tabindex="-1">参考与延伸阅读（精选） <a class="header-anchor" href="#参考与延伸阅读-精选" aria-label="Permalink to &quot;参考与延伸阅读（精选）&quot;">​</a></h1><ul><li><strong>高熵少数派（Top-ρ 掩码）</strong>：熵定义、Top-ρ 阈值与目标式(6)，及“不加 KL/entropy bonus”的配方与结果。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>熵优势塑形</strong>：熵定义(式(4))、优势塑形(式(5)(6))、“一行实现”与理论动机。(<a href="https://www.ar5iv.org/pdf/2506.14758v2" title="[2506.14758] Reasoning with Exploration: An Entropy Perspective" target="_blank" rel="noreferrer">ar5iv</a>)</li><li><strong>FR3E</strong>：熵计算与 Top-K 选点、分块/中间状态构造、锚点局部 rollouts 与自适应优势调制。(<a href="https://arxiv.org/html/2507.07017v1" title="First Return, Entropy-Eliciting Explore" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>Clip-Higher / DAPO</strong>：为何放宽上剪切能鼓励探索 token；DAPO 全配方与公开报告。(<a href="https://arxiv.org/pdf/2503.14476?utm_source=chatgpt.com" title="DAPO: An Open-Source LLM Reinforcement Learning ..." target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>GRPO 实现简介</strong>（veRL 文档）：组采样、群内相对优势与无 critic 策略。(<a href="https://verl.readthedocs.io/en/latest/algo/grpo.html?utm_source=chatgpt.com" title="Group Relative Policy Optimization (GRPO) - verl documentation" target="_blank" rel="noreferrer">Verl</a>)</li><li><strong>H-DPO</strong>：在偏好优化中显式<strong>控制策略熵</strong>（非 RLVR）。(<a href="https://arxiv.org/abs/2411.07595" title="[2411.07595] Entropy Controllable Direct Preference Optimization" target="_blank" rel="noreferrer">arXiv</a>, <a href="https://ar5iv.org/pdf/2411.07595" title="[2411.07595] Entropy Controllable Direct Preference Optimization" target="_blank" rel="noreferrer">ar5iv</a>)</li><li><strong>ETPO</strong>：token-级软 Bellman + 熵正则的理论-实践框架。(<a href="https://arxiv.org/html/2402.06700v1" title="Entropy-Regularized Token-Level Policy Optimization for Large Language Models" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>VinePPO / PRIME / S-GRPO</strong>：改进 credit assignment 与中间奖励的代表工作，可与“熵锚点/Top-ρ/优势塑形”互补。(<a href="https://arxiv.org/html/2410.01679v2?utm_source=chatgpt.com" title="VinePPO: Refining Credit Assignment in RL Training of LLMs" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>拒绝采样基线（RAFT / Reinforce-Rej）</strong>：提醒评测要对齐与做充足消融。(<a href="https://arxiv.org/html/2504.11343v1?utm_source=chatgpt.com" title="A Minimalist Approach to LLM Reasoning: from Rejection ..." target="_blank" rel="noreferrer">arXiv</a>)</li></ul><blockquote><p>如果你要把这些机制接到现有 veRL/GRPO++ 训练脚本里： <strong>Top-ρ</strong> 属于“优势前的掩码”；<strong>熵优势塑形</strong>属于“计算完优势后一行相加（detach+截断）”；<strong>FR3E</strong> 需在采样器里加入“按高熵锚点做局部展开与评估”的流程，随后把得到的“锚点价值/优势”写回策略更新。以上三者均不要求你改动模型架构，只是调整<strong>优势/采样/反传范围</strong>与<strong>采样策略</strong>。</p></blockquote></div></div></main><footer class="VPDocFooter" data-v-39a288b8 data-v-e257564d><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-e257564d><span class="visually-hidden" id="doc-footer-aria-label" data-v-e257564d>Pager</span><div class="pager" data-v-e257564d><a class="VPLink link pager-link prev" href="/ZhiGrove/50-reports/250820-0754-OCR%E8%B0%83%E7%A0%94.html" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Previous page</span><span class="title" data-v-e257564d>250820-0754-OCR调研</span><!--]--></a></div><div class="pager" data-v-e257564d><a class="VPLink link pager-link next" href="/ZhiGrove/50-reports/250901-0735-resume.html" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Next page</span><span class="title" data-v-e257564d>250901-0735-resume</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-5d98c3a5 data-v-e315a0ad><div class="container" data-v-e315a0ad><p class="message" data-v-e315a0ad>Released under the MIT License.</p><p class="copyright" data-v-e315a0ad>Copyright © 2025 Wang Yaqi</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"00-inbox_index.md\":\"DLcZKgze\",\"00-inbox_readme.md\":\"CROMh2To\",\"00-inbox_week-34_triage.md\":\"Bd21h7VP\",\"00-inbox_week-35_triage.md\":\"vLr3Y60-\",\"00-inbox_week-36_triage.md\":\"D2WG-JFq\",\"00-inbox_week-37_250908-0020-clip损失.md\":\"BF9Ao-im\",\"00-inbox_week-37_250908-0020-moe llama结构.md\":\"DoV69DcU\",\"00-inbox_week-37_250908-0020-siglip损失.md\":\"tNmo36ye\",\"00-inbox_week-37_250909-0023-dino语义监督.md\":\"COxAmtT6\",\"00-inbox_week-37_250910-0024-clip-siglip-vicreg损失区别与适用场景.md\":\"DiCvqywx\",\"00-inbox_week-37_250910-0024-训练loss调试思路.md\":\"av_Unh76\",\"00-inbox_week-38_250918-0024-0916talk.md\":\"CAT-H946\",\"00-inbox_week-38_250920-0022-check_code.md\":\"BLX1z2V6\",\"00-inbox_week-39_250919-0024-0919-代码框架问题排查.md\":\"B6Wv4tLA\",\"00-inbox_week-39_250924-0024-0922-ocr-todolist.md\":\"OJ38MYqH\",\"00-inbox_week-39_250925-0025-0923-复现crossflow.md\":\"i5f0OFHX\",\"00-inbox_week-39_250925-0025-语义调音.md\":\"DITAIE7L\",\"00-inbox_week-41_251010-0025-0922-ocr-todolist.md\":\"DZPDtC9H\",\"00-inbox_week-41_251010-0025-0923-复现crossflow.md\":\"BFR0ZHnn\",\"00-inbox_week-41_251010-0025-dots_ocr服务部署 _ 调用.md\":\"zVyuBHXn\",\"00-inbox_week-41_251010-0025-laion400m训练数据处理.md\":\"1RHkz-kk\",\"00-inbox_week-41_251010-0025-语义调音.md\":\"lKF60PSI\",\"00-inbox_week-45_251027-1431-vla-vla调研分享.md\":\"iFv3PXRc\",\"00-inbox_week-45_251104-1431-ocr任务-docx等格式文档检测.md\":\"YRWX1WUF\",\"00-inbox_week-45_251104-1431-vla-扩散生成调研.md\":\"CMpVtTg7\",\"00-inbox_week-45_251104-1431-vla-综述论文节选翻译-0925发布.md\":\"DU4Vlch_\",\"00-inbox_week-46_251111-0027-k线记忆卡.md\":\"DuXtCsiw\",\"00-inbox_week-50_251113-1650-自动交易策略实现log.md\":\"DjqNSPf3\",\"00-inbox_week-50_251211-1650-deepseekv3_2.md\":\"Bko8VItX\",\"00-inbox_week-50_251211-1650-jit论文-视频要点.md\":\"DHEX-o3m\",\"00-inbox_week-50_251211-1650-rustdesk安装及配置.md\":\"w8RvI8-8\",\"00-inbox_week-50_251211-1650-trm应用于vla.md\":\"8TG5-tPZ\",\"00-inbox_week-50_251211-1650-trm应用大纲.md\":\"DiOjYmi3\",\"00-inbox_week-50_251211-1650-trm论文精读笔记.md\":\"T6iy4inR\",\"00-inbox_week-50_251211-1650-vae原理.md\":\"tn1tCE3Z\",\"00-inbox_week-50_251211-1650-图像生成基座模型调研.md\":\"C3_ev7jC\",\"00-inbox_week-50_251211-1650-场景理解分类功能.md\":\"D6A0V50r\",\"00-inbox_week-50__251212-0031_index.md\":\"C3GQgXuF\",\"00-inbox_测试.md\":\"C7fhBSuK\",\"10-knowledge_250822-0737-重启训练一个半小时内不开始训-overview.md\":\"DTlEsSVk\",\"10-knowledge_250826-0737-i2i训练方案-overview.md\":\"BWqUgHb_\",\"10-knowledge_250826-0737-openuni训练方法-overview.md\":\"D5Pf2zFA\",\"10-knowledge_250827-0736-token熵-overview.md\":\"yMmqgiKV\",\"10-knowledge_250827-0736-扩散过程中打印数值-overview.md\":\"vw7Gz5_4\",\"10-knowledge_250828-0735-fm_transformers模型参数-overview.md\":\"wqj-DMMn\",\"10-knowledge_250829-0735-困惑度-overview.md\":\"msel50i0\",\"10-knowledge_250829-0735-惊讶度-overview.md\":\"DRmLapWg\",\"10-knowledge_250830-0735-pipeline检查-overview.md\":\"Dm1xkb3f\",\"10-knowledge_250903-0735-macos快捷操作.md\":\"D3c37vqB\",\"10-knowledge_250906-0735-vicreg监督.md\":\"Dd57c5Wc\",\"10-knowledge_250906-0735-投影层方案.md\":\"C22clrJL\",\"10-knowledge_250907-0735-特征解耦方案gpt参考.md\":\"QE9RuHLF\",\"10-knowledge_example-knowledge.md\":\"DWSfhqqG\",\"10-knowledge_index.md\":\"D7YErs8N\",\"10-knowledge_readme.md\":\"DX_wp92N\",\"20-papers_2025_2025-08-20-.md\":\"DCTyHO3m\",\"20-papers_2025_2025-08-20-vtla-preference-learning.md\":\"BizrrC37\",\"20-papers_250829-0735-qwen2.5-omni论文阅读.md\":\"Cx8gSJFV\",\"20-papers_index.md\":\"Rx3N4286\",\"20-papers_vla_all_papers.md\":\"CYgyoP_l\",\"30-ideas_2025_9月_250826-0737-与弘扬讨论-overview.md\":\"C6iq2em0\",\"30-ideas_2025_9月_250826-0737-现有模型结构可改进点-overview.md\":\"CR_pnMmR\",\"30-ideas_2025_9月_250827-0736-0826-与弘扬沟通-overview.md\":\"C2BHfEZx\",\"30-ideas_2025_9月_250829-0735-中间层模型方案梳理-overview.md\":\"QONbFOex\",\"30-ideas_2025_9月_250829-0735-论文理论包装方案.md\":\"D5ny1W3u\",\"30-ideas_2025_9月_250904-0735-多轮对话拓展idea-overview.md\":\"DogjKQ42\",\"30-ideas_2025_9月_250906-0735-0905_今日晨思.md\":\"CpoE1vCq\",\"30-ideas_2025_9月_250907-0735-梦梦的建议.md\":\"DPat3o-T\",\"30-ideas_2025_9月_week36-论文实验idea汇总.md\":\"DklZ5tYa\",\"30-ideas_index.md\":\"vC2LKruP\",\"40-experiments_250826-0736-0826测评日志.md\":\"DcuRq2z5\",\"40-experiments_250902-0735-0901实验分析.md\":\"CJ-n6KHc\",\"40-experiments_250904-0735-exp_card_0903_transencodder.md\":\"DngFnrmi\",\"40-experiments_250904-0735-实验日志模板.md\":\"z_bUPbrc\",\"40-experiments_exp_card_0908_dinov2.md\":\"aIgTjKuw\",\"40-experiments_index.md\":\"OjZdn4zK\",\"50-reports_250820-0754-ocr调研.md\":\"76hfXUhd\",\"50-reports_250827-0736-高熵强化学习.md\":\"BGjTIXbQ\",\"50-reports_250901-0735-resume.md\":\"Cht0KdOP\",\"50-reports_250903-0735-座舱vla端云协同方案.md\":\"BUGw-tzo\",\"50-reports_index.md\":\"Dd1uw3NR\",\"50-reports_resume_250901-0735-resume_network.md\":\"DkL0TB9W\",\"50-reports_座舱vla端云协同方案.md\":\"BoR0GFx-\",\"50-reports_文本扩散模型调研.md\":\"C_U4opUc\",\"50-reports_训练与部署资源申请书.md\":\"D_ztKl30\",\"50-reports_近期发布模型调研-0908.md\":\"bm38CHwU\",\"50-reports_长视频理解综述.md\":\"CEqqO3ga\",\"50-reports_高熵rl - 数据构建增益.md\":\"L_PhsjTX\",\"index.md\":\"DZVZkJJg\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"ZhiGrove\",\"description\":\"Wang Yaqi's Knowledge Base\",\"base\":\"/ZhiGrove/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"收件箱\",\"link\":\"/00-inbox/\"},{\"text\":\"知识库\",\"link\":\"/10-knowledge/\"},{\"text\":\"论文\",\"link\":\"/20-papers/\"},{\"text\":\"灵感\",\"link\":\"/30-ideas/\"},{\"text\":\"实验\",\"link\":\"/40-experiments/\"},{\"text\":\"报告\",\"link\":\"/50-reports/\"}],\"sidebar\":{\"/00-inbox/\":[{\"text\":\"Inbox\",\"items\":[{\"text\":\"README\",\"link\":\"/00-inbox/\"},{\"text\":\"测试\",\"link\":\"/00-inbox/测试\"},{\"text\":\"week-34\",\"collapsed\":true,\"items\":[{\"text\":\"TRIAGE\",\"link\":\"/00-inbox/week-34/TRIAGE\"}]},{\"text\":\"week-35\",\"collapsed\":true,\"items\":[{\"text\":\"TRIAGE\",\"link\":\"/00-inbox/week-35/TRIAGE\"}]},{\"text\":\"week-36\",\"collapsed\":true,\"items\":[{\"text\":\"TRIAGE\",\"link\":\"/00-inbox/week-36/TRIAGE\"}]},{\"text\":\"week-37\",\"collapsed\":true,\"items\":[{\"text\":\"250908-0020-MoE llama结构\",\"link\":\"/00-inbox/week-37/250908-0020-MoE llama结构\"},{\"text\":\"250908-0020-clip损失\",\"link\":\"/00-inbox/week-37/250908-0020-clip损失\"},{\"text\":\"250908-0020-siglip损失\",\"link\":\"/00-inbox/week-37/250908-0020-siglip损失\"},{\"text\":\"250909-0023-dino语义监督\",\"link\":\"/00-inbox/week-37/250909-0023-dino语义监督\"},{\"text\":\"250910-0024-clip-siglip-vicreg损失区别与适用场景\",\"link\":\"/00-inbox/week-37/250910-0024-clip-siglip-vicreg损失区别与适用场景\"},{\"text\":\"250910-0024-训练loss调试思路\",\"link\":\"/00-inbox/week-37/250910-0024-训练loss调试思路\"}]},{\"text\":\"week-38\",\"collapsed\":true,\"items\":[{\"text\":\"250918-0024-0916talk\",\"link\":\"/00-inbox/week-38/250918-0024-0916talk\"},{\"text\":\"250920-0022-check_code\",\"link\":\"/00-inbox/week-38/250920-0022-check_code\"}]},{\"text\":\"week-39\",\"collapsed\":true,\"items\":[{\"text\":\"250919-0024-0919-代码框架问题排查\",\"link\":\"/00-inbox/week-39/250919-0024-0919-代码框架问题排查\"},{\"text\":\"250924-0024-0922-ocr-TODOlist\",\"link\":\"/00-inbox/week-39/250924-0024-0922-ocr-TODOlist\"},{\"text\":\"250925-0025-0923-复现crossflow\",\"link\":\"/00-inbox/week-39/250925-0025-0923-复现crossflow\"},{\"text\":\"250925-0025-语义调音\",\"link\":\"/00-inbox/week-39/250925-0025-语义调音\"}]},{\"text\":\"week-41\",\"collapsed\":true,\"items\":[{\"text\":\"251010-0025-0922-ocr-TODOlist\",\"link\":\"/00-inbox/week-41/251010-0025-0922-ocr-TODOlist\"},{\"text\":\"251010-0025-0923-复现crossflow\",\"link\":\"/00-inbox/week-41/251010-0025-0923-复现crossflow\"},{\"text\":\"251010-0025-dots_ocr服务部署 & 调用\",\"link\":\"/00-inbox/week-41/251010-0025-dots_ocr服务部署 & 调用\"},{\"text\":\"251010-0025-laion400m训练数据处理\",\"link\":\"/00-inbox/week-41/251010-0025-laion400m训练数据处理\"},{\"text\":\"251010-0025-语义调音\",\"link\":\"/00-inbox/week-41/251010-0025-语义调音\"}]},{\"text\":\"week-45\",\"collapsed\":true,\"items\":[{\"text\":\"251027-1431-VLA-VLA调研分享\",\"link\":\"/00-inbox/week-45/251027-1431-VLA-VLA调研分享\"},{\"text\":\"251104-1431-OCR任务-docx等格式文档检测\",\"link\":\"/00-inbox/week-45/251104-1431-OCR任务-docx等格式文档检测\"},{\"text\":\"251104-1431-VLA-扩散生成调研\",\"link\":\"/00-inbox/week-45/251104-1431-VLA-扩散生成调研\"},{\"text\":\"251104-1431-VLA-综述论文节选翻译-0925发布\",\"link\":\"/00-inbox/week-45/251104-1431-VLA-综述论文节选翻译-0925发布\"}]},{\"text\":\"week-46\",\"collapsed\":true,\"items\":[{\"text\":\"251111-0027-k线记忆卡\",\"link\":\"/00-inbox/week-46/251111-0027-k线记忆卡\"}]},{\"text\":\"week-50\",\"collapsed\":true,\"items\":[{\"text\":\"251113-1650-自动交易策略实现log\",\"link\":\"/00-inbox/week-50/251113-1650-自动交易策略实现log\"},{\"text\":\"251211-1650-JiT论文-视频要点\",\"link\":\"/00-inbox/week-50/251211-1650-JiT论文-视频要点\"},{\"text\":\"251211-1650-TRM应用于VLA\",\"link\":\"/00-inbox/week-50/251211-1650-TRM应用于VLA\"},{\"text\":\"251211-1650-TRM应用大纲\",\"link\":\"/00-inbox/week-50/251211-1650-TRM应用大纲\"},{\"text\":\"251211-1650-TRM论文精读笔记\",\"link\":\"/00-inbox/week-50/251211-1650-TRM论文精读笔记\"},{\"text\":\"251211-1650-VAE原理\",\"link\":\"/00-inbox/week-50/251211-1650-VAE原理\"},{\"text\":\"251211-1650-deepseekV3_2\",\"link\":\"/00-inbox/week-50/251211-1650-deepseekV3_2\"},{\"text\":\"251211-1650-rustdesk安装及配置\",\"link\":\"/00-inbox/week-50/251211-1650-rustdesk安装及配置\"},{\"text\":\"251211-1650-图像生成基座模型调研\",\"link\":\"/00-inbox/week-50/251211-1650-图像生成基座模型调研\"},{\"text\":\"251211-1650-场景理解分类功能\",\"link\":\"/00-inbox/week-50/251211-1650-场景理解分类功能\"},{\"text\":\"[251212-0031]index\",\"link\":\"/00-inbox/week-50/[251212-0031]index\"}]}]}],\"/10-knowledge/\":[{\"text\":\"Knowledge\",\"items\":[{\"text\":\"README\",\"link\":\"/10-knowledge/\"},{\"text\":\"250822-0737-重启训练一个半小时内不开始训-overview\",\"link\":\"/10-knowledge/250822-0737-重启训练一个半小时内不开始训-overview\"},{\"text\":\"250826-0737-OpenUni训练方法-overview\",\"link\":\"/10-knowledge/250826-0737-OpenUni训练方法-overview\"},{\"text\":\"250826-0737-i2i训练方案-overview\",\"link\":\"/10-knowledge/250826-0737-i2i训练方案-overview\"},{\"text\":\"250827-0736-token熵-overview\",\"link\":\"/10-knowledge/250827-0736-token熵-overview\"},{\"text\":\"250827-0736-扩散过程中打印数值-overview\",\"link\":\"/10-knowledge/250827-0736-扩散过程中打印数值-overview\"},{\"text\":\"250828-0735-fm_transformers模型参数-overview\",\"link\":\"/10-knowledge/250828-0735-fm_transformers模型参数-overview\"},{\"text\":\"250829-0735-困惑度-overview\",\"link\":\"/10-knowledge/250829-0735-困惑度-overview\"},{\"text\":\"250829-0735-惊讶度-overview\",\"link\":\"/10-knowledge/250829-0735-惊讶度-overview\"},{\"text\":\"250830-0735-pipeline检查-overview\",\"link\":\"/10-knowledge/250830-0735-pipeline检查-overview\"},{\"text\":\"250903-0735-macos快捷操作\",\"link\":\"/10-knowledge/250903-0735-macos快捷操作\"},{\"text\":\"250906-0735-VICReg监督\",\"link\":\"/10-knowledge/250906-0735-VICReg监督\"},{\"text\":\"250906-0735-投影层方案\",\"link\":\"/10-knowledge/250906-0735-投影层方案\"},{\"text\":\"250907-0735-特征解耦方案GPT参考\",\"link\":\"/10-knowledge/250907-0735-特征解耦方案GPT参考\"},{\"text\":\"example-knowledge\",\"link\":\"/10-knowledge/example-knowledge\"}]}],\"/20-papers/\":[{\"text\":\"Papers\",\"items\":[{\"text\":\"README\",\"link\":\"/20-papers/\"},{\"text\":\"250829-0735-Qwen2.5-Omni论文阅读\",\"link\":\"/20-papers/250829-0735-Qwen2.5-Omni论文阅读\"},{\"text\":\"2025\",\"collapsed\":true,\"items\":[{\"text\":\"2025-08-20-\",\"link\":\"/20-papers/2025/2025-08-20-\"},{\"text\":\"2025-08-20-vtla-preference-learning\",\"link\":\"/20-papers/2025/2025-08-20-vtla-preference-learning\"}]},{\"text\":\"vla\",\"collapsed\":true,\"items\":[{\"text\":\"all_papers\",\"link\":\"/20-papers/vla/all_papers\"}]}]}],\"/30-ideas/\":[{\"text\":\"Ideas\",\"items\":[{\"text\":\"README\",\"link\":\"/30-ideas/\"},{\"text\":\"2025\",\"collapsed\":true,\"items\":[{\"text\":\"9月\",\"collapsed\":true,\"items\":[{\"text\":\"250826-0737-与弘扬讨论-overview\",\"link\":\"/30-ideas/2025/9月/250826-0737-与弘扬讨论-overview\"},{\"text\":\"250826-0737-现有模型结构可改进点-overview\",\"link\":\"/30-ideas/2025/9月/250826-0737-现有模型结构可改进点-overview\"},{\"text\":\"250827-0736-0826-与弘扬沟通-overview\",\"link\":\"/30-ideas/2025/9月/250827-0736-0826-与弘扬沟通-overview\"},{\"text\":\"250829-0735-中间层模型方案梳理-overview\",\"link\":\"/30-ideas/2025/9月/250829-0735-中间层模型方案梳理-overview\"},{\"text\":\"250829-0735-论文理论包装方案\",\"link\":\"/30-ideas/2025/9月/250829-0735-论文理论包装方案\"},{\"text\":\"250904-0735-多轮对话拓展idea-overview\",\"link\":\"/30-ideas/2025/9月/250904-0735-多轮对话拓展idea-overview\"},{\"text\":\"250906-0735-0905_今日晨思\",\"link\":\"/30-ideas/2025/9月/250906-0735-0905_今日晨思\"},{\"text\":\"250907-0735-梦梦的建议\",\"link\":\"/30-ideas/2025/9月/250907-0735-梦梦的建议\"},{\"text\":\"week36-论文实验idea汇总\",\"link\":\"/30-ideas/2025/9月/week36-论文实验idea汇总\"}]}]}]}],\"/40-experiments/\":[{\"text\":\"Experiments\",\"items\":[{\"text\":\"README\",\"link\":\"/40-experiments/\"},{\"text\":\"250826-0736-0826测评日志\",\"link\":\"/40-experiments/250826-0736-0826测评日志\"},{\"text\":\"250902-0735-0901实验分析\",\"link\":\"/40-experiments/250902-0735-0901实验分析\"},{\"text\":\"250904-0735-exp_card_0903_transencodder\",\"link\":\"/40-experiments/250904-0735-exp_card_0903_transencodder\"},{\"text\":\"250904-0735-实验日志模板\",\"link\":\"/40-experiments/250904-0735-实验日志模板\"},{\"text\":\"exp_card_0908_dinov2\",\"link\":\"/40-experiments/exp_card_0908_dinov2\"}]}],\"/50-reports/\":[{\"text\":\"Reports\",\"items\":[{\"text\":\"README\",\"link\":\"/50-reports/\"},{\"text\":\"250820-0754-OCR调研\",\"link\":\"/50-reports/250820-0754-OCR调研\"},{\"text\":\"250827-0736-高熵强化学习\",\"link\":\"/50-reports/250827-0736-高熵强化学习\"},{\"text\":\"250901-0735-resume\",\"link\":\"/50-reports/250901-0735-resume\"},{\"text\":\"250903-0735-座舱VLA端云协同方案\",\"link\":\"/50-reports/250903-0735-座舱VLA端云协同方案\"},{\"text\":\"座舱VLA端云协同方案\",\"link\":\"/50-reports/座舱VLA端云协同方案\"},{\"text\":\"文本扩散模型调研\",\"link\":\"/50-reports/文本扩散模型调研\"},{\"text\":\"训练与部署资源申请书\",\"link\":\"/50-reports/训练与部署资源申请书\"},{\"text\":\"近期发布模型调研-0908\",\"link\":\"/50-reports/近期发布模型调研-0908\"},{\"text\":\"长视频理解综述\",\"link\":\"/50-reports/长视频理解综述\"},{\"text\":\"高熵RL - 数据构建增益\",\"link\":\"/50-reports/高熵RL - 数据构建增益\"},{\"text\":\"resume\",\"collapsed\":true,\"items\":[{\"text\":\"250901-0735-resume_network\",\"link\":\"/50-reports/resume/250901-0735-resume_network\"}]}]}]},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/wangyaqi/ZhiGrove\"}],\"search\":{\"provider\":\"local\"},\"footer\":{\"message\":\"Released under the MIT License.\",\"copyright\":\"Copyright © 2025 Wang Yaqi\"}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>