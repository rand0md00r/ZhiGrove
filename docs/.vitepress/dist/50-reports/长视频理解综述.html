<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>MA-LMM 中的 Query-Former 与长时记忆模块 | ZhiGrove</title>
    <meta name="description" content="Wang Yaqi's Knowledge Base">
    <meta name="generator" content="VitePress v1.6.4">
    <link rel="preload stylesheet" href="/assets/style.BhX5jokk.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.GWSF0Fav.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.DQSJWTrN.js">
    <link rel="modulepreload" href="/assets/chunks/framework.CQuhCYrb.js">
    <link rel="modulepreload" href="/assets/50-reports_长视频理解综述.md.BseMc13e.lean.js">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-5d98c3a5><!--[--><!--]--><!--[--><span tabindex="-1" data-v-0b0ada53></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0b0ada53>Skip to content</a><!--]--><!----><header class="VPNav" data-v-5d98c3a5 data-v-ae24b3ad><div class="VPNavBar" data-v-ae24b3ad data-v-6aa21345><div class="wrapper" data-v-6aa21345><div class="container" data-v-6aa21345><div class="title" data-v-6aa21345><div class="VPNavBarTitle has-sidebar" data-v-6aa21345 data-v-1168a8e4><a class="title" href="/" data-v-1168a8e4><!--[--><!--]--><!----><span data-v-1168a8e4>ZhiGrove</span><!--[--><!--]--></a></div></div><div class="content" data-v-6aa21345><div class="content-body" data-v-6aa21345><!--[--><!--]--><div class="VPNavBarSearch search" data-v-6aa21345><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-6aa21345 data-v-dc692963><span id="main-nav-aria-label" class="visually-hidden" data-v-dc692963> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>首页</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/00-inbox/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>收件箱</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/10-knowledge/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>知识库</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/20-papers/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>论文</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/30-ideas/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>灵感</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/40-experiments/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>实验</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/50-reports/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>报告</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-6aa21345 data-v-6c893767><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-6c893767 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-6aa21345 data-v-0394ad82 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/wangyaqi/ZhiGrove" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-6aa21345 data-v-bb2aa2f0 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-cf11d7a2><span class="vpi-more-horizontal icon" data-v-cf11d7a2></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><!----><!--[--><!--[--><!----><div class="group" data-v-bb2aa2f0><div class="item appearance" data-v-bb2aa2f0><p class="label" data-v-bb2aa2f0>Appearance</p><div class="appearance-action" data-v-bb2aa2f0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-bb2aa2f0 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div></div></div><div class="group" data-v-bb2aa2f0><div class="item social-links" data-v-bb2aa2f0><div class="VPSocialLinks social-links-list" data-v-bb2aa2f0 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/wangyaqi/ZhiGrove" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-6aa21345 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><div class="divider" data-v-6aa21345><div class="divider-line" data-v-6aa21345></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-5d98c3a5 data-v-a6f0e41e><div class="container" data-v-a6f0e41e><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-a6f0e41e><span class="vpi-align-left menu-icon" data-v-a6f0e41e></span><span class="menu-text" data-v-a6f0e41e>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-a6f0e41e data-v-8a42e2b4><button data-v-8a42e2b4>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-5d98c3a5 data-v-319d5ca6><div class="curtain" data-v-319d5ca6></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-319d5ca6><span class="visually-hidden" id="sidebar-aria-label" data-v-319d5ca6> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0 has-active" data-v-c40bc020 data-v-b3fd67f8><div class="item" role="button" tabindex="0" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><h2 class="text" data-v-b3fd67f8>Reports</h2><!----></div><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/50-reports/" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>README</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/50-reports/250820-0754-OCR%E8%B0%83%E7%A0%94.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>250820-0754-OCR调研</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/50-reports/250827-0736-%E9%AB%98%E7%86%B5%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>250827-0736-高熵强化学习</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/50-reports/250901-0735-resume.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>250901-0735-resume</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/50-reports/250903-0735-%E5%BA%A7%E8%88%B1VLA%E7%AB%AF%E4%BA%91%E5%8D%8F%E5%90%8C%E6%96%B9%E6%A1%88.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>250903-0735-座舱VLA端云协同方案</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/50-reports/%E5%BA%A7%E8%88%B1VLA%E7%AB%AF%E4%BA%91%E5%8D%8F%E5%90%8C%E6%96%B9%E6%A1%88.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>座舱VLA端云协同方案</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/50-reports/%E6%96%87%E6%9C%AC%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%A0%94.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>文本扩散模型调研</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/50-reports/%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%83%A8%E7%BD%B2%E8%B5%84%E6%BA%90%E7%94%B3%E8%AF%B7%E4%B9%A6.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>训练与部署资源申请书</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/50-reports/%E8%BF%91%E6%9C%9F%E5%8F%91%E5%B8%83%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%A0%94-0908.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>近期发布模型调研-0908</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/50-reports/%E9%95%BF%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3%E7%BB%BC%E8%BF%B0.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>长视频理解综述</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/50-reports/%E9%AB%98%E7%86%B5RL%20-%20%E6%95%B0%E6%8D%AE%E6%9E%84%E5%BB%BA%E5%A2%9E%E7%9B%8A.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>高熵RL - 数据构建增益</p><!--]--></a><!----></div><!----></div><section class="VPSidebarItem level-1 collapsible collapsed" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" role="button" tabindex="0" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><h3 class="text" data-v-b3fd67f8>resume</h3><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-b3fd67f8><span class="vpi-chevron-right caret-icon" data-v-b3fd67f8></span></div></div><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-2 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/50-reports/resume/250901-0735-resume_network.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>250901-0735-resume_network</p><!--]--></a><!----></div><!----></div><!--]--></div></section><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-5d98c3a5 data-v-1428d186><div class="VPDoc has-sidebar has-aside" data-v-1428d186 data-v-39a288b8><!--[--><!--]--><div class="container" data-v-39a288b8><div class="aside" data-v-39a288b8><div class="aside-curtain" data-v-39a288b8></div><div class="aside-container" data-v-39a288b8><div class="aside-content" data-v-39a288b8><div class="VPDocAside" data-v-39a288b8 data-v-3f215769><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3f215769 data-v-a5bbad30><div class="content" data-v-a5bbad30><div class="outline-marker" data-v-a5bbad30></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-a5bbad30>On this page</div><ul class="VPDocOutlineItem root" data-v-a5bbad30 data-v-b933a997><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-39a288b8><div class="content-container" data-v-39a288b8><!--[--><!--]--><main class="main" data-v-39a288b8><div style="position:relative;" class="vp-doc _50-reports_%E9%95%BF%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3%E7%BB%BC%E8%BF%B0" data-v-39a288b8><div><h1 id="ma-lmm-中的-query-former-与长时记忆模块" tabindex="-1">MA-LMM 中的 Query-Former 与长时记忆模块 <a class="header-anchor" href="#ma-lmm-中的-query-former-与长时记忆模块" aria-label="Permalink to &quot;MA-LMM 中的 Query-Former 与长时记忆模块&quot;">​</a></h1><h2 id="方法流程" tabindex="-1">方法流程 <a class="header-anchor" href="#方法流程" aria-label="Permalink to &quot;方法流程&quot;">​</a></h2><ol><li><p><strong>视觉特征提取</strong></p><ul><li>视频逐帧输入视觉编码器，提取帧特征并加入时间位置编码。</li><li>特征存入长时记忆模块。</li></ul></li><li><p><strong>长时记忆建模</strong></p><ul><li>使用 Query-Former 作为桥梁，对齐视觉特征与文本空间。</li><li>引入两个记忆库： <ul><li><strong>视觉记忆库（Visual Memory Bank）</strong>：保存所有历史帧特征，用于 cross-attention。</li><li><strong>查询记忆库（Query Memory Bank）</strong>：保存历次查询结果（learned queries），用于 self-attention。</li></ul></li></ul></li><li><p><strong>文本解码</strong></p><ul><li>Q-Former 输出的最终表示输入 LLM，生成回答或描述。</li></ul></li></ol><h2 id="输入输出" tabindex="-1">输入输出 <a class="header-anchor" href="#输入输出" aria-label="Permalink to &quot;输入输出&quot;">​</a></h2><ul><li><strong>输入</strong>：视频帧序列 + 文本提示</li><li><strong>输出</strong>：自然语言回答、视频描述或分类标签</li></ul><h2 id="记忆更新机制" tabindex="-1">记忆更新机制 <a class="header-anchor" href="#记忆更新机制" aria-label="Permalink to &quot;记忆更新机制&quot;">​</a></h2><ol><li><p><strong>视觉记忆库更新</strong></p><ul><li>每新一帧加入库中，作为 cross-attention 的 Key/Value。</li></ul></li><li><p><strong>查询记忆库更新</strong></p><ul><li>每个时间步的查询 (z_t) 累积存入库中，作为 self-attention 的 Key/Value。</li></ul></li><li><p><strong>记忆压缩（MBC）</strong></p><ul><li>计算相邻帧特征余弦相似度，合并最相似的一对特征。</li><li>保留判别性特征并减少冗余，避免显存线性增长。</li></ul></li></ol><h1 id="长视频理解方法对比-ma-lmm-vs-streaming-lvu" tabindex="-1">长视频理解方法对比：MA-LMM vs. Streaming-LVU <a class="header-anchor" href="#长视频理解方法对比-ma-lmm-vs-streaming-lvu" aria-label="Permalink to &quot;长视频理解方法对比：MA-LMM vs. Streaming-LVU&quot;">​</a></h1><table tabindex="0"><thead><tr><th>方法</th><th>对已有记忆库方法的批评</th><th>改进思路</th><th>特点</th></tr></thead><tbody><tr><td><strong>MA-LMM</strong> (Memory-Augmented LMM for Long-Term Video Understanding)</td><td>记忆库在召回历史信息时依赖<strong>显式时间戳</strong>，如果没有时间指引，难以生成全面的回答</td><td>引入 <strong>Query-Former + 双记忆库（视觉记忆库 + 查询记忆库）</strong>，通过语义查询与历史特征交互进行检索</td><td>解决了对时间戳的依赖，支持语义驱动的历史信息回忆</td></tr><tr><td><strong>Streaming-LVU</strong> (Streaming Long Video Understanding with LLMs)</td><td>在长视频场景中，传统记忆库容易出现 <strong>检索效率低</strong>，且依赖时间片段索引，难以进行流式理解</td><td>提出 <strong>Streaming 机制</strong>，动态维护上下文记忆，支持流式输入和压缩更新</td><td>强调连续视频流处理与高效记忆管理，适合超长视频场景</td></tr></tbody></table><h1 id="videostreaming-memory-propagated-streaming-encoding-adaptive-memory-selection" tabindex="-1">VideoStreaming (Memory-Propagated Streaming Encoding + Adaptive Memory Selection) <a class="header-anchor" href="#videostreaming-memory-propagated-streaming-encoding-adaptive-memory-selection" aria-label="Permalink to &quot;VideoStreaming (Memory-Propagated Streaming Encoding + Adaptive Memory Selection)&quot;">​</a></h1><h2 id="方法核心思想" tabindex="-1">方法核心思想 <a class="header-anchor" href="#方法核心思想" aria-label="Permalink to &quot;方法核心思想&quot;">​</a></h2><ul><li><strong>目标</strong>：在长视频理解中保留关键的空间信息与时间动态，同时减少冗余。</li><li><strong>核心机制</strong>： <ol><li><strong>Memory-Propagated Streaming Encoding</strong>：逐段编码视频，并将前一段的记忆传递给后一段，形成连续表示。</li><li><strong>Adaptive Memory Selection</strong>：针对具体问题，自适应地选择相关历史记忆，提升回答的针对性和精确性。</li></ol></li></ul><hr><h2 id="方法流程-1" tabindex="-1">方法流程 <a class="header-anchor" href="#方法流程-1" aria-label="Permalink to &quot;方法流程&quot;">​</a></h2><ol><li><p><strong>视频分段与逐段编码</strong></p><ul><li>将长视频划分为多个短片段（clip）。</li><li>编码当前片段时，会先参考前一个片段的记忆，再与当前特征拼接后输入到 <strong>小型 Decoder-only LM</strong>。</li><li>由于自回归特性，序列信息逐渐累积到最后几个 token，因此使用最后几个 token 作为更新后的记忆，代表当前时间点之前的全部视频信息。</li></ul></li><li><p><strong>固定长度记忆</strong></p><ul><li>模型在编码过程中始终保持固定长度的记忆，用以表示任意长的视频。</li><li>但这会导致早期细节被压缩或丢失。</li></ul></li><li><p><strong>历史记忆保存与问题驱动的记忆选择</strong></p><ul><li>为弥补固定长度记忆的不足，方法会保存 <strong>所有片段的历史记忆</strong>。</li><li>在编码每个片段时，附加一个 <strong>summary token</strong>（片段指示符），用于概括该片段的语义信息。</li><li>在回答具体问题时： <ol><li>将最终迭代的 condensed memory 与问题拼接，输入同一个小型 LM。</li><li>获取问题指示符（question indicator）。</li><li>计算问题指示符与所有片段指示符的相似度。</li><li>选取相似度最高的若干 clip memories，作为与问题最相关的历史信息。</li></ol></li><li>最终将这些 <strong>自适应选择的记忆</strong> 输入 LLM，进行详细问答。</li></ul></li></ol><hr><h2 id="输入与输出" tabindex="-1">输入与输出 <a class="header-anchor" href="#输入与输出" aria-label="Permalink to &quot;输入与输出&quot;">​</a></h2><ul><li><strong>输入</strong>：长视频（切分为多个 clip）+ 问题（question）。</li><li><strong>输出</strong>：对问题的自然语言回答，具备明确的时间指向性（temporal grounding）。</li></ul><hr><h2 id="训练策略" tabindex="-1">训练策略 <a class="header-anchor" href="#训练策略" aria-label="Permalink to &quot;训练策略&quot;">​</a></h2><ul><li><p><strong>两阶段训练</strong>：</p><ol><li><strong>单片段预训练</strong>：通过 prefix 任务，增强小型 LM 的单片段编码能力。</li><li><strong>流式训练</strong>：让其作为 streaming encoder，并与 LLM 联合训练，实现长视频理解。</li></ol></li><li><p><strong>数据构建</strong>：</p><ul><li>将短视频拼接成长视频，并保留原有问题。</li><li>使用 Panda-70M 的长视频及分段描述，构建多轮长视频 QA 数据（带显式时间戳），指导模型学习准确的记忆选择。</li></ul></li></ul><hr><h2 id="方法贡献" tabindex="-1">方法贡献 <a class="header-anchor" href="#方法贡献" aria-label="Permalink to &quot;方法贡献&quot;">​</a></h2><ol><li>分析了长视频理解中的挑战，指出现有方法存在编码效率低的问题。</li><li>提出了两大关键设计： <ul><li><strong>Memory-Propagated Streaming Encoding</strong></li><li><strong>Adaptive Memory Selection</strong></li></ul></li><li>实验表明：VideoStreaming 能在长视频基准测试中实现更精确的时间定位、更优性能和更高推理效率。</li></ol><h2 id="流式编码流程" tabindex="-1">流式编码流程 <a class="header-anchor" href="#流式编码流程" aria-label="Permalink to &quot;流式编码流程&quot;">​</a></h2><p>第一阶段</p><ol><li>将一段视频分割为多个clip(剪辑)。对每一个clip提取均匀采样16帧。通过小语言模型和mlp投影层将每一帧蒸馏为4个token，最终使得每一个每个clip具有64个token。</li></ol><p>第二阶段</p><h1 id="an-exploration-of-video-understanding-in-large-multimodal-models" tabindex="-1">An Exploration of Video Understanding in Large Multimodal Models <a class="header-anchor" href="#an-exploration-of-video-understanding-in-large-multimodal-models" aria-label="Permalink to &quot;An Exploration of Video Understanding in Large Multimodal Models&quot;">​</a></h1><h1 id="apollo-an-exploration-of-video-understanding-in-large-multimodal-models" tabindex="-1">Apollo: An Exploration of Video Understanding in Large Multimodal Models <a class="header-anchor" href="#apollo-an-exploration-of-video-understanding-in-large-multimodal-models" aria-label="Permalink to &quot;Apollo: An Exploration of Video Understanding in Large Multimodal Models&quot;">​</a></h1><h2 id="论文贡献" tabindex="-1">论文贡献 <a class="header-anchor" href="#论文贡献" aria-label="Permalink to &quot;论文贡献&quot;">​</a></h2><ol><li><p><strong>系统性探索设计空间</strong></p><ul><li>全面研究视频大规模多模态模型（video-LMMs）的关键要素：<br> 视频采样、编码器选择、token 重采样、token 融合、训练调度、数据构成等。</li><li>揭示了真正驱动视频理解性能的关键因素，并提供了可操作的设计建议。</li></ul></li><li><p><strong>提出 “Scaling Consistency”</strong></p><ul><li>发现小规模模型（2B–4B 参数以上）和小数据集上的设计选择可以可靠迁移到大模型。</li><li>显著降低研究成本，使研究者可以在低资源条件下高效实验。</li></ul></li><li><p><strong>改进评测方式：ApolloBench</strong></p><ul><li>指出现有基准测试中大量问题无需视频信息即可解答，存在冗余。</li><li>提出 <strong>ApolloBench</strong>：更高效、更紧凑的基准，评测速度快 41 倍，并更能区分视频感知与推理能力。</li></ul></li><li><p><strong>推出 Apollo 模型家族</strong></p><ul><li>基于研究洞察，训练了 <strong>Apollo-1.5B, Apollo-3B, Apollo-7B</strong> 三类模型。</li><li><strong>Apollo-3B 超越大多数 7B 模型</strong>，而 <strong>Apollo-7B 在同类中达到 SOTA</strong>，甚至超过部分 30B 模型。</li><li>展现了通过合理设计与训练，中等规模模型也能实现先进的视频理解能力。</li></ul></li></ol><hr><h2 id="论文结论" tabindex="-1">论文结论 <a class="header-anchor" href="#论文结论" aria-label="Permalink to &quot;论文结论&quot;">​</a></h2><ul><li>本研究填补了视频 LMM 缺乏系统性探索的空白，提供了从 <strong>设计 → 训练 → 评测 → 模型实现</strong> 的完整指南。</li><li><strong>Scaling Consistency</strong> 的提出，使得研究者无需依赖超大模型，也能获得可靠的实验结论，加快研究迭代。</li><li><strong>ApolloBench</strong> 提供了一个更高质量的评测工具，避免了现有基准中的“假视频理解”现象。</li><li><strong>Apollo 模型家族</strong> 的结果证明：通过精心设计，中小规模模型也能匹敌甚至超越更大规模模型。</li><li>整体上，Apollo 工作为视频 LMM 的未来发展提供了 <strong>理论洞察、实证结果与实用工具</strong>，推动该领域向更高效、更可解释的方向前进。</li></ul><h1 id="ovo-bench-在线视频理解基准" tabindex="-1">OVO-Bench: 在线视频理解基准 <a class="header-anchor" href="#ovo-bench-在线视频理解基准" aria-label="Permalink to &quot;OVO-Bench: 在线视频理解基准&quot;">​</a></h1><h2 id="研究背景" tabindex="-1">研究背景 <a class="header-anchor" href="#研究背景" aria-label="Permalink to &quot;研究背景&quot;">​</a></h2><ul><li><strong>时间感知（Temporal Awareness）</strong> 是在线视频大模型（Video-LLMs）的关键能力。</li><li>离线模型：依赖完整视频，做静态、事后分析。</li><li>在线模型：需在视频流过程中，随时间戳动态推理与回答。</li><li><strong>现有基准不足</strong>：未能充分评估“时间感知”能力。</li></ul><hr><h2 id="三大评测场景" tabindex="-1">三大评测场景 <a class="header-anchor" href="#三大评测场景" aria-label="Permalink to &quot;三大评测场景&quot;">​</a></h2><ol><li><strong>Backward tracing（回溯推理）</strong><ul><li>追溯过去事件来回答问题。</li></ul></li><li><strong>Real-time understanding（实时理解）</strong><ul><li>在当前时间戳即时理解并回答。</li></ul></li><li><strong>Forward active responding（前瞻响应）</strong><ul><li>等待未来信息足够时，再准确作答。</li></ul></li></ol><hr><h2 id="数据与任务设计" tabindex="-1">数据与任务设计 <a class="header-anchor" href="#数据与任务设计" aria-label="Permalink to &quot;数据与任务设计&quot;">​</a></h2><ul><li>共 <strong>12 个任务</strong>，<strong>644 个视频</strong>。</li><li>含 <strong>约 2800 条人工精细标注</strong>，均带有 <strong>精确时间戳</strong>。</li><li>结合 <strong>自动生成管线</strong> 与 <strong>人工标注</strong>，保证数据高质量。</li></ul><hr><h2 id="评测方法" tabindex="-1">评测方法 <a class="header-anchor" href="#评测方法" aria-label="Permalink to &quot;评测方法&quot;">​</a></h2><ul><li>设计了系统化评估流程。</li><li>沿视频时间轴动态提问，模拟真实使用场景。</li></ul><hr><h2 id="实验结果" tabindex="-1">实验结果 <a class="header-anchor" href="#实验结果" aria-label="Permalink to &quot;实验结果&quot;">​</a></h2><ul><li>评估了 <strong>11 个现有 Video-LLMs</strong>。</li><li>结果显示： <ul><li>在传统基准上表现良好。</li><li>在 <strong>在线视频理解</strong> 上，显著落后于人类表现。</li></ul></li></ul><hr><h2 id="贡献与意义" tabindex="-1">贡献与意义 <a class="header-anchor" href="#贡献与意义" aria-label="Permalink to &quot;贡献与意义&quot;">​</a></h2><ul><li><strong>填补空白</strong>：首次系统性评估视频 LLM 的时间感知能力。</li><li><strong>推动研究</strong>：促进模型在 <strong>在线动态推理</strong> 方向的进步。</li><li><strong>开源资源</strong>：<a href="https://github.com/JoeLeelyf/OVO-Bench" target="_blank" rel="noreferrer">GitHub - OVO-Bench</a></li></ul><h1 id="temporal-preference-optimization-for-long-form-video-understanding" tabindex="-1">Temporal Preference Optimization for Long-Form Video Understanding <a class="header-anchor" href="#temporal-preference-optimization-for-long-form-video-understanding" aria-label="Permalink to &quot;Temporal Preference Optimization for Long-Form Video Understanding&quot;">​</a></h1><h2 id="核心问题" tabindex="-1">核心问题 <a class="header-anchor" href="#核心问题" aria-label="Permalink to &quot;核心问题&quot;">​</a></h2><ul><li>当前视频大模型（video-LMMs）在长视频理解上表现欠佳，尤其是 <strong>时间定位 (temporal grounding)</strong> 能力不足。</li><li>现有方法依赖大规模合成/标注数据，且缺乏显式的时间优化信号。</li></ul><h2 id="方法论-temporal-preference-optimization-tpo" tabindex="-1">方法论：Temporal Preference Optimization (TPO) <a class="header-anchor" href="#方法论-temporal-preference-optimization-tpo" aria-label="Permalink to &quot;方法论：Temporal Preference Optimization (TPO)&quot;">​</a></h2><p>TPO 是一个 <strong>后训练(post-training)</strong> 框架，结合 <strong>自动偏好数据构造</strong> 与 <strong>Direct Preference Optimization (DPO)</strong>，提升模型的长时序理解能力。</p><h3 id="_1-偏好数据构造-self-training" tabindex="-1">1. 偏好数据构造 (Self-Training) <a class="header-anchor" href="#_1-偏好数据构造-self-training" aria-label="Permalink to &quot;1. 偏好数据构造 (Self-Training)&quot;">​</a></h3><p>TPO 自动生成 <strong>优选 (preferred)</strong> 与 <strong>劣选 (dis-preferred)</strong> 响应对，形成偏好数据：</p><ul><li><strong>Localized Temporal Preference</strong><ul><li>针对视频子片段生成问题。</li><li>r⁺：基于相关片段的回答。</li><li>r⁻：基于去掉该片段的其他部分生成的回答。</li></ul></li><li><strong>Comprehensive Temporal Preference</strong><ul><li>针对全局长时序问题。</li><li>r⁺：基于完整视频的回答。</li><li>r⁻：基于下采样/缺失片段的视频回答。</li></ul></li><li><strong>Post-filtering</strong>：利用 LLM（GPT-4o-mini）过滤掉错误或质量不佳的pair。</li></ul><h3 id="_2-偏好优化-direct-preference-optimization" tabindex="-1">2. 偏好优化 (Direct Preference Optimization) <a class="header-anchor" href="#_2-偏好优化-direct-preference-optimization" aria-label="Permalink to &quot;2. 偏好优化 (Direct Preference Optimization)&quot;">​</a></h3><ul><li><p>使用 DPO 训练 video-LMM，使模型更倾向输出 r⁺ 而非 r⁻。</p></li><li><p>同时结合少量 <strong>SFT Loss</strong>，保证训练稳定性。</p></li><li><p>优化目标：</p><p>$\pi_\theta(r^+|V,q) &gt; \pi_\theta(r^-|V,q)$</p></li></ul><hr><h2 id="贡献" tabindex="-1">贡献 <a class="header-anchor" href="#贡献" aria-label="Permalink to &quot;贡献&quot;">​</a></h2><ol><li><strong>提出 TPO 框架</strong>：一种轻量、可扩展的后训练方法，显式增强长视频的时间理解能力。</li><li><strong>双粒度时间偏好建模</strong>： <ul><li>Localized（细粒度）+ Comprehensive（全局依赖），实现互补效果。</li></ul></li><li><strong>无需大规模人工标注</strong>：通过自训练自动生成偏好对，降低了数据成本。</li></ol><hr><h2 id="总结" tabindex="-1">总结 <a class="header-anchor" href="#总结" aria-label="Permalink to &quot;总结&quot;">​</a></h2><p>TPO 通过 <strong>自动构造多粒度时间偏好数据 + 偏好优化训练</strong>，显著提升视频大模型的<strong>长时序理解与时间定位能力</strong>，为长视频理解提供了一种高效的<strong>后训练解决方案</strong>。</p><h3 id="方法论总结-temporal-preference-optimization-tpo" tabindex="-1">方法论总结：Temporal Preference Optimization (TPO) <a class="header-anchor" href="#方法论总结-temporal-preference-optimization-tpo" aria-label="Permalink to &quot;方法论总结：Temporal Preference Optimization (TPO)&quot;">​</a></h3><ul><li><strong>核心思想</strong>：通过构造 <strong>优选 (preferred)</strong> 与 <strong>劣选 (dis-preferred)</strong> 响应对，引导模型学习时间定位能力。</li></ul><h4 id="两类时间偏好数据" tabindex="-1">两类时间偏好数据 <a class="header-anchor" href="#两类时间偏好数据" aria-label="Permalink to &quot;两类时间偏好数据&quot;">​</a></h4><ol><li><p><strong>局部时间定位 (Localized Temporal Grounding)</strong></p><ul><li>问题针对特定片段。</li><li>r⁺：来自对应片段的回答。</li><li>r⁻：来自无关片段的回答。</li></ul></li><li><p><strong>全局时间定位 (Comprehensive Temporal Grounding)</strong></p><ul><li>问题涉及更长时序依赖。</li><li>r⁺：基于完整视频的回答。</li><li>r⁻：基于下采样、缺失关键信息的视频回答。</li></ul></li></ol><h4 id="优化方式" tabindex="-1">优化方式 <a class="header-anchor" href="#优化方式" aria-label="Permalink to &quot;优化方式&quot;">​</a></h4><ul><li>利用 <strong>Direct Preference Optimization (DPO)</strong>，让模型更倾向输出 r⁺。</li><li>简单的视频输入变换即可自动生成偏好数据。</li></ul><h4 id="效果" tabindex="-1">效果 <a class="header-anchor" href="#效果" aria-label="Permalink to &quot;效果&quot;">​</a></h4><ul><li>在 <strong>细粒度 (局部)</strong> 与 <strong>长上下文 (全局)</strong> 场景下均能提升时间推理能力。</li><li>为长视频理解提供稳健的后训练解决方案。</li></ul><h1 id="video-xl-extra-long-vision-language-model-for-hour-scale-video-understanding" tabindex="-1">Video-XL: Extra-Long Vision Language Model for Hour-Scale Video Understanding <a class="header-anchor" href="#video-xl-extra-long-vision-language-model-for-hour-scale-video-understanding" aria-label="Permalink to &quot;Video-XL:  Extra-Long Vision Language Model for Hour-Scale Video Understanding&quot;">​</a></h1><ol><li>输入与特征抽取</li></ol><ul><li>输入：长视频（小时级别，可包含 2k+ 帧）</li><li>处理： <ul><li>从视频中抽帧</li><li>每帧送入 <strong>CLIP ViT-L/14</strong>，得到帧级视觉 token（包括 [CLS] 表示）</li></ul></li></ul><hr><ol start="2"><li>语义一致性分段（Semantic Consistency Segmentation）</li></ol><ul><li>目标：避免固定长度切片，做到“语义稳定 → 粗切，语义突变 → 细切”</li><li>方法： <ol><li>使用 CLIP [CLS] embedding 表征每帧</li><li>计算 <strong>Depth Score</strong>： <ul><li>$D_t = 1 - \cos(f_t, f_{t+1})$</li></ul></li><li>在窗口内找到局部最大值作为 <strong>分段边界</strong></li></ol></li><li>输出：动态长度的片段 $${X_1, X_2, \ldots, X_i}$$</li></ul><hr><ol start="3"><li>动态压缩率分配</li></ol><ul><li>每个片段 $X_i$$ 分配一个压缩率 $ $\alpha_i $ ： <ul><li>语义复杂 (Depth Score 高) → 小 $\alpha$ → 更多 VST</li><li>语义平稳 (Depth Score 低) → 大 $\alpha$ → 更少 VST</li></ul></li></ul><hr><ol start="4"><li>插入 Visual Summarization Token (VST)</li></ol><ul><li>在片段 $X_i$ 中，每隔 $\alpha_i$ 个视觉 token 插入一个 VST： <ul><li>公式：$ k_i = |X_i| / \alpha_i $$</li><li>插入后得到：<br> $$X&#39;<em i1="">i = [x</em>, ..., x_{i\alpha_i}, \langle vs \rangle_{i1}, ..., x_{i|X_i|}, \langle vs \rangle_{ik_i}]$$</li></ul></li></ul><hr><ol start="5"><li>LLM 编码 + KV 压缩</li></ol><ul><li>基础模型：<strong>Qwen2-7B</strong></li><li>编码流程： <ol><li>将片段 $X&#39;_i$ 送入 LLM，所有 token（包括 VST）参与多头自注意力</li><li>片段结束后： <ul><li><strong>丢弃普通视觉 token 的 K,V</strong></li><li><strong>保留 VST 的 K,V</strong> 作为片段的压缩记忆</li></ul></li><li>编码下一个片段 $X_{i+1}$ 时： <ul><li>仅能 attend 到之前所有 VST 的 KV（作为代理）</li></ul></li></ol></li></ul><hr><ol start="6"><li>累积记忆 &amp; 长视频建模</li></ol><ul><li>VST KV-cache 在不同片段间累积，形成“压缩版的全局记忆”</li><li>优点： <ul><li>上下文长度消耗 ~ 与 VST 数量线性相关</li><li>避免原始视觉 token 的二次复杂度 &amp; 巨大显存开销</li></ul></li></ul><hr><ol start="7"><li>训练策略</li></ol><ul><li><strong>指令微调</strong>：统一图像/多图/视频任务</li><li><strong>课程学习</strong>：从小压缩比 (2×/4×) → 大压缩比 (8×/16×)，逐步适应</li><li><strong>合成数据 VICO</strong>：基于视频分段字幕生成跨段 QA，用于训练长视频推理能力</li></ul><hr><ol start="8"><li>整体特点</li></ol><ul><li>基于 LLaVA-like MLLM，而非专门的 Video-Encoder</li><li>创新点：<strong>LLM 内部的 KV 压缩 (VST) + 动态自适应分段</strong></li><li>能在单卡上处理 <strong>小时级视频 (2000+ 帧)</strong>，同时保持细粒度推理能力</li></ul><h1 id="streammem-query-agnostic-kv-cache-memory-for-streaming-video-understanding" tabindex="-1">StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding <a class="header-anchor" href="#streammem-query-agnostic-kv-cache-memory-for-streaming-video-understanding" aria-label="Permalink to &quot;StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding&quot;">​</a></h1><table tabindex="0"><thead><tr><th>模块</th><th>内容</th></tr></thead><tbody><tr><td><strong>输入</strong></td><td>流式视频（逐段到达的帧序列）</td></tr><tr><td><strong>输出</strong></td><td>视频问答 / 推理结果（QA、检索、排序等任务）</td></tr><tr><td><strong>主干网络 (Backbone)</strong></td><td>现有 MLLMs（如 <strong>LLaVA-OneVision, Qwen2-VL, Qwen2.5-VL</strong>），<strong>StreamMem 作为插件，不需再训练</strong></td></tr><tr><td><strong>流程概述</strong></td><td>1. 视频帧流式输入 2. 输入帧压缩（减少冗余） 3. 送入视觉编码器 → 投影到 MLLM 4. 每段计算 KV-cache → 通过 <strong>Saliency Metric</strong> 选择重要 KV 5. <strong>Frame-wise KV merging</strong> 构建帧级 prototype 表示 6. 累积形成 <strong>紧凑 KV memory</strong> 7. QA 阶段利用 KV memory 进行推理</td></tr><tr><td><strong>KV 压缩机制</strong></td><td>- <strong>显著性筛选 (Saliency Metric)</strong>：基于 visual token 与 chat template token 的 cross-attention 分数，选出重要 KV - <strong>输入压缩</strong>：减少重复帧输入 - <strong>KV 合并</strong>：相似 KV 合并为 prototype，保持多样性同时节省内存</td></tr><tr><td><strong>特点</strong></td><td>- <strong>Query-agnostic</strong>：不依赖提前知道问题，压缩时通用保留信息 - <strong>Training-free</strong>：无需微调，可直接接入任意 MLLM - <strong>Bounded Memory</strong>：内存占用随视频长度不增长，避免 OOM</td></tr><tr><td><strong>应用场景</strong></td><td>- 实时流式视频理解 - 内存受限设备上的长视频推理 - 开放世界的连续视频 Agent</td></tr></tbody></table><table tabindex="0"><thead><tr><th>方面</th><th><strong>Video-XL: 动态压缩率分配</strong></th><th><strong>StreamMem: Saliency Metric</strong></th></tr></thead><tbody><tr><td><strong>触发时机</strong></td><td>在 <strong>视频分段阶段</strong> 就决定压缩率</td><td>在 <strong>KV 压缩阶段</strong> 选择哪些 KV 保留</td></tr><tr><td><strong>依据信号</strong></td><td>- <strong>Depth Score</strong>：基于相邻帧的 CLIP [CLS] 余弦相似度差 - 捕捉语义突变（动作切换点）</td><td>- <strong>Cross-attention 分数</strong>：视觉 token 与 chat template token 的注意力强度 - 捕捉哪些视觉 token 对语言理解更有用</td></tr><tr><td><strong>压缩方式</strong></td><td>- 动态设置 <strong>插入 VST 的频率</strong> - 语义复杂 → 更多 VST，保留更细信息 - 语义平稳 → 更少 VST，节省内存</td><td>- 从已有的 KV-cache 中 <strong>选择/合并重要 token</strong> - 保留显著 token 的 KV - 合并冗余 KV 为 prototype</td></tr><tr><td><strong>目标</strong></td><td>- 自适应分配压缩比，保证语义边界处信息密集 - <strong>“结构级别”压缩</strong>（按片段分配 VST 数量）</td><td>- 无需 query，也能在 KV-cache 中挑选关键信息 - <strong>“内容级别”压缩</strong>（按 token 显著性筛选）</td></tr><tr><td><strong>优势</strong></td><td>- 对“语义边界”敏感，能在动作切换点保留更多细节 - 易解释（Depth Score 可视化清晰）</td><td>- 与 query 无关，支持 <strong>流式输入</strong> - 不需重新切片，能在线动态选择 KV</td></tr><tr><td><strong>局限</strong></td><td>- 需要提前做分段（offline） - 偏离线场景，不适合纯 streaming</td><td>- 注意力显著性可能不等于任务显著性 - 可能保留了一些“假重要”的 token</td></tr></tbody></table><h1 id="qwen2-5-omni-end-to-end-multimodal-model" tabindex="-1">Qwen2.5-Omni: End-to-End Multimodal Model <a class="header-anchor" href="#qwen2-5-omni-end-to-end-multimodal-model" aria-label="Permalink to &quot;Qwen2.5-Omni: End-to-End Multimodal Model&quot;">​</a></h1><h2 id="模型特点" tabindex="-1">模型特点 <a class="header-anchor" href="#模型特点" aria-label="Permalink to &quot;模型特点&quot;">​</a></h2><ul><li><strong>输入模态</strong>：文本、图像、音频、视频</li><li><strong>输出模态</strong>：文本 + 语音（自然语音，支持流式生成）</li><li><strong>流式处理</strong>：支持输入和输出的实时流式处理</li></ul><h2 id="核心方法" tabindex="-1">核心方法 <a class="header-anchor" href="#核心方法" aria-label="Permalink to &quot;核心方法&quot;">​</a></h2><ol><li><p><strong>Block-wise Processing</strong></p><ul><li>音频和视频编码器采用分块处理</li><li>编码器负责感知 → LLM 负责长序列建模</li><li>实现高效解耦和跨模态融合</li></ul></li><li><p><strong>TMRoPE (Time-aligned Multimodal RoPE)</strong></p><ul><li>新的时间对齐位置编码</li><li>将音频与视频帧按时间顺序交错排列，实现跨模态时间同步</li></ul></li><li><p><strong>Thinker–Talker 架构</strong></p><ul><li><strong>Thinker</strong>：LLM，负责文本生成</li><li><strong>Talker</strong>：双轨自回归模型，直接基于 Thinker 的隐表示生成音频 token</li><li>两者端到端联合训练与推理，避免模态间干扰</li></ul></li><li><p><strong>Streaming Audio Decoding</strong></p><ul><li>使用滑动窗口 DiT（Diffusion Transformer）</li><li>限制感受野以降低初始延迟，提高流式语音生成体验</li></ul></li></ol><h2 id="实验结果-1" tabindex="-1">实验结果 <a class="header-anchor" href="#实验结果-1" aria-label="Permalink to &quot;实验结果&quot;">​</a></h2><ul><li>与 <strong>Qwen2.5-VL</strong> 相当，优于 <strong>Qwen2-Audio</strong></li><li>在 <strong>Omni-Bench</strong> 等多模态基准上达到 SOTA</li><li>语音指令跟随任务效果接近文本输入能力（MMLU, GSM8K）</li><li>流式语音生成在稳健性与自然度上超越大多数现有方法（包括非流式）</li></ul><h2 id="关键贡献" tabindex="-1">关键贡献 <a class="header-anchor" href="#关键贡献" aria-label="Permalink to &quot;关键贡献&quot;">​</a></h2><ul><li>首个端到端 <strong>流式多模态输入–输出</strong> 模型</li><li>提出 <strong>TMRoPE</strong> 解决跨模态时间同步</li><li>提出 <strong>Thinker–Talker</strong> 架构实现文本与语音的并行生成</li><li>推出高效 <strong>流式语音解码</strong> 方法，降低延迟，提升交互体验</li></ul><p><strong>不同模态的处理方式</strong></p><ul><li><strong>Text 文本</strong><ul><li>只用 1D 的位置 ID（等价于普通的 RoPE）。</li></ul></li><li><strong>Audio 音频</strong><ul><li>每 40ms 对应一个 temporal ID。</li><li>相当于：音频序列直接按时间轴对齐。</li></ul></li><li><strong>Image 图像</strong><ul><li>每个图像 token 的 temporal ID 相同（因为一张图像就是一个时间点）。</li><li>高度、宽度的 ID 则根据在图像中的位置分配（2D RoPE）。</li></ul></li><li><strong>Video 视频</strong><ul><li>视为多张图像序列。</li><li>每帧视频有一个<strong>独立的 temporal ID</strong>（随时间推移递增）。</li><li><strong>高度、宽度依然按图像 token 的 2D 位置分配</strong>。</li><li><strong>帧率不固定时</strong>：temporal ID 会根据实际时间调整，<strong>确保 40ms 对应 1 个 ID，与音频对齐。</strong></li></ul></li></ul><p>在 TMRoPE 中，视频被视作一系列图像，每一帧除了<strong>分配高度和宽度位置 ID</strong> 外，还会根据<strong>时间顺序获得递增的时间 ID</strong>。由于视频帧率并不固定，TMRoPE 会依据帧的实际时间戳<strong>动态调整时间 ID</strong>，确保始终遵循“一个时间 ID 对应 40ms”的规则。这样，<strong>视频帧的时间 ID 能与音频的时间 ID 对齐</strong>，<strong>实现跨模态的精确时序同步</strong>，从而保证模型在多模态输入下既能<strong>捕捉空间结构</strong>，又能保持<strong>时间一致性</strong>。</p><table tabindex="0"><thead><tr><th>阶段</th><th>训练策略</th><th>数据规模 &amp; 类型</th><th>目标</th></tr></thead><tbody><tr><td><strong>阶段一</strong>：模态编码器适配</td><td>- 冻结 LLM- 仅训练视觉/音频编码器（先 adapter 后 encoder）</td><td>- 图文对- 音频-文本对</td><td>- 建立视觉-文本、音频-文本的语义对齐- 为 LLM 融合多模态奠定基础</td></tr><tr><td><strong>阶段二</strong>：多模态联合训练</td><td>- 解冻所有参数，全模型训练</td><td>- 图像/视频相关 <strong>8000 亿 tokens</strong>- 音频相关 <strong>3000 亿 tokens</strong>- 音视频联合 <strong>1000 亿 tokens</strong>- 纯文本数据</td><td>- 跨模态交互与理解- 多任务学习能力- 保持/提升语言能力</td></tr><tr><td><strong>阶段三</strong>：长序列训练</td><td>- 扩展序列长度到 32,768 tokens</td><td>- 长文本- 长音频- 长视频- 长图像序列</td><td>- 增强复杂长序列建模能力- 适应真实场景下的长时依赖</td></tr></tbody></table><h2 id="qwen-vl-视频能力分析" tabindex="-1">Qwen-VL 视频能力分析 <a class="header-anchor" href="#qwen-vl-视频能力分析" aria-label="Permalink to &quot;Qwen-VL 视频能力分析&quot;">​</a></h2><table tabindex="0"><thead><tr><th>方面</th><th>优势</th><th>缺陷</th></tr></thead><tbody><tr><td><strong>时间建模</strong></td><td>- 动态 FPS 采样，避免冗余帧- MRoPE 与绝对时间对齐，可进行秒级事件定位</td><td>- 没有显式记忆机制，无法保持跨片段的长时依赖- 跨场景因果推理能力有限</td></tr><tr><td><strong>Token 处理</strong></td><td>- 14×14 patch 切分 + 2×2 merge + MLP 压缩，显著减少 token 数- 支持更长视频输入（最高 32k tokens）</td><td>- 压缩导致细节丢失（如小目标跟踪、动作细节理解）- 对高精度需求的视频任务可能性能下降</td></tr><tr><td><strong>注意力机制</strong></td><td>- 窗口注意力降低计算复杂度，仅少数层全局 self-attention- 能处理分钟级视频而保持效率</td><td>- 全局一致性弱，长视频中不同片段之间的语义连接不稳固</td></tr><tr><td><strong>长序列支持</strong></td><td>- 最终阶段训练扩展至 32k tokens，覆盖长视频、长音频数据- 在 LongVideoBench、LVBench 等基准超越 GPT-4o</td><td>- 32k tokens 仍有限制，无法端到端覆盖小时级超长视频- 实际需依赖分段/滑动窗口处理</td></tr><tr><td><strong>整体表现</strong></td><td>- 在分钟级长视频（几千帧）中具备强时序理解和事件定位能力</td><td>- 超长视频（&gt;30 分钟至小时级）理解力不足，易片段化、遗忘全局语义</td></tr></tbody></table><h3 id="边界与缺陷" tabindex="-1">边界与缺陷 <a class="header-anchor" href="#边界与缺陷" aria-label="Permalink to &quot;边界与缺陷&quot;">​</a></h3><ul><li><strong>记忆机制缺失</strong>：依赖 <strong>token 压缩 + 窗口 attention</strong>，没有引入 memory bank / 分层时序建模 → 在跨场景、长时依赖视频任务上仍会失效。</li><li><strong>小时级视频 ≠ 全局建模</strong>：虽然可以输入小时级视频，但受限于 <strong>32k token 上限</strong>，无法在端到端条件下完整编码小时级视频（需要分块/滑动窗口）。</li><li><strong>细粒度信息丢失</strong>：2×2 patch merge 等压缩方式减少了时空分辨率 → 对精细动作、细微物体变化不够敏感。</li><li><strong>跨片段推理能力有限</strong>：在需要全局语义一致性、因果链路建模的任务（如电影剧情问答）上存在弱点。</li></ul></div></div></main><footer class="VPDocFooter" data-v-39a288b8 data-v-e257564d><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-e257564d><span class="visually-hidden" id="doc-footer-aria-label" data-v-e257564d>Pager</span><div class="pager" data-v-e257564d><a class="VPLink link pager-link prev" href="/50-reports/%E8%BF%91%E6%9C%9F%E5%8F%91%E5%B8%83%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%A0%94-0908.html" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Previous page</span><span class="title" data-v-e257564d>近期发布模型调研-0908</span><!--]--></a></div><div class="pager" data-v-e257564d><a class="VPLink link pager-link next" href="/50-reports/%E9%AB%98%E7%86%B5RL%20-%20%E6%95%B0%E6%8D%AE%E6%9E%84%E5%BB%BA%E5%A2%9E%E7%9B%8A.html" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Next page</span><span class="title" data-v-e257564d>高熵RL - 数据构建增益</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-5d98c3a5 data-v-e315a0ad><div class="container" data-v-e315a0ad><p class="message" data-v-e315a0ad>Released under the MIT License.</p><p class="copyright" data-v-e315a0ad>Copyright © 2025 Wang Yaqi</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"00-inbox_index.md\":\"BxMK2y0K\",\"00-inbox_readme.md\":\"RyqrPFdx\",\"00-inbox_week-34_triage.md\":\"DVk_gK-E\",\"00-inbox_week-35_triage.md\":\"CJ7x_I7h\",\"00-inbox_week-36_triage.md\":\"Bw37u8Cj\",\"00-inbox_week-37_250908-0020-clip损失.md\":\"D6cejIoN\",\"00-inbox_week-37_250908-0020-moe llama结构.md\":\"_N3i4Vl_\",\"00-inbox_week-37_250908-0020-siglip损失.md\":\"DuDR7RnK\",\"00-inbox_week-37_250909-0023-dino语义监督.md\":\"Dx3ylhPa\",\"00-inbox_week-37_250910-0024-clip-siglip-vicreg损失区别与适用场景.md\":\"C2M3PCzT\",\"00-inbox_week-37_250910-0024-训练loss调试思路.md\":\"CGjZKtrV\",\"00-inbox_week-38_250918-0024-0916talk.md\":\"CAnJpmsX\",\"00-inbox_week-38_250920-0022-check_code.md\":\"B8OVOP6a\",\"00-inbox_week-39_250919-0024-0919-代码框架问题排查.md\":\"BPGxg1yy\",\"00-inbox_week-39_250924-0024-0922-ocr-todolist.md\":\"B57-S-36\",\"00-inbox_week-39_250925-0025-0923-复现crossflow.md\":\"cxp65mul\",\"00-inbox_week-39_250925-0025-语义调音.md\":\"Cmh-y7Ll\",\"00-inbox_week-41_251010-0025-0922-ocr-todolist.md\":\"CoLfL63b\",\"00-inbox_week-41_251010-0025-0923-复现crossflow.md\":\"DKn9e9Z3\",\"00-inbox_week-41_251010-0025-dots_ocr服务部署 _ 调用.md\":\"CsnpHxq_\",\"00-inbox_week-41_251010-0025-laion400m训练数据处理.md\":\"BkdfG2q1\",\"00-inbox_week-41_251010-0025-语义调音.md\":\"C8I2YM0Z\",\"00-inbox_week-45_251027-1431-vla-vla调研分享.md\":\"DiWBICAy\",\"00-inbox_week-45_251104-1431-ocr任务-docx等格式文档检测.md\":\"ClznCRj-\",\"00-inbox_week-45_251104-1431-vla-扩散生成调研.md\":\"Dc_BUAti\",\"00-inbox_week-45_251104-1431-vla-综述论文节选翻译-0925发布.md\":\"DlcaK2ts\",\"00-inbox_week-46_251111-0027-k线记忆卡.md\":\"CjGZKdiV\",\"00-inbox_week-50_251113-1650-自动交易策略实现log.md\":\"C0_jSP06\",\"00-inbox_week-50_251211-1650-deepseekv3_2.md\":\"CW71BT6n\",\"00-inbox_week-50_251211-1650-jit论文-视频要点.md\":\"wGtIpzah\",\"00-inbox_week-50_251211-1650-rustdesk安装及配置.md\":\"b3esiVId\",\"00-inbox_week-50_251211-1650-trm应用于vla.md\":\"-yGNCErp\",\"00-inbox_week-50_251211-1650-trm应用大纲.md\":\"CnrnC4gM\",\"00-inbox_week-50_251211-1650-trm论文精读笔记.md\":\"BpdLK_Tu\",\"00-inbox_week-50_251211-1650-vae原理.md\":\"BYNqrid7\",\"00-inbox_week-50_251211-1650-图像生成基座模型调研.md\":\"DGL06d4n\",\"00-inbox_week-50_251211-1650-场景理解分类功能.md\":\"CpvDMYk-\",\"10-knowledge_250822-0737-重启训练一个半小时内不开始训-overview.md\":\"Chmk5W5x\",\"10-knowledge_250826-0737-i2i训练方案-overview.md\":\"ndnUB9Hh\",\"10-knowledge_250826-0737-openuni训练方法-overview.md\":\"Bz71xLqy\",\"10-knowledge_250827-0736-token熵-overview.md\":\"B1Mv5o5q\",\"10-knowledge_250827-0736-扩散过程中打印数值-overview.md\":\"2CZHDGmA\",\"10-knowledge_250828-0735-fm_transformers模型参数-overview.md\":\"BZZWEWfO\",\"10-knowledge_250829-0735-困惑度-overview.md\":\"BQF8-SBk\",\"10-knowledge_250829-0735-惊讶度-overview.md\":\"DlUdKycc\",\"10-knowledge_250830-0735-pipeline检查-overview.md\":\"DfRDzeKl\",\"10-knowledge_250903-0735-macos快捷操作.md\":\"DGlbZob2\",\"10-knowledge_250906-0735-vicreg监督.md\":\"DRzQqJ3b\",\"10-knowledge_250906-0735-投影层方案.md\":\"7iWiO02D\",\"10-knowledge_250907-0735-特征解耦方案gpt参考.md\":\"UwEg4l-R\",\"10-knowledge_example-knowledge.md\":\"D90jZDvZ\",\"10-knowledge_index.md\":\"DX2HZJ0R\",\"10-knowledge_readme.md\":\"DEZ7OggO\",\"20-papers_2025_2025-08-20-.md\":\"OPzruvQ2\",\"20-papers_2025_2025-08-20-vtla-preference-learning.md\":\"Dz6vUCR_\",\"20-papers_250829-0735-qwen2.5-omni论文阅读.md\":\"BUa0mMWP\",\"20-papers_index.md\":\"CgQW4AZg\",\"20-papers_vla_all_papers.md\":\"5XYncC_w\",\"30-ideas_2025_9月_250826-0737-与弘扬讨论-overview.md\":\"BNUaUdAg\",\"30-ideas_2025_9月_250826-0737-现有模型结构可改进点-overview.md\":\"BUFGBCsU\",\"30-ideas_2025_9月_250827-0736-0826-与弘扬沟通-overview.md\":\"BJckJ-Fw\",\"30-ideas_2025_9月_250829-0735-中间层模型方案梳理-overview.md\":\"BB7Rf6Gc\",\"30-ideas_2025_9月_250829-0735-论文理论包装方案.md\":\"DK30ycFI\",\"30-ideas_2025_9月_250904-0735-多轮对话拓展idea-overview.md\":\"B_AZd7pL\",\"30-ideas_2025_9月_250906-0735-0905_今日晨思.md\":\"DeW5zvzz\",\"30-ideas_2025_9月_250907-0735-梦梦的建议.md\":\"b2uAz91C\",\"30-ideas_2025_9月_week36-论文实验idea汇总.md\":\"DO5mY2KW\",\"30-ideas_index.md\":\"BLsaRW7n\",\"40-experiments_250826-0736-0826测评日志.md\":\"Ceca8mbE\",\"40-experiments_250902-0735-0901实验分析.md\":\"l-G-GhLH\",\"40-experiments_250904-0735-exp_card_0903_transencodder.md\":\"NlMuTFDx\",\"40-experiments_250904-0735-实验日志模板.md\":\"B9zK1IYR\",\"40-experiments_exp_card_0908_dinov2.md\":\"3Vfhs2r5\",\"40-experiments_index.md\":\"sKDrNhTt\",\"50-reports_250820-0754-ocr调研.md\":\"CJ40cd-y\",\"50-reports_250827-0736-高熵强化学习.md\":\"DC-sefpp\",\"50-reports_250901-0735-resume.md\":\"DtNSBA5Y\",\"50-reports_250903-0735-座舱vla端云协同方案.md\":\"C4iR_FDg\",\"50-reports_index.md\":\"BaF-7d1_\",\"50-reports_resume_250901-0735-resume_network.md\":\"CxyKg_Il\",\"50-reports_座舱vla端云协同方案.md\":\"DuVATegy\",\"50-reports_文本扩散模型调研.md\":\"DEi7WSBa\",\"50-reports_训练与部署资源申请书.md\":\"DUY8Bcs6\",\"50-reports_近期发布模型调研-0908.md\":\"Q46srq98\",\"50-reports_长视频理解综述.md\":\"BseMc13e\",\"50-reports_高熵rl - 数据构建增益.md\":\"DU4AIY7y\",\"index.md\":\"CEbDhJUx\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"ZhiGrove\",\"description\":\"Wang Yaqi's Knowledge Base\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"收件箱\",\"link\":\"/00-inbox/\"},{\"text\":\"知识库\",\"link\":\"/10-knowledge/\"},{\"text\":\"论文\",\"link\":\"/20-papers/\"},{\"text\":\"灵感\",\"link\":\"/30-ideas/\"},{\"text\":\"实验\",\"link\":\"/40-experiments/\"},{\"text\":\"报告\",\"link\":\"/50-reports/\"}],\"sidebar\":{\"/00-inbox/\":[{\"text\":\"Inbox\",\"items\":[{\"text\":\"README\",\"link\":\"/00-inbox/\"},{\"text\":\"week-34\",\"collapsed\":true,\"items\":[{\"text\":\"TRIAGE\",\"link\":\"/00-inbox/week-34/TRIAGE\"}]},{\"text\":\"week-35\",\"collapsed\":true,\"items\":[{\"text\":\"TRIAGE\",\"link\":\"/00-inbox/week-35/TRIAGE\"}]},{\"text\":\"week-36\",\"collapsed\":true,\"items\":[{\"text\":\"TRIAGE\",\"link\":\"/00-inbox/week-36/TRIAGE\"}]},{\"text\":\"week-37\",\"collapsed\":true,\"items\":[{\"text\":\"250908-0020-MoE llama结构\",\"link\":\"/00-inbox/week-37/250908-0020-MoE llama结构\"},{\"text\":\"250908-0020-clip损失\",\"link\":\"/00-inbox/week-37/250908-0020-clip损失\"},{\"text\":\"250908-0020-siglip损失\",\"link\":\"/00-inbox/week-37/250908-0020-siglip损失\"},{\"text\":\"250909-0023-dino语义监督\",\"link\":\"/00-inbox/week-37/250909-0023-dino语义监督\"},{\"text\":\"250910-0024-clip-siglip-vicreg损失区别与适用场景\",\"link\":\"/00-inbox/week-37/250910-0024-clip-siglip-vicreg损失区别与适用场景\"},{\"text\":\"250910-0024-训练loss调试思路\",\"link\":\"/00-inbox/week-37/250910-0024-训练loss调试思路\"}]},{\"text\":\"week-38\",\"collapsed\":true,\"items\":[{\"text\":\"250918-0024-0916talk\",\"link\":\"/00-inbox/week-38/250918-0024-0916talk\"},{\"text\":\"250920-0022-check_code\",\"link\":\"/00-inbox/week-38/250920-0022-check_code\"}]},{\"text\":\"week-39\",\"collapsed\":true,\"items\":[{\"text\":\"250919-0024-0919-代码框架问题排查\",\"link\":\"/00-inbox/week-39/250919-0024-0919-代码框架问题排查\"},{\"text\":\"250924-0024-0922-ocr-TODOlist\",\"link\":\"/00-inbox/week-39/250924-0024-0922-ocr-TODOlist\"},{\"text\":\"250925-0025-0923-复现crossflow\",\"link\":\"/00-inbox/week-39/250925-0025-0923-复现crossflow\"},{\"text\":\"250925-0025-语义调音\",\"link\":\"/00-inbox/week-39/250925-0025-语义调音\"}]},{\"text\":\"week-41\",\"collapsed\":true,\"items\":[{\"text\":\"251010-0025-0922-ocr-TODOlist\",\"link\":\"/00-inbox/week-41/251010-0025-0922-ocr-TODOlist\"},{\"text\":\"251010-0025-0923-复现crossflow\",\"link\":\"/00-inbox/week-41/251010-0025-0923-复现crossflow\"},{\"text\":\"251010-0025-dots_ocr服务部署 & 调用\",\"link\":\"/00-inbox/week-41/251010-0025-dots_ocr服务部署 & 调用\"},{\"text\":\"251010-0025-laion400m训练数据处理\",\"link\":\"/00-inbox/week-41/251010-0025-laion400m训练数据处理\"},{\"text\":\"251010-0025-语义调音\",\"link\":\"/00-inbox/week-41/251010-0025-语义调音\"}]},{\"text\":\"week-45\",\"collapsed\":true,\"items\":[{\"text\":\"251027-1431-VLA-VLA调研分享\",\"link\":\"/00-inbox/week-45/251027-1431-VLA-VLA调研分享\"},{\"text\":\"251104-1431-OCR任务-docx等格式文档检测\",\"link\":\"/00-inbox/week-45/251104-1431-OCR任务-docx等格式文档检测\"},{\"text\":\"251104-1431-VLA-扩散生成调研\",\"link\":\"/00-inbox/week-45/251104-1431-VLA-扩散生成调研\"},{\"text\":\"251104-1431-VLA-综述论文节选翻译-0925发布\",\"link\":\"/00-inbox/week-45/251104-1431-VLA-综述论文节选翻译-0925发布\"}]},{\"text\":\"week-46\",\"collapsed\":true,\"items\":[{\"text\":\"251111-0027-k线记忆卡\",\"link\":\"/00-inbox/week-46/251111-0027-k线记忆卡\"}]},{\"text\":\"week-50\",\"collapsed\":true,\"items\":[{\"text\":\"251113-1650-自动交易策略实现log\",\"link\":\"/00-inbox/week-50/251113-1650-自动交易策略实现log\"},{\"text\":\"251211-1650-JiT论文-视频要点\",\"link\":\"/00-inbox/week-50/251211-1650-JiT论文-视频要点\"},{\"text\":\"251211-1650-TRM应用于VLA\",\"link\":\"/00-inbox/week-50/251211-1650-TRM应用于VLA\"},{\"text\":\"251211-1650-TRM应用大纲\",\"link\":\"/00-inbox/week-50/251211-1650-TRM应用大纲\"},{\"text\":\"251211-1650-TRM论文精读笔记\",\"link\":\"/00-inbox/week-50/251211-1650-TRM论文精读笔记\"},{\"text\":\"251211-1650-VAE原理\",\"link\":\"/00-inbox/week-50/251211-1650-VAE原理\"},{\"text\":\"251211-1650-deepseekV3_2\",\"link\":\"/00-inbox/week-50/251211-1650-deepseekV3_2\"},{\"text\":\"251211-1650-rustdesk安装及配置\",\"link\":\"/00-inbox/week-50/251211-1650-rustdesk安装及配置\"},{\"text\":\"251211-1650-图像生成基座模型调研\",\"link\":\"/00-inbox/week-50/251211-1650-图像生成基座模型调研\"},{\"text\":\"251211-1650-场景理解分类功能\",\"link\":\"/00-inbox/week-50/251211-1650-场景理解分类功能\"}]}]}],\"/10-knowledge/\":[{\"text\":\"Knowledge\",\"items\":[{\"text\":\"README\",\"link\":\"/10-knowledge/\"},{\"text\":\"250822-0737-重启训练一个半小时内不开始训-overview\",\"link\":\"/10-knowledge/250822-0737-重启训练一个半小时内不开始训-overview\"},{\"text\":\"250826-0737-OpenUni训练方法-overview\",\"link\":\"/10-knowledge/250826-0737-OpenUni训练方法-overview\"},{\"text\":\"250826-0737-i2i训练方案-overview\",\"link\":\"/10-knowledge/250826-0737-i2i训练方案-overview\"},{\"text\":\"250827-0736-token熵-overview\",\"link\":\"/10-knowledge/250827-0736-token熵-overview\"},{\"text\":\"250827-0736-扩散过程中打印数值-overview\",\"link\":\"/10-knowledge/250827-0736-扩散过程中打印数值-overview\"},{\"text\":\"250828-0735-fm_transformers模型参数-overview\",\"link\":\"/10-knowledge/250828-0735-fm_transformers模型参数-overview\"},{\"text\":\"250829-0735-困惑度-overview\",\"link\":\"/10-knowledge/250829-0735-困惑度-overview\"},{\"text\":\"250829-0735-惊讶度-overview\",\"link\":\"/10-knowledge/250829-0735-惊讶度-overview\"},{\"text\":\"250830-0735-pipeline检查-overview\",\"link\":\"/10-knowledge/250830-0735-pipeline检查-overview\"},{\"text\":\"250903-0735-macos快捷操作\",\"link\":\"/10-knowledge/250903-0735-macos快捷操作\"},{\"text\":\"250906-0735-VICReg监督\",\"link\":\"/10-knowledge/250906-0735-VICReg监督\"},{\"text\":\"250906-0735-投影层方案\",\"link\":\"/10-knowledge/250906-0735-投影层方案\"},{\"text\":\"250907-0735-特征解耦方案GPT参考\",\"link\":\"/10-knowledge/250907-0735-特征解耦方案GPT参考\"},{\"text\":\"example-knowledge\",\"link\":\"/10-knowledge/example-knowledge\"}]}],\"/20-papers/\":[{\"text\":\"Papers\",\"items\":[{\"text\":\"README\",\"link\":\"/20-papers/\"},{\"text\":\"250829-0735-Qwen2.5-Omni论文阅读\",\"link\":\"/20-papers/250829-0735-Qwen2.5-Omni论文阅读\"},{\"text\":\"2025\",\"collapsed\":true,\"items\":[{\"text\":\"2025-08-20-\",\"link\":\"/20-papers/2025/2025-08-20-\"},{\"text\":\"2025-08-20-vtla-preference-learning\",\"link\":\"/20-papers/2025/2025-08-20-vtla-preference-learning\"}]},{\"text\":\"vla\",\"collapsed\":true,\"items\":[{\"text\":\"all_papers\",\"link\":\"/20-papers/vla/all_papers\"}]}]}],\"/30-ideas/\":[{\"text\":\"Ideas\",\"items\":[{\"text\":\"README\",\"link\":\"/30-ideas/\"},{\"text\":\"2025\",\"collapsed\":true,\"items\":[{\"text\":\"9月\",\"collapsed\":true,\"items\":[{\"text\":\"250826-0737-与弘扬讨论-overview\",\"link\":\"/30-ideas/2025/9月/250826-0737-与弘扬讨论-overview\"},{\"text\":\"250826-0737-现有模型结构可改进点-overview\",\"link\":\"/30-ideas/2025/9月/250826-0737-现有模型结构可改进点-overview\"},{\"text\":\"250827-0736-0826-与弘扬沟通-overview\",\"link\":\"/30-ideas/2025/9月/250827-0736-0826-与弘扬沟通-overview\"},{\"text\":\"250829-0735-中间层模型方案梳理-overview\",\"link\":\"/30-ideas/2025/9月/250829-0735-中间层模型方案梳理-overview\"},{\"text\":\"250829-0735-论文理论包装方案\",\"link\":\"/30-ideas/2025/9月/250829-0735-论文理论包装方案\"},{\"text\":\"250904-0735-多轮对话拓展idea-overview\",\"link\":\"/30-ideas/2025/9月/250904-0735-多轮对话拓展idea-overview\"},{\"text\":\"250906-0735-0905_今日晨思\",\"link\":\"/30-ideas/2025/9月/250906-0735-0905_今日晨思\"},{\"text\":\"250907-0735-梦梦的建议\",\"link\":\"/30-ideas/2025/9月/250907-0735-梦梦的建议\"},{\"text\":\"week36-论文实验idea汇总\",\"link\":\"/30-ideas/2025/9月/week36-论文实验idea汇总\"}]}]}]}],\"/40-experiments/\":[{\"text\":\"Experiments\",\"items\":[{\"text\":\"README\",\"link\":\"/40-experiments/\"},{\"text\":\"250826-0736-0826测评日志\",\"link\":\"/40-experiments/250826-0736-0826测评日志\"},{\"text\":\"250902-0735-0901实验分析\",\"link\":\"/40-experiments/250902-0735-0901实验分析\"},{\"text\":\"250904-0735-exp_card_0903_transencodder\",\"link\":\"/40-experiments/250904-0735-exp_card_0903_transencodder\"},{\"text\":\"250904-0735-实验日志模板\",\"link\":\"/40-experiments/250904-0735-实验日志模板\"},{\"text\":\"exp_card_0908_dinov2\",\"link\":\"/40-experiments/exp_card_0908_dinov2\"}]}],\"/50-reports/\":[{\"text\":\"Reports\",\"items\":[{\"text\":\"README\",\"link\":\"/50-reports/\"},{\"text\":\"250820-0754-OCR调研\",\"link\":\"/50-reports/250820-0754-OCR调研\"},{\"text\":\"250827-0736-高熵强化学习\",\"link\":\"/50-reports/250827-0736-高熵强化学习\"},{\"text\":\"250901-0735-resume\",\"link\":\"/50-reports/250901-0735-resume\"},{\"text\":\"250903-0735-座舱VLA端云协同方案\",\"link\":\"/50-reports/250903-0735-座舱VLA端云协同方案\"},{\"text\":\"座舱VLA端云协同方案\",\"link\":\"/50-reports/座舱VLA端云协同方案\"},{\"text\":\"文本扩散模型调研\",\"link\":\"/50-reports/文本扩散模型调研\"},{\"text\":\"训练与部署资源申请书\",\"link\":\"/50-reports/训练与部署资源申请书\"},{\"text\":\"近期发布模型调研-0908\",\"link\":\"/50-reports/近期发布模型调研-0908\"},{\"text\":\"长视频理解综述\",\"link\":\"/50-reports/长视频理解综述\"},{\"text\":\"高熵RL - 数据构建增益\",\"link\":\"/50-reports/高熵RL - 数据构建增益\"},{\"text\":\"resume\",\"collapsed\":true,\"items\":[{\"text\":\"250901-0735-resume_network\",\"link\":\"/50-reports/resume/250901-0735-resume_network\"}]}]}]},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/wangyaqi/ZhiGrove\"}],\"search\":{\"provider\":\"local\"},\"footer\":{\"message\":\"Released under the MIT License.\",\"copyright\":\"Copyright © 2025 Wang Yaqi\"}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>