import{_ as e,c as a,o as r,ag as i}from"./chunks/framework.CQuhCYrb.js";const c=JSON.parse('{"title":"i2i训练方案","description":"","frontmatter":{"title":"i2i训练方案","created":"2025-09-07 17:01","updated":"2025-09-07T00:00:00.000Z","origin":"week-35","type":"knowledge","status":"draft","tags":["i2i"],"links":[]},"headers":[],"relativePath":"10-knowledge/250826-0737-i2i训练方案-overview.md","filePath":"10-knowledge/250826-0737-i2i训练方案-overview.md"}'),n={name:"10-knowledge/250826-0737-i2i训练方案-overview.md"};function o(l,t,s,d,h,g){return r(),a("div",null,[...t[0]||(t[0]=[i('<h2 id="tl-dr-≤3点" tabindex="-1">TL;DR（≤3点） <a class="header-anchor" href="#tl-dr-≤3点" aria-label="Permalink to &quot;TL;DR（≤3点）&quot;">​</a></h2><ul><li>OpenUni没有额外的i2i训练数据，只采用了“保持图片不变”的方法进行训练；</li><li>CrossFlow没有开源i2i的训练数据和代码；</li><li></li></ul><h2 id="what-是什么" tabindex="-1">What（是什么） <a class="header-anchor" href="#what-是什么" aria-label="Permalink to &quot;What（是什么）&quot;">​</a></h2><ul><li></li></ul><h2 id="why-为什么这么做-何时使用" tabindex="-1">Why（为什么这么做/何时使用） <a class="header-anchor" href="#why-为什么这么做-何时使用" aria-label="Permalink to &quot;Why（为什么这么做/何时使用）&quot;">​</a></h2><ul><li></li></ul><h2 id="how-最小复现配方-≤5步" tabindex="-1">How（最小复现配方，≤5步） <a class="header-anchor" href="#how-最小复现配方-≤5步" aria-label="Permalink to &quot;How（最小复现配方，≤5步）&quot;">​</a></h2><ul><li></li></ul><h2 id="gotchas-坑点与边界" tabindex="-1">Gotchas（坑点与边界） <a class="header-anchor" href="#gotchas-坑点与边界" aria-label="Permalink to &quot;Gotchas（坑点与边界）&quot;">​</a></h2><ul><li></li></ul><h2 id="raw-notes" tabindex="-1">Raw Notes <a class="header-anchor" href="#raw-notes" aria-label="Permalink to &quot;Raw Notes&quot;">​</a></h2><h1 id="metaquery" tabindex="-1">MetaQuery <a class="header-anchor" href="#metaquery" aria-label="Permalink to &quot;MetaQuery&quot;">​</a></h1><h2 id="_1-训练方法-training-method" tabindex="-1">1）训练方法（Training Method） <a class="header-anchor" href="#_1-训练方法-training-method" aria-label="Permalink to &quot;1）训练方法（Training Method）&quot;">​</a></h2><ul><li><p>总体范式：用一组可学习的 MetaQueries 直接喂给冻结的多模态 LLM（MLLM），取其输出隐表征作为条件，经一个可训练的 Connector对齐到扩散式生成模型（DiT）的条件输入空间；全流程只用标准去噪目标（diffusion denoising objective）在图文配对数据上训练。可选再加入图像重建目标与去噪目标混合训练以获得重建/编辑能力。</p></li><li><p>模块是否训练：默认冻结 MLLM；训练 MetaQuery（可学习查询）+ Connector；扩散解码器（DiT）既可冻结也可微调，论文给出 ablation：只冻 LLM 时已可达高水准；进一步微调 DiT能继续提升画质。</p></li><li><p>Connector 设计：两种结构对比——</p><ul><li><p>Proj‑Enc：先投到 DiT 条件维度再过 Transformer Encoder；</p></li><li><p>Enc‑Proj：先在与 MLLM 隐维相同的维度用 Transformer Encoder 对齐，再投到 DiT 条件维度。 实验更推荐 Enc‑Proj（更省参、效果更好）。Connector 采用双向注意力的 Transformer Encoder。</p></li></ul></li><li><p>训练目标对比：只用 T2I 去噪目标最佳；纯重建目标最差；混合（T2I+重建）可在不明显伤害 T2I 的前提下获得重建/编辑能力。</p></li></ul><h2 id="_2-训练参数-training-hyperparameters" tabindex="-1">2）训练参数（Training Hyperparameters） <a class="header-anchor" href="#_2-训练参数-training-hyperparameters" aria-label="Permalink to &quot;2）训练参数（Training Hyperparameters）&quot;">​</a></h2><h3 id="预训练-pre‐training" tabindex="-1">预训练（Pre‑training） <a class="header-anchor" href="#预训练-pre‐training" aria-label="Permalink to &quot;预训练（Pre‑training）&quot;">​</a></h3><ul><li>数据规模：25M 公开图文对。</li><li>轮数：8 epochs（3.2 概述段落曾写 4 epochs，以 4 节为准）。</li><li>全局 batch size：4096。</li><li>学习率与调度：初始 1e‑4，cosine decay；4000 步 warm‑up；最终衰减到 1e‑5。</li><li>是否冻结：MLLM 冻结；训练 MetaQueries + Connector；DiT 可冻或微调（微调更好）。</li><li>分辨率 / 生成头：文中对齐实验多用 Sana‑0.6B@512 分辨率；COCO FID 评测使用 Stable Diffusion v1.5。</li></ul><h3 id="指令微调-instruction-tuning" tabindex="-1">指令微调（Instruction Tuning） <a class="header-anchor" href="#指令微调-instruction-tuning" aria-label="Permalink to &quot;指令微调（Instruction Tuning）&quot;">​</a></h3><ul><li>数据规模：构建 2.4M “成对图像 + 指令” 样本（见下文数据结构）。</li><li>轮数：3 epochs。</li><li>batch size：2048。</li><li>学习率调度：与预训练相同（cosine + 4k warm‑up）.</li></ul><h3 id="架构-token-侧关键超参" tabindex="-1">架构/Token 侧关键超参 <a class="header-anchor" href="#架构-token-侧关键超参" aria-label="Permalink to &quot;架构/Token 侧关键超参&quot;">​</a></h3><ul><li>MetaQuery 数量（#tokens）：系统性标度实验给出 64 已较好，更大量能进一步提升对齐；论文在模型家族实验中统一设为 256 tokens（博客页明确载明）。</li><li>Connector 层数与维度：推荐 Enc‑Proj，24 层；示例维度 896（Enc‑Proj） vs 2304（Proj‑Enc）；24 层 Enc‑Proj 参数量约 316M。</li><li>背骨与生成头组合：MLLM 采用 LLaVA‑OneVision‑0.5B / Qwen2.5‑VL‑3B / Qwen2.5‑VL‑7B；生成头测试 SD‑v1.5 与 Sana‑1.6B。</li></ul><h2 id="_3-数据结构-data-structure-dataset-construction" tabindex="-1">3）数据结构（Data Structure / Dataset Construction） <a class="header-anchor" href="#_3-数据结构-data-structure-dataset-construction" aria-label="Permalink to &quot;3）数据结构（Data Structure / Dataset Construction）&quot;">​</a></h2><h3 id="a-预训练数据-统一建模的基础对齐" tabindex="-1">A. 预训练数据（统一建模的基础对齐） <a class="header-anchor" href="#a-预训练数据-统一建模的基础对齐" aria-label="Permalink to &quot;A. 预训练数据（统一建模的基础对齐）&quot;">​</a></h3><ul><li>标准 (image, caption) 图文对，规模 25M；用于把冻结的 MLLM 条件通过 Connector 对齐到扩散解码器，仅用去噪目标训练。</li></ul><h3 id="b-指令微调数据-编辑-主体驱动等进阶能力" tabindex="-1">B. 指令微调数据（编辑/主体驱动等进阶能力） <a class="header-anchor" href="#b-指令微调数据-编辑-主体驱动等进阶能力" aria-label="Permalink to &quot;B. 指令微调数据（编辑/主体驱动等进阶能力）&quot;">​</a></h3><ul><li><p>来源：从 mmc4 fewer‑faces 子集拿到“图像 + caption”，用 SigLIP 进行按 caption 相似度聚类（每簇 ≤6，阈值 0.5）；每簇中与其他图像平均相似度最低者设为 target，其余为 source；得到 2.4M 组 (sources, target)。随后用 Qwen2.5‑VL‑3B 为每对生成开放式指令。</p></li><li><p>样本格式： (source_images: 1..N, target_image, instruction_text) —— instruction 要同时描述一条与 sources 的笼统相似点（如“同款上衣/相似斧头/相似建筑”）以及 target 独有的全部差异；不得包含足以单独重建 target 的具体细节（避免泄露/投机），鼓励简洁。</p></li><li><p>用途：在保持 MLLM 冻结的前提下，对 MetaQueries + Connector +（可选）DiT 进行指令微调，获得图像编辑、主体驱动、“视觉联想”“Logo 设计”等能力。</p></li></ul><h3 id="补充" tabindex="-1">补充 <a class="header-anchor" href="#补充" aria-label="Permalink to &quot;补充&quot;">​</a></h3><ul><li>用 可学习查询（MetaQueries） 不仅在画质与对齐上可与“最后一层嵌入”相当/更好，更关键是保留了 MLLM 的“在上下文学习/推理/知识迁移”能力，在需要世界知识与推理的生成上显著更强。</li></ul><h1 id="openuni" tabindex="-1">OpenUni <a class="header-anchor" href="#openuni" aria-label="Permalink to &quot;OpenUni&quot;">​</a></h1><h2 id="训练框架概览" tabindex="-1">训练框架概览 <a class="header-anchor" href="#训练框架概览" aria-label="Permalink to &quot;训练框架概览&quot;">​</a></h2><ul><li>模型结构 OpenUni 建立在**冻结的多模态大语言模型（InternVL3）**与 **扩散模型（SANA DiT）**之间，通过 可学习查询（256 tokens） + 轻量连接器（6 层 Transformer） 实现语义桥接，从而统一理解与图像生成功能。</li></ul><h2 id="一、stage-1-预训练-pre-training" tabindex="-1">一、Stage 1：预训练（Pre-training） <a class="header-anchor" href="#一、stage-1-预训练-pre-training" aria-label="Permalink to &quot;一、Stage 1：预训练（Pre-training）&quot;">​</a></h2><ul><li>目标：仅训练 learnable queries + lightweight connector（LLM 与扩散模型权重均冻结），让连接模块学习将 LLM 输出映射为图像生成条件</li><li>数据来源：共计约 23M 图像-文本对，来自多个公开数据集（text‑to‑image‑2M、LAION‑Aesthetic‑6M、Megalith‑10M、RedCaps‑5M），所有 caption 由 LLM 重写生成。 | 超参数 | 数值 | | ------------- | ------------------ | | 容器 Batch Size | 512 | | 优化器 | AdamW | | 学习率 | $1 \\times 10^{-4}$ | | 权重衰减 | 0.05 | | 梯度裁剪 | 1.0 | | Betas | (0.9, 0.95) | | 学习率调度 | Cosine decay | | Warm‑up Steps | 1,000 | | 总训练步数 | 100,000 steps |</li></ul><h2 id="二、stage-2-高质量微调-high-quality-fine-tuning" tabindex="-1">二、Stage 2：高质量微调（High-Quality Fine-tuning） <a class="header-anchor" href="#二、stage-2-高质量微调-high-quality-fine-tuning" aria-label="Permalink to &quot;二、Stage 2：高质量微调（High-Quality Fine-tuning）&quot;">​</a></h2><ul><li>目标：解冻扩散模型，让 connector 和 diffusion 模型一起进一步优化，以提升生成质量、对指令的响应度及鲁棒性。</li><li>数据来源：使用 BLIP3‑o 提供的 60,000 条高质量 instruction-image 样本，这些样本基于 GPT‑4o + DALL‑E3 / Midjourney 生成。</li><li>训练超参数： | 超参数 | 数值 | | ------------- | ------------------ | | Batch Size | 256 | | 优化器 | AdamW | | 学习率 | $1 \\times 10^{-5}$ | | 权重衰减 | 0.05 | | 梯度裁剪 | 1.0 | | Betas | (0.9, 0.95) | | 学习率调度 | Cosine decay | | Warm‑up Steps | 100 | | 总训练步数 | 10,000 steps |</li></ul><h1 id="crossflow" tabindex="-1">CrossFlow <a class="header-anchor" href="#crossflow" aria-label="Permalink to &quot;CrossFlow&quot;">​</a></h1><h2 id="_1-方法概览-training-approach" tabindex="-1">1. 方法概览（Training Approach） <a class="header-anchor" href="#_1-方法概览-training-approach" aria-label="Permalink to &quot;1. 方法概览（Training Approach）&quot;">​</a></h2><ul><li><p>核心创新 CrossFlow 完全打破传统扩散/流匹配模型必须从随机噪声开始的限制。它将源模态（如文本、低分辨率图像）分布直接映射到目标模态（如高分辨率图像、图像描述等），无需噪声输入或条件机制（如 cross-attention）。</p></li><li><p>关键技术突破</p><ul><li>Variational Encoder：用于将源模态编码成与目标模态相同维度与空间结构，解决模态间数据形状不一致的问题，同时引入正则化效果。</li><li>Classifer-Free Guidance（CFG）：通过在训练中引入二值指示变量，实现无需条件架构也能控制生成质量的引导机制。</li></ul></li><li><p>架构简洁高效 使用最普通的 Transformer（无 cross-attention）、统一处理输入与输出编码的 Token，展现跨模态生成的普适性。无需为特定任务额外设计结构。</p></li></ul><h2 id="_2-应用范例及任务覆盖-use-cases-performance" tabindex="-1">2. 应用范例及任务覆盖（Use Cases &amp; Performance） <a class="header-anchor" href="#_2-应用范例及任务覆盖-use-cases-performance" aria-label="Permalink to &quot;2. 应用范例及任务覆盖（Use Cases &amp; Performance）&quot;">​</a></h2><p>CrossFlow 在多个任务上与主流方法表现旗鼓相当或更优：</p><ul><li><p>主推任务：文本转图像生成（Text-to-Image）。</p><ul><li>CrossFlow 在该任务中，即使不采用条件机制，也略微优于常规 flow matching 模型，同时在大规模训练与模型扩展下更具优势。</li></ul></li><li><p>扩展任务：</p><ul><li>图像描述（Image captioning）</li><li>单目深度估计（Monocular depth estimation）</li><li>图像超分辨率（Super-resolution） 在这些任务中，CrossFlow 均与或优于现有专用架构方法，体现其通用架构的潜力。</li></ul></li><li><p>Latent Arithmetic 得益于 Variational Encoder 编码出的源分布具有语义结构，可在潜空间中进行有意思的编辑运算，实现对输出模态的语义控制。</p></li></ul><h2 id="_3-精炼训练策略-training-scheme" tabindex="-1">3. 精炼训练策略（Training Scheme） <a class="header-anchor" href="#_3-精炼训练策略-training-scheme" aria-label="Permalink to &quot;3. 精炼训练策略（Training Scheme）&quot;">​</a></h2><h2 id="_4-训练参数" tabindex="-1">4. 训练参数 <a class="header-anchor" href="#_4-训练参数" aria-label="Permalink to &quot;4. 训练参数&quot;">​</a></h2><table tabindex="0"><thead><tr><th>任务类别</th><th>模型规模</th><th>Epoch / Steps</th><th>Batch Size</th><th>Learning Rate</th><th>Warm-up</th><th>备注</th></tr></thead><tbody><tr><td>图像描述</td><td>351M</td><td>100 epochs</td><td>256</td><td>2 × 10⁻⁴</td><td>5 epochs</td><td>–</td></tr><tr><td>深度估计</td><td>527M</td><td>50 epochs</td><td>64</td><td>1 × 10⁻⁴ → 1 × 10⁻⁸</td><td>cosine annealing</td><td>–</td></tr><tr><td>图像超分辨率</td><td>505M</td><td>1,000,000 steps</td><td>512</td><td>1 × 10⁻⁴</td><td>5,000 steps</td><td>–</td></tr><tr><td>文本→图像（T2I）</td><td>~950M</td><td>~300K steps</td><td>未指出</td><td>未指出</td><td>未指出</td><td>同步 baseline，性能略优</td></tr></tbody></table><ul><li>Image Captioning</li></ul><p>“We train a 351M model for 100 epochs with a batch size of 256 and a learning rate of 2e-4 with 5 warm-up epochs.”【CVPR2025 补充材料】</p><ul><li>Depth Estimation</li></ul><p>“We train a 527M model for 50 epochs with a batch size of 64 and a learning rate decayed from 1e-4 to 1e-8 with cosine annealing.”</p><ul><li>Super-Resolution</li></ul><p>“We train a 505M model for 1M steps with a batch size of 512, a learning rate of 1e-4, and 5k warm-up steps.”</p><ul><li>Text-to-Image</li></ul><p>“We train a ~0.95B model for 300K steps with the same training budget as baseline (FID 10.13 vs 10.79).”</p><h1 id="图像编辑-图生图-数据训练表" tabindex="-1">图像编辑/图生图 - 数据训练表 <a class="header-anchor" href="#图像编辑-图生图-数据训练表" aria-label="Permalink to &quot;图像编辑/图生图 - 数据训练表&quot;">​</a></h1><table tabindex="0"><thead><tr><th>用途</th><th>数据/项目</th><th style="text-align:right;">开源</th><th>规模/形式</th><th>许可</th><th>获取/备注</th></tr></thead><tbody><tr><td>指令编辑 (Instruction-based)</td><td><strong>UltraEdit</strong></td><td style="text-align:right;">✅</td><td>≈ <strong>4M</strong> 编辑样本（含自由编辑与区域编辑）</td><td><strong>CC-BY-4.0</strong></td><td>代码、模型与<strong>数据集</strong>均提供，HF 数据集条目：<em>BleachNick/UltraEdit</em>。(<a href="https://github.com/HaozheZhao/UltraEdit" title="GitHub - HaozheZhao/UltraEdit" target="_blank" rel="noreferrer">GitHub</a>, <a href="https://huggingface.co/datasets/BleachNick/UltraEdit" title="BleachNick/UltraEdit · Datasets at Hugging Face" target="_blank" rel="noreferrer">Hugging Face</a>)</td></tr><tr><td>指令编辑</td><td><strong>MagicBrush</strong></td><td style="text-align:right;">✅</td><td><strong>10K</strong> 三元组（源图、指令、目标图），含单/多轮、带/不带掩码</td><td><strong>CC-BY-4.0</strong></td><td>GitHub 与 HF 提供<strong>训练/验证集下载</strong>；测试集需单独压缩包下载。(<a href="https://github.com/OSU-NLP-Group/MagicBrush" title="GitHub - OSU-NLP-Group/MagicBrush: [NeurIPS&#39;23] &quot;MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing&quot;." target="_blank" rel="noreferrer">GitHub</a>, <a href="https://huggingface.co/datasets/osunlp/MagicBrush" title="osunlp/MagicBrush · Datasets at Hugging Face" target="_blank" rel="noreferrer">Hugging Face</a>)</td></tr><tr><td>指令编辑</td><td><strong>HQ-Edit</strong></td><td style="text-align:right;">✅</td><td><strong>197,350</strong> 次编辑，高分辨率，含（正向/反向）编辑文本</td><td><strong>CC-BY-NC-4.0</strong></td><td>HF 数据集条目与项目页均公开（<strong>非商用</strong>）。(<a href="https://huggingface.co/datasets/UCSC-VLAA/HQ-Edit" title="UCSC-VLAA/HQ-Edit · Datasets at Hugging Face" target="_blank" rel="noreferrer">Hugging Face</a>)</td></tr><tr><td>指令编辑（合成）</td><td><strong>InstructPix2Pix 生成数据</strong></td><td style="text-align:right;">✅</td><td><strong>451,990</strong>（随机采样）/ <strong>313,010</strong>（CLIP 过滤）对，附下载脚本</td><td><em>未明示</em>（随项目/源数据）</td><td>官方仓库提供两版数据规模与下载方法（已做 NSFW 过滤）。(<a href="https://github.com/timothybrooks/instruct-pix2pix" title="GitHub - timothybrooks/instruct-pix2pix" target="_blank" rel="noreferrer">GitHub</a>)</td></tr><tr><td>聚合编辑集</td><td><strong>GPT-Image-Edit-1.5M</strong>（编辑基准 V2）</td><td style="text-align:right;">✅</td><td><strong>1.5M</strong>，聚合自 UltraEdit、HQ-Edit、OmniEdit、Complex-Edit 等</td><td>依<strong>来源</strong>而异</td><td>提供统一格式与子集说明；使用时需遵守各源集许可。(<a href="https://huggingface.co/datasets/UCSC-VLAA/GPT-Image-Edit-1.5M?utm_source=chatgpt.com" title="UCSC-VLAA/GPT-Image-Edit-1.5M · Datasets at ..." target="_blank" rel="noreferrer">Hugging Face</a>)</td></tr><tr><td>结构/控制条件</td><td><strong>ControlNet（示例集 fill50k）</strong></td><td style="text-align:right;">✅</td><td><strong>fill50k</strong> 示范数据包（教学/验证）</td><td><em>未明示</em></td><td>官方未发布完整训练语料；推荐<strong>用检测器在公有图集上自建条件</strong>（姿态、边缘、深度等）。(<a href="https://github.com/lllyasviel/ControlNet?utm_source=chatgpt.com" title="lllyasviel/ControlNet: Let us control diffusion models!" target="_blank" rel="noreferrer">GitHub</a>, <a href="https://huggingface.co/blog/train-your-controlnet?utm_source=chatgpt.com" title="Train your ControlNet with diffusers" target="_blank" rel="noreferrer">Hugging Face</a>, <a href="https://mmagic.readthedocs.io/en/latest/autoapi/mmagic/datasets/controlnet_dataset/index.html?utm_source=chatgpt.com" title="mmagic.datasets.controlnet_dataset" target="_blank" rel="noreferrer">MMagic</a>)</td></tr><tr><td>结构/控制条件</td><td><strong>T2I-Adapter</strong></td><td style="text-align:right;">✅（代码/权重）</td><td><em>无官方固定数据集</em></td><td>—</td><td>官方建议<strong>自备数据</strong>（如 COCO/LAION 派生并生成控制信号）进行训练。(<a href="https://github.com/TencentARC/T2I-Adapter?utm_source=chatgpt.com" title="TencentARC/T2I-Adapter" target="_blank" rel="noreferrer">GitHub</a>, <a href="https://arxiv.org/abs/2302.08453?utm_source=chatgpt.com" title="T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models" target="_blank" rel="noreferrer">arXiv</a>)</td></tr><tr><td>图像提示/风格参照</td><td><strong>IP-Adapter</strong></td><td style="text-align:right;">✅（代码/权重）</td><td><em>无官方固定数据集</em></td><td><strong>Apache-2.0</strong>（仓库）</td><td>提供<strong>训练脚本</strong>与数据 JSON 规范，需自建（参考图像 ↔ 文本/目标）样本。(<a href="https://github.com/tencent-ailab/IP-Adapter" title="GitHub - tencent-ailab/IP-Adapter: The image prompt adapter is designed to enable a pretrained text-to-image diffusion model to generate images with image prompt." target="_blank" rel="noreferrer">GitHub</a>)</td></tr><tr><td>示例参照编辑</td><td><strong>Paint-by-Example</strong></td><td style="text-align:right;">✅（代码/权重）</td><td><em>方法型</em>（基于自监督裁剪/遮挡构造对）</td><td>—</td><td>论文/代码公开，训练对通常从通用图集<strong>自生成</strong>（非独立发布数据）。(<a href="https://github.com/Fantasy-Studio/Paint-by-Example?utm_source=chatgpt.com" title="Paint by Example: Exemplar-based Image Editing with ..." target="_blank" rel="noreferrer">GitHub</a>, <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Paint_by_Example_Exemplar-Based_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.pdf?utm_source=chatgpt.com" title="Exemplar-Based Image Editing With Diffusion Models" target="_blank" rel="noreferrer">CVF开放获取</a>)</td></tr><tr><td>通用 I2I 任务（复原/修补等）</td><td><strong>Palette</strong></td><td style="text-align:right;">论文/项目页公开</td><td><em>任务可复现</em>（在 ImageNet/COCO 等上<strong>合成退化/掩码</strong>）</td><td>—</td><td>无独立数据发布；按论文流程<strong>自建合成任务</strong>（上色、补全、去 JPEG 等）。(<a href="https://arxiv.org/abs/2111.05826?utm_source=chatgpt.com" title="Palette: Image-to-Image Diffusion Models" target="_blank" rel="noreferrer">arXiv</a>, <a href="https://iterative-refinement.github.io/palette/" title="Palette: Image-to-Image Diffusion Models" target="_blank" rel="noreferrer">SR3</a>)</td></tr></tbody></table>',54)])])}const p=e(n,[["render",o]]);export{c as __pageData,p as default};
