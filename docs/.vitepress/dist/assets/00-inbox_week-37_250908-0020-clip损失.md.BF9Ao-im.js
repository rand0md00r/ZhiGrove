import{_ as l,c as n,o as e,ag as t,j as s,a as i}from"./chunks/framework.OaOo95RB.js";const E=JSON.parse('{"title":"是什么（核心思想）","description":"","frontmatter":{},"headers":[],"relativePath":"00-inbox/week-37/250908-0020-clip损失.md","filePath":"00-inbox/week-37/250908-0020-clip损失.md"}'),h={name:"00-inbox/week-37/250908-0020-clip损失.md"};function p(k,a,r,o,g,d){return e(),n("div",null,[...a[0]||(a[0]=[t('<p>下面用和刚才<strong>SigLIP</strong>相同的结构，简要介绍 <strong>CLIP 损失（InfoNCE / 对比学习交叉熵）</strong>：</p><h1 id="是什么-核心思想" tabindex="-1">是什么（核心思想） <a class="header-anchor" href="#是什么-核心思想" aria-label="Permalink to &quot;是什么（核心思想）&quot;">​</a></h1><p>把一个 batch 的图文配对看作“<strong>在这一行/这一列里选出正确对象</strong>”的多类分类问题。 对每张图，以所有文本为候选做 softmax；对每段文本，以所有图片为候选做 softmax；两者的交叉熵取平均。</p><h1 id="数学定义-最小公式" tabindex="-1">数学定义（最小公式） <a class="header-anchor" href="#数学定义-最小公式" aria-label="Permalink to &quot;数学定义（最小公式）&quot;">​</a></h1><p>设已归一化的图像/文本向量为 $v_i, t_j$，相似度</p><p>$$ s_{ij}=\\tau, v_i^\\top t_j,\\quad \\tau=\\exp(\\text{logit_scale}) $$</p><p>行方向（图→文）损失：</p>',7),s("p",null,[i("$$ \\mathcal{L}"),s("em",{i:"1"},"{\\text{i2t}} =\\frac{1}{B}\\sum"),i("^{B} \\Big[-\\log \\frac{e^{s_{ii}}}{\\sum_{j=1}^{B} e^{s_{ij}}}\\Big] $$")],-1),s("p",null,"列方向（文→图）损失：",-1),s("p",null,[i("$$ \\mathcal{L}"),s("em",{j:"1"},"{\\text{t2i}} =\\frac{1}{B}\\sum"),i("^{B} \\Big[-\\log \\frac{e^{s_{jj}}}{\\sum_{i=1}^{B} e^{s_{ij}}}\\Big] $$")],-1),t(`<p>总损失：$\\mathcal{L}=\\tfrac{1}{2}(\\mathcal{L}<em>{\\text{i2t}}+\\mathcal{L}</em>{\\text{t2i}})$。</p><h1 id="和-siglip-的关键差异" tabindex="-1">和 SigLIP 的关键差异 <a class="header-anchor" href="#和-siglip-的关键差异" aria-label="Permalink to &quot;和 SigLIP 的关键差异&quot;">​</a></h1><ul><li><strong>归一化 Softmax vs. 独立二分类</strong>：CLIP在行/列上做 softmax，只关心“对角比分母里所有候选更大”；SigLIP对所有 $(i,j)$ 做独立 BCE。</li><li><strong>负样本聚焦</strong>：CLIP的梯度更聚焦于<strong>难负样本</strong>（分母里最大的几项），收敛快但也更“尖锐”；SigLIP更平滑、对弱对齐/多正样本更友好。</li><li><strong>多正样本处理</strong>：CLIP天然是“每行/列一个正例”；多正样本需改造（如对同类做聚合/soft targets），而 SigLIP 直接多标签更自然。</li></ul><h1 id="怎么用-落地步骤" tabindex="-1">怎么用（落地步骤） <a class="header-anchor" href="#怎么用-落地步骤" aria-label="Permalink to &quot;怎么用（落地步骤）&quot;">​</a></h1><ol><li><strong>特征提取并 L2 归一化</strong>：<code>v = img_enc(x)</code>、<code>t = txt_enc(y)</code> 后 <code>F.normalize</code>。</li><li><strong>相似度矩阵</strong>：<code>logits = exp(logit_scale) * (v @ t.T)</code>。</li><li><strong>行/列交叉熵</strong>：目标标签是 <code>0..B-1</code> 的对角索引。计算 <code>CE(logits, target)</code> 与 <code>CE(logits.T, target)</code> 并平均。</li><li><strong>分布式放大负样本</strong>：用 all-gather 在多卡间拼 batch 再算 logits（注意 stop-grad 与梯度归属）。</li><li><strong>温度与稳定性</strong>：<code>logit_scale</code> 初始化为 <code>log(1/0.07)</code>，训练中<strong>裁剪到 <code>[0, ln 100]</code></strong> 以防爆梯/塌陷。</li></ol><h1 id="最小-pytorch-示例" tabindex="-1">最小 PyTorch 示例 <a class="header-anchor" href="#最小-pytorch-示例" aria-label="Permalink to &quot;最小 PyTorch 示例&quot;">​</a></h1><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.nn.functional </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> F</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> clip_loss</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(img_emb, txt_emb, logit_scale</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    img_emb, txt_emb: [B, D]，需已 L2 归一化</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    logit_scale: 可学习标量（log 温度）；若 None 用默认值</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    B </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> img_emb.size(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> logit_scale </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">is</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # log(1/0.07) ≈ 2.659。训练时可对该参数做 clamp_ 到 [0, ln(100)]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        logit_scale </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.tensor(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.07</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">img_emb.device).log()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    logits </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.exp(logit_scale) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> img_emb </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> txt_emb.t()  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># [B, B]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    targets </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.arange(B, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">device</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">img_emb.device)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    loss_i2t </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> F.cross_entropy(logits, targets)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    loss_t2i </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> F.cross_entropy(logits.t(), targets)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (loss_i2t </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> loss_t2i) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.5</span></span></code></pre></div><h1 id="使用建议-在你的项目里" tabindex="-1">使用建议（在你的项目里） <a class="header-anchor" href="#使用建议-在你的项目里" aria-label="Permalink to &quot;使用建议（在你的项目里）&quot;">​</a></h1><ul><li><p><strong>与 FM/KL 等多目标并行</strong>：CLIP 梯度更“尖”，容易主导更新；建议给 CLIP 一个<strong>中等权重</strong>，并监控各损失的梯度范数，必要时用自适应权重或温度上限抑制。</p></li><li><p><strong>多正样本场景</strong>（一图多文/一文多图）：</p><ul><li>方案A（最小改动）：在构建 batch 时只保留一个正样本（或做重复采样）以维持“一行一正”。</li><li>方案B（soft targets）：将同类文本/图的对角标签做<strong>分布化</strong>（把同类位置的目标概率均分），等价于 label smoothing 到同类集合。</li><li>方案C：切换到 SigLIP 的 BCE 框架来天然支持多标签。</li></ul></li><li><p><strong>指标与可视化</strong>：row/col 的 Recall@k、对角与最大非对角的 margin 分布、<code>logit_scale</code> 的轨迹；观察“难负样本”比值是否逐步下降。</p></li><li><p><strong>工程细节</strong>：</p><ul><li>跨卡 all-gather 后要避免重复反向（通常只对本地片段回传梯度）。</li><li>大 batch 与温度退火（或上限裁剪）能显著稳定训练。</li><li>若数据噪声大，可加轻度 label smoothing（如 0.05）或使用混合精度下的 <code>F.cross_entropy</code> + <code>torch.cuda.amp</code>.</li></ul></li></ul><p>需要的话，我可以把你现有的对齐模块做一份<strong>可切换 CLIP / SigLIP</strong> 的实现骨架（含多卡 all-gather、温度裁剪、指标统计）。</p>`,10)])])}const _=l(h,[["render",p]]);export{E as __pageData,_ as default};
