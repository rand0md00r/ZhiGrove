import{_ as e,c as n,o as i,ag as r}from"./chunks/framework.OaOo95RB.js";const b=JSON.parse('{"title":"论文记录模板（复制用）","description":"","frontmatter":{},"headers":[],"relativePath":"20-papers/vla/all_papers.md","filePath":"20-papers/vla/all_papers.md"}'),l={name:"20-papers/vla/all_papers.md"};function o(a,t,g,s,p,c){return i(),n("div",null,[...t[0]||(t[0]=[r('<h1 id="论文记录模板-复制用" tabindex="-1">论文记录模板（复制用） <a class="header-anchor" href="#论文记录模板-复制用" aria-label="Permalink to &quot;论文记录模板（复制用）&quot;">​</a></h1><blockquote><p>每篇论文按此模板填写，并放到合适分区（可在多个分区留“短条目”交叉引用）</p></blockquote><h2 id="标题-年份-会议-期刊" tabindex="-1">标题（年份，会议/期刊） <a class="header-anchor" href="#标题-年份-会议-期刊" aria-label="Permalink to &quot;标题（年份，会议/期刊）&quot;">​</a></h2><ul><li><strong>Citation</strong>：作者，题目，会议/期刊，年份</li><li><strong>链接</strong>：论文 | 代码 | 项目页（若有）</li><li><strong>任务</strong>：操作 / 导航 / 混合；场景（家庭/工业/车载…）</li><li><strong>架构</strong>：Planner-Executor / 端到端；是否工具调用；是否记忆模块</li><li><strong>动作范式</strong>：AR / 扩散 / Flow Matching / 混合；动作空间（离散/连续；关节/技能/程序）</li><li><strong>感知输入</strong>：RGB / 深度 / 语言 / 多视角 / 视频长度</li><li><strong>训练</strong>：预训练数据（规模/来源）、SFT、偏好/强化（DPO/RLAIF/RLHF）、奖励设计</li><li><strong>数据</strong>：自建 / 公共；真机/仿真；合成/蒸馏</li><li><strong>评测</strong>：基准与指标；关键结果（可列 1–3 个数字）</li><li><strong>开源程度</strong>：权重 / 训练代码 / 推理代码 / 数据（许可证）</li><li><strong>部署</strong>：推理硬件、时延、吞吐、边缘可行性</li><li><strong>亮点</strong>：3–5 条要点</li><li><strong>局限</strong>：2–3 条要点</li><li><strong>个人笔记</strong>：你的理解、与现有系统的可复用性、TODO</li></ul><hr><h2 id="π0-5-a-vision-language-action-model-with-open-world-generalization-2025-corl" tabindex="-1">π₀․₅: a Vision-Language-Action Model with Open-World Generalization（2025，CoRL） <a class="header-anchor" href="#π0-5-a-vision-language-action-model-with-open-world-generalization-2025-corl" aria-label="Permalink to &quot;π₀․₅: a Vision-Language-Action Model with Open-World Generalization（2025，CoRL）&quot;">​</a></h2><ul><li><p><strong>Citation</strong>：Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, <em>et al.</em> “π₀․₅: a Vision-Language-Action Model with Open-World Generalization.” <em>Proceedings of the 9th Conference on Robot Learning (CoRL)</em>, PMLR 305:17–40, 2025. (<a href="https://proceedings.mlr.press/v305/black25a.html" title="$\\pi_0.5$: a Vision-Language-Action Model with Open-World Generalization" target="_blank" rel="noreferrer">Proceedings of Machine Learning Research</a>)</p></li><li><p><strong>链接</strong>：论文（arXiv | PMLR）| 代码（openpi）| 项目页/博文</p><ul><li>arXiv：(<a href="https://arxiv.org/abs/2504.16054?utm_source=chatgpt.com" title="[2504.16054] $π_{0.5}$: a Vision-Language-Action Model ..." target="_blank" rel="noreferrer">arXiv</a>)</li><li>PMLR页面与PDF：(<a href="https://proceedings.mlr.press/v305/black25a.html" title="$\\pi_0.5$: a Vision-Language-Action Model with Open-World Generalization" target="_blank" rel="noreferrer">Proceedings of Machine Learning Research</a>)</li><li>博文解读：(<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</li><li>开源仓库（Apache-2.0）：(<a href="https://github.com/Physical-Intelligence/openpi" title="GitHub - Physical-Intelligence/openpi" target="_blank" rel="noreferrer">GitHub</a>)</li><li>HF 权重（LeRobot 转换）：(<a href="https://huggingface.co/lerobot/pi05_base?utm_source=chatgpt.com" title="lerobot/pi05_base" target="_blank" rel="noreferrer">Hugging Face</a>)</li></ul></li><li><p><strong>任务</strong>：操作（家居场景的长任务，如收拾餐具、擦除污渍、整理卧室）；强调“陌生家庭”零样本泛化。 (<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</p></li><li><p><strong>架构</strong>：单一 VLA 模型同时做高层语义决策与低层连续控制；推理时先自述式高层子任务（AR 文本），再生成低层“动作块”（Flow Matching 连续控制）。非工具调用范式；无专门记忆模块描述。 (<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</p></li><li><p><strong>动作范式</strong>：<strong>混合</strong>＝高层<strong>自回归</strong>（文本子任务）+ 低层<strong>Flow Matching</strong>（连续关节控制）；低层以约 1s/50 步的“action chunk”输出连续关节指令；动作空间为<strong>连续</strong>（关节级）。另含约 3e8 参数的“action expert”。 (<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</p></li><li><p><strong>感知输入</strong>：RGB 视觉 + 语言指令；训练期采用混合多模态信号（检测框、子任务标签、网页多模态等）。(<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</p></li><li><p><strong>训练</strong>：核心是**异构协同训练（co-training）**与混合监督：</p><ul><li>来源：移动操作真机数据、跨环境静态/移动机器人数据、跨载体（cross-embodiment）数据、网页多模态任务（VQA/Caption/Detection）、“口头教练”逐步指令等；文中对<strong>取消某一来源</strong>的消融。(<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</li><li>规模：开源仓库给出 openpi 基座模型“<strong>10k+ 小时机器人数据</strong>”预训练（π₀/π₀-FAST/π₀․₅均提供 base ckpt）；文中移动操作自采数据在某些消融中约 <strong>400 小时</strong>。(<a href="https://github.com/Physical-Intelligence/openpi" title="GitHub - Physical-Intelligence/openpi" target="_blank" rel="noreferrer">GitHub</a>)</li><li>相关技术：知识绝缘（Knowledge Insulation）作为 π₀․₅ 训练配方升级的一部分在 repo/白皮书中提及。(<a href="https://github.com/Physical-Intelligence/openpi" title="GitHub - Physical-Intelligence/openpi" target="_blank" rel="noreferrer">GitHub</a>)</li></ul></li><li><p><strong>数据</strong>：<strong>真机</strong>为主，家庭/办公室等多环境；<strong>跨载体/跨环境</strong>整合 + <strong>网页多模态</strong>；并在开源模型中提供与 <strong>DROID/LIBERO</strong> 等<strong>公共数据</strong>相关的变体与微调示例。(<a href="https://github.com/Physical-Intelligence/openpi" title="GitHub - Physical-Intelligence/openpi" target="_blank" rel="noreferrer">GitHub</a>)</p></li><li><p><strong>评测</strong>：两大设置——<strong>完整清洁任务</strong>与 <strong>OOD 指定物体入抽屉</strong>；指标为<strong>语言跟随率</strong>与<strong>成功率</strong>。关键数字（博文给出）：</p><ul><li><strong>IID 成功率</strong>：83%（π₀․₅） vs 57%（无多环境数据 ME）/67%（无跨载体 CE）。</li><li><strong>OOD 成功率</strong>：<strong>94%</strong>（π₀․₅） vs 31%（无 ME）/49%（无 CE）/74%（无网页数据 WD）。</li><li><strong>结论</strong>：WD 对 OOD 类别识别助益大；ME/CE 对整体泛化关键。(<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</li></ul></li><li><p><strong>开源程度</strong>：<strong>权重</strong>（π₀․₅ base 与若干任务专家）、<strong>训练与推理代码</strong>（现已含 PyTorch 训练流程）、<strong>数据接口/示例</strong>均开放（Apache-2.0）；HF 提供 PyTorch safetensors 转换。(<a href="https://github.com/Physical-Intelligence/openpi" title="GitHub - Physical-Intelligence/openpi" target="_blank" rel="noreferrer">GitHub</a>)</p></li><li><p><strong>部署</strong>：官方 openpi README 建议<strong>单卡 RTX 4090（&gt;8GB）可推理</strong>；LoRA 微调约 &gt;22.5GB；全参微调需 A100/H100 级显存。已给出 Docker/UV 环境与多 GPU FSDP 选项。(<a href="https://github.com/Physical-Intelligence/openpi" title="GitHub - Physical-Intelligence/openpi" target="_blank" rel="noreferrer">GitHub</a>)</p></li><li><p><strong>亮点</strong>：</p><ol><li><strong>高/低层一体化</strong>：同一模型先“想”（AR 文本）再“做”（Flow连续控制），贴近“内在推理→动作”的链式流程。(<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</li><li><strong>协同训练配方</strong>显著提升陌生环境泛化，<strong>WD/ME/CE</strong> 各司其职。(<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</li><li><strong>开放生态</strong>：代码、基座与专家权重、训练脚本、与 DROID/LIBERO 的适配齐备，易于再训练与复现。(<a href="https://github.com/Physical-Intelligence/openpi" title="GitHub - Physical-Intelligence/openpi" target="_blank" rel="noreferrer">GitHub</a>)</li><li><strong>连续动作的 Flow Matching</strong> + <strong>动作专家</strong>，在灵活性与稳定性间取得平衡。(<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</li><li><strong>可扩展数据配方</strong>与环境数缩放实验，显示“100+ 环境”后接近在测试环境训练的上限。(<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</li></ol></li><li><p><strong>局限</strong>：</p><ol><li>官方明确<strong>非追求极致灵巧度</strong>，在新家务场景仍有失败案例；成功并非稳定 100%。(<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</li><li>训练依赖<strong>大规模多源数据</strong>与较重 GPU 资源；低资源/新平台迁移仍需微调与工程适配。(<a href="https://github.com/Physical-Intelligence/openpi" title="GitHub - Physical-Intelligence/openpi" target="_blank" rel="noreferrer">GitHub</a>)</li><li>文中未强调外部工具/记忆模块，<strong>长时任务的显式记忆与可解释规划</strong>仍有提升空间（研究社区正在探索）。〔基于文献对比推断；论文未主打该点〕</li></ol></li><li><p><strong>个人笔记</strong>：</p><ul><li><p><strong>与现有系统复用性</strong>：对车内/家居“多阶段流程”的<strong>高层语言-子任务</strong>再到<strong>低层控制</strong>非常契合“Planner（文本）→ Executor（连续动作）”的工程拆分；你现有 VLA-Cabin/工业巡检可复用其<strong>协同训练配方</strong>与<strong>Flow 低层头</strong>，并将**WD（网页/合成）+ CE/ME（跨载体/跨环境）**纳入数据管线。</p></li><li><p><strong>落地建议/TODO</strong>：</p><ol><li>以 <strong>π₀․₅ base</strong> 为起点，在自家数据上做 <strong>KI（知识绝缘）配置 + LoRA</strong> 微调；优先接入 <strong>DROID/LIBERO</strong> 公共数据做对齐；桌面→移动基座分阶段蒸馏。(<a href="https://github.com/Physical-Intelligence/openpi" title="GitHub - Physical-Intelligence/openpi" target="_blank" rel="noreferrer">GitHub</a>)</li><li>保持<strong>高层 AR 子任务链</strong>可观测（日志化），同时评估低层 <strong>1s/50步动作块</strong>的时延与稳定性，必要时在仿真中做 action-chunk horizon 的灵敏度实验。(<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</li><li>建立<strong>WD/ME/CE</strong> 的<strong>可插拔数据开关</strong>，复现实验室里的 ablation 曲线，作为泛化“健康度”回归测试。(<a href="https://www.physicalintelligence.company/blog/pi05" title="A VLA with Open-World Generalization" target="_blank" rel="noreferrer">Physical Intelligence</a>)</li></ol></li></ul></li></ul><hr><h2 id="roboomni-proactive-robot-manipulation-in-omni-modal-context-2025-arxiv" tabindex="-1">RoboOmni: Proactive Robot Manipulation in Omni-modal Context（2025，arXiv） <a class="header-anchor" href="#roboomni-proactive-robot-manipulation-in-omni-modal-context-2025-arxiv" aria-label="Permalink to &quot;RoboOmni: Proactive Robot Manipulation in Omni-modal Context（2025，arXiv）&quot;">​</a></h2><ul><li><strong>Citation</strong>：Siyin Wang, Jinlan Fu, Feihong Liu, et al. RoboOmni: Proactive Robot Manipulation in Omni-modal Context. arXiv:2510.23763v3 [cs.RO], 2025</li><li><strong>链接</strong>：论文 <a href="https://arxiv.org/pdf/2510.23763" target="_blank" rel="noreferrer">https://arxiv.org/pdf/2510.23763</a> | 代码 <a href="https://github.com/OpenMOSS/RoboOmni" target="_blank" rel="noreferrer">https://github.com/OpenMOSS/RoboOmni</a> | 项目页 <a href="https://OpenMOSS.github.io/RoboOmni" target="_blank" rel="noreferrer">https://OpenMOSS.github.io/RoboOmni</a> | Hugging Face <a href="https://huggingface.co/collections/fnlp/roboomni" target="_blank" rel="noreferrer">https://huggingface.co/collections/fnlp/roboomni</a></li><li><strong>任务</strong>：操作；场景（家庭）</li><li><strong>架构</strong>：Perceiver-Thinker-Talker-Executor 端到端；无工具调用；无记忆模块</li><li><strong>动作范式</strong>：离散令牌（FAST+ tokenizer）；动作空间（离散2048个令牌，映射7-DoF连续关节级控制向量）</li><li><strong>感知输入</strong>：RGB、语言（文本）、音频（语音+环境声音）；多模态时序输入</li><li><strong>训练</strong>：预训练数据（OmniAction数据集141k episodes + Open-X Embodiment子集；规模140k+ episodes、来源合成构建）、SFT（下游任务微调）、无偏好/强化训练、奖励设计（自回归最大似然目标，融合对话与动作生成损失）</li><li><strong>数据</strong>：自建（OmniAction）+ 扩展公共（LIBERO）；真机+仿真；合成（文本脚本+听觉实现+验证）</li><li><strong>评测</strong>：基准（OmniAction-LIBERO-TTS、OmniAction-LIBERO-Real）；指标（成功率、意图识别准确率、推理时延）；关键结果（平均成功率85.6%、意图识别准确率88.9%、推理时延为ASR+OpenVLA的0.49倍）</li><li><strong>开源程度</strong>：权重（Hugging Face） / 训练代码（开源） / 推理代码（开源） / 数据（开源，许可证未知）</li><li><strong>部署</strong>：推理硬件（RTX 4090）、时延（0.49×相对基线）、吞吐（未明确）、边缘可行性（潜力较高，端到端效率优）</li><li><strong>亮点</strong>： <ol><li>提出跨模态上下文指令新范式，无需显式指令，从视、听、文本多模态推断用户意图</li><li>端到端框架统一意图识别、交互确认、动作执行，支持直接语音交互（无需ASR）</li><li>自建大规模OmniAction数据集，覆盖6类上下文指令、5k+说话人、多场景声学环境</li><li>推理速度远超基线，时延仅为传统ASR+VLA pipeline的一半</li><li>真机与仿真实验均验证优势，意图识别与主动协助能力突出</li></ol></li><li><strong>局限</strong>： <ol><li>非语言指令任务成功率相对较低（~82%），仍是核心挑战</li><li>预训练依赖大规模硬件资源（64 A100 GPUs训练10天），训练成本高</li><li>真机实验仅基于WidowX 250S机械臂，复杂场景与多机器人适配性待验证</li></ol></li><li><strong>个人笔记</strong>：RoboOmni的核心突破是打破了机器人对显式指令的依赖，多模态端到端设计既保留了细粒度语义信息，又提升了执行效率，OmniAction数据集的构建流程（文本脚本→听觉实现→验证）可复用在多模态机器人任务中。其主动交互逻辑（推断意图→确认→执行）贴近真实人机协作场景，可复用至家庭服务机器人系统。TODO：关注其在工业场景的适配方案，以及轻量化模型的部署可能性。</li></ul><hr>',11)])])}const u=e(l,[["render",o]]);export{b as __pageData,u as default};
