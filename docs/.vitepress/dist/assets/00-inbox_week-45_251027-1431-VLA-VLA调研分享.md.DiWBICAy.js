import{_ as t,c as n,o as r,ag as s}from"./chunks/framework.CQuhCYrb.js";const g="/assets/2025-10-27-03-25-37.BMJ-jziq.png",i="/assets/2025-10-27-10-42-20.CnAZhGCp.png",l="/assets/2025-10-27-10-58-27.YKKcdZtH.png",a="/assets/2025-10-27-02-42-54.DW_xoU6V.png",e="/assets/2025-10-27-02-47-15.BEN189B2.png",h="/assets/2025-10-27-02-53-25.xZS97iIv.png",u="/assets/2025-10-27-03-04-46.7FQiF-3b.png",c="/assets/2025-10-27-03-12-57.UTbgJxOj.png",f="/assets/2025-10-27-03-21-26.BTJ7uysH.png",D=JSON.parse('{"title":"VLA（Vision–Language–Action）核心技术与主要方案","description":"","frontmatter":{},"headers":[],"relativePath":"00-inbox/week-45/251027-1431-VLA-VLA调研分享.md","filePath":"00-inbox/week-45/251027-1431-VLA-VLA调研分享.md"}'),d={name:"00-inbox/week-45/251027-1431-VLA-VLA调研分享.md"};function p(b,o,m,_,L,A){return r(),n("div",null,[...o[0]||(o[0]=[s('<h1 id="vla-vision–language–action-核心技术与主要方案" tabindex="-1">VLA（Vision–Language–Action）核心技术与主要方案 <a class="header-anchor" href="#vla-vision–language–action-核心技术与主要方案" aria-label="Permalink to &quot;VLA（Vision–Language–Action）核心技术与主要方案&quot;">​</a></h1><h2 id="目录-table-of-contents" tabindex="-1">目录（Table of Contents） <a class="header-anchor" href="#目录-table-of-contents" aria-label="Permalink to &quot;目录（Table of Contents）&quot;">​</a></h2><ul><li><a href="#vla-visionlanguageaction-调研报告">VLA（Vision–Language–Action）调研报告</a><ul><li><a href="#1-背景引入">1. 背景引入</a><ul><li><a href="#11-概念与发展脉络">1.1 概念与发展脉络</a></li><li><a href="#12-产业落地需求">1.2 产业落地需求</a></li><li><a href="#13-技术演进趋势">1.3 技术演进趋势</a></li></ul></li><li><a href="#2-核心技术">2. 核心技术</a><ul><li><a href="#21-理解基座vl--action">2.1 理解基座（V+L → Action）</a></li><li><a href="#22-信息表征如何处理视觉文本动作">2.2 信息表征（如何处理视觉/文本/动作）</a></li><li><a href="#23-特征传递理解模型--生成控制模型-的桥接">2.3 特征传递（理解模型 → 生成/控制模型 的桥接）</a></li><li><a href="#24-动作生成-扩散--流匹配-的关键做法">2.4 动作生成（扩散 / 流匹配 的关键做法）</a></li><li><a href="#25-任务规划会想再去做并保证完备性">2.5 任务规划（会想再去做，并保证完备性）</a></li></ul></li><li><a href="#3-方案与架构">3. 方案与架构</a><ul><li><a href="#31-自回归架构">3.1 自回归架构</a><ul><li><a href="#311-openvla7b-开源-vla业界通用强基线--202409--github">3.1.1 OpenVLA（7B 开源 VLA，业界通用强基线）</a></li><li><a href="#312-π₀-fastopenpi--202501--github">3.1.2 π₀-FAST（openpi）</a></li></ul></li><li><a href="#32-扩散流匹配架构diffusion--flow-matching">3.2 扩散/流匹配架构（Diffusion / Flow Matching）</a><ul><li><a href="#321-π₀₅openpi流匹配动作头开放世界泛化--202504--github">3.2.1 π₀.₅（openpi，流匹配动作头，开放世界泛化）</a></li><li><a href="#322-rdt-1brobotics-diffusion-transformer--202505--github--iclr-2025">3.2.2 RDT-1B（Robotics Diffusion Transformer）</a></li></ul></li><li><a href="#33-商业闭源参考方案">3.3 商业闭源参考方案</a><ul><li><a href="#331-seed-gr-3bytedance-seed--202507">3.3.1 Seed GR-3（ByteDance Seed）</a></li><li><a href="#332-gemini-roboticsgoogle-deepmind--202509">3.3.2 Gemini Robotics（Google DeepMind）</a></li><li><a href="#333-isaac-gr00t-n1nvidia">3.3.3 Isaac GR00T N1（NVIDIA）</a></li></ul></li></ul></li><li><a href="#4-落地展望要点">4. 落地展望（要点）</a><ul><li><a href="#41-基于开源方案的快速落地技术路线两种备选">4.1 基于开源方案的快速落地技术路线（两种备选）</a></li><li><a href="#42-数据源与数据飞轮">4.2 数据源与数据飞轮</a></li><li><a href="#43-可支持的业务场景方向级">4.3 可支持的业务场景（方向级）</a></li></ul></li></ul></li></ul><hr><h2 id="_1-背景引入" tabindex="-1">1. 背景引入 <a class="header-anchor" href="#_1-背景引入" aria-label="Permalink to &quot;1. 背景引入&quot;">​</a></h2><h3 id="_1-1-概念与发展脉络" tabindex="-1">1.1 概念与发展脉络 <a class="header-anchor" href="#_1-1-概念与发展脉络" aria-label="Permalink to &quot;1.1 概念与发展脉络&quot;">​</a></h3><ul><li><strong>定义</strong>：<strong>VLA（Vision–Language–Action）</strong> 模型通过统一的多模态表示，将<strong>视觉感知</strong>、<strong>自然语言理解</strong>和<strong>动作规划</strong>耦合在一起，以支持机器人在开放世界执行复杂任务。</li><li><strong>发展脉络</strong>：早期机器人策略依赖<strong>硬编码</strong>或<strong>纯视觉/状态输入</strong>；2019–2021 年涌现的 <strong>Vision–Language（VL）</strong> 模型（如 <strong>CLIP、BLIP</strong>）在感知与语义理解上取得突破；<strong>SayCan、PaLM-SayCan</strong> 等把 <strong>LLM</strong> 引入机器人决策；2022 年谷歌推出 <strong>RT-1</strong>，首次以大规模真实机器人数据训练 <strong>Transformer</strong>；2023–2024 年出现 <strong>RT-2、PaLM-E、π 系列、Diffusion Policy</strong> 等面向通用机器人智能的 <strong>VLA</strong> 模型。</li><li><strong>核心目标</strong>：在单一模型中实现“<strong>看懂场景 — 理解任务 — 输出动作</strong>”，使机器人具备<strong>泛化能力</strong>和<strong>零样本/少样本适应</strong>能力。</li></ul><h3 id="_1-2-产业落地需求" tabindex="-1">1.2 产业落地需求 <a class="header-anchor" href="#_1-2-产业落地需求" aria-label="Permalink to &quot;1.2 产业落地需求&quot;">​</a></h3><ul><li><strong>长尾任务</strong>：家庭、仓储、制造等场景存在庞大且多变的任务组合，传统专用机器人难以覆盖。</li><li><strong>人机交互成本</strong>：通过<strong>自然语言</strong>描述任务，减少对示教器或编程的依赖，提高部署效率。</li><li><strong>全链条自动化</strong>：机器人从<strong>感知到执行</strong>的闭环，减少人工监控和中间件依赖，推动<strong>柔性自动化</strong>。</li></ul><h3 id="_1-3-技术演进趋势" tabindex="-1">1.3 技术演进趋势 <a class="header-anchor" href="#_1-3-技术演进趋势" aria-label="Permalink to &quot;1.3 技术演进趋势&quot;">​</a></h3><ul><li><strong>数据驱动</strong>：从<strong>手工策略</strong>转向“<strong>数据 + 大模型</strong>”范式，强调<strong>多源、多模态、跨域</strong>数据的利用。</li><li><strong>模型统一</strong>：使用<strong>单一 Transformer / 扩散器</strong>处理视觉、语言、动作序列，减少模块化设计的<strong>误差累积</strong>。</li><li><strong>开放世界泛化</strong>：从<strong>封闭集任务</strong>走向<strong>开放环境</strong>，对 <strong>zero-shot / few-shot</strong> 能力提出要求。</li><li><strong>实时与安全</strong>：在保证泛化的同时，关注<strong>延迟、可靠性、安全性、可解释性</strong>。</li></ul><hr><h2 id="_2-核心技术" tabindex="-1">2. 核心技术 <a class="header-anchor" href="#_2-核心技术" aria-label="Permalink to &quot;2. 核心技术&quot;">​</a></h2><h3 id="_2-1-理解基座-v-l-→-action" tabindex="-1">2.1 理解基座（<strong>V+L → Action</strong>） <a class="header-anchor" href="#_2-1-理解基座-v-l-→-action" aria-label="Permalink to &quot;2.1 理解基座（**V+L → Action**）&quot;">​</a></h3><p><img src="'+g+'" alt=""></p><ol><li>用强视觉主干（<strong>SigLIP / DINOv2</strong> 等）+ 语言主干（<strong>Llama / Qwen</strong> 等）做<strong>多模态对齐</strong>，再把视觉 <strong>token</strong> 通过<strong>投影头</strong>送入 <strong>LLM</strong>，形成<strong>可指令化</strong>的通用理解基座。</li><li>随后在 <strong>VLA</strong> 场景上再训练<strong>动作头</strong>（<strong>自回归</strong>或<strong>扩散/流匹配</strong>）。以 <strong>OpenVLA</strong> 为代表的开源基座，已给出<strong>端到端链路</strong>与<strong>复现实践</strong>。</li></ol><h3 id="_2-2-信息表征-如何处理视觉-文本-动作" tabindex="-1">2.2 信息表征（<strong>如何处理视觉/文本/动作</strong>） <a class="header-anchor" href="#_2-2-信息表征-如何处理视觉-文本-动作" aria-label="Permalink to &quot;2.2 信息表征（**如何处理视觉/文本/动作**）&quot;">​</a></h3><p><img src="'+i+'" alt=""></p><ul><li><strong>问题</strong>：传统“<strong>像素 → VAE 潜空间 → 生成/控制</strong>”链条，潜空间<strong>压缩过度、语义稀薄</strong>，不利于下游（包括<strong>动作扩散头</strong>）稳定学习。 <ul><li><strong>架构老旧</strong>：它基于卷积网络，与现代Transformer驱动的DiT架构显得格格不入</li><li><strong>信息瓶颈</strong>：它将图像极度压缩到一个低维度的潜在空间（例如，将256x256的图像压缩到32x32x4），这不可避免地会丢失大量细节</li><li><strong>语义贫乏</strong>：VAE的训练目标仅仅是“重建”目标模态，导致其潜在空间虽然能保留模态的局部外观，但缺乏对内容的高层语义理解</li></ul></li><li><strong>近期方向</strong>： <ul><li><strong>RAE（Representation Autoencoder）</strong><ul><li>用<strong>冻结的表征编码器</strong>（如 <strong>DINO、SigLIP、MAE</strong>）替代 <strong>VAE</strong> 做“<strong>潜空间编码</strong>”，再配<strong>轻量 ViT 解码器</strong>；</li><li>在<strong>高维、语义更强</strong>的潜空间上训练 <strong>DiT/扩散</strong>，<strong>收敛更快、质量更高</strong>；</li><li>对 <strong>VLA</strong>：把更<strong>语义化</strong>的视觉潜表示<strong>直接喂给动作头</strong>（<strong>扩散/流匹配/自回归</strong>），减少“<strong>像素 ↔ 动作</strong>”错配。</li></ul></li><li><strong>SVG（Self-supervised representation for Visual Generation）</strong><ul><li><strong>移除 VAE</strong>，直接在<strong>自监督表征潜空间</strong>做扩散；</li><li>以<strong>残差分支</strong>补细节、<strong>卷积解码器</strong>回像素；</li><li>对 <strong>VLA</strong>：在<strong>统一自监督潜表示</strong>中承载视觉语义与运动条件，更稳。</li></ul></li></ul></li></ul><h3 id="_2-3-特征传递-理解模型-→-生成-控制模型-的桥接" tabindex="-1">2.3 特征传递（<strong>理解模型 → 生成/控制模型 的桥接</strong>） <a class="header-anchor" href="#_2-3-特征传递-理解模型-→-生成-控制模型-的桥接" aria-label="Permalink to &quot;2.3 特征传递（**理解模型 → 生成/控制模型 的桥接**）&quot;">​</a></h3><p><img src="'+l+'" alt=""></p><ul><li><strong>问题</strong>：理解侧（<strong>VLM/LLM</strong>）输出<strong>离散语义/推理特征</strong>；生成/控制侧（<strong>动作扩散/流匹配/离散解码</strong>）需要<strong>可条件化、连续且时序稳定</strong>的表征。</li><li><strong>可行范式</strong>： <ul><li><strong>MetaQuery</strong>：通过一组<strong>可学习查询向量</strong>在<strong>冻结多模态 LLM</strong>与<strong>扩散生成器</strong>之间<strong>传递/对齐语义</strong>，避免端到端重训；同理可用于“<strong>VLM → 动作扩散头</strong>”的桥接（将<strong>推理语义蒸馏</strong>到控制条件）。</li><li><strong>Connector/Adapter</strong>：以<strong>轻量连接器</strong>把理解端<strong>高层语义</strong>投影到生成端<strong>条件空间</strong>；已有视觉生成工作对 <strong>MetaQuery/Connector</strong> 给出系统评测，<strong>工程成熟</strong>。</li></ul></li><li><strong>要点</strong>： <ul><li>保持<strong>时序条件</strong>（历史观测/动作）与<strong>任务语义</strong>（文本、子目标）<strong>双通道</strong>输入；</li><li>面向<strong>长程任务</strong>，建议在桥接层显式携带<strong>规划/子目标 latent</strong>。</li></ul></li></ul><h3 id="_2-4-动作生成-扩散-流匹配-的关键做法" tabindex="-1">2.4 动作生成（<strong>扩散 / 流匹配</strong> 的关键做法） <a class="header-anchor" href="#_2-4-动作生成-扩散-流匹配-的关键做法" aria-label="Permalink to &quot;2.4 动作生成（**扩散 / 流匹配** 的关键做法）&quot;">​</a></h3><ul><li><p><strong>连续扩散 / 流匹配</strong></p><ul><li>在<strong>连续动作或短轨迹</strong>上做<strong>条件去噪/流匹配</strong>；</li><li>代表： <ul><li><strong>Diffusion Policy</strong>： <ul><li>视觉/状态编码器：将多视角 RGB（可含本体状态、历史动作）编码成时序特征；</li><li>时间序列去噪器：时域 U-Net/Transformer 对“被加噪的动作序列”做条件去噪（条件=视觉/状态/指令 + 噪声时间步）；</li><li>执行：生成长度 𝐻 的动作轨迹，用 RHC 每次执行前 𝑘 步并滚动更新。</li></ul></li><li><strong>RDT-1B</strong>： <ul><li>感知/语言条件：多视角 RGB → 视觉编码；文本指令 → 文本编码；拼接成条件序列；</li><li>去噪主干：Diffusion Transformer（DiT） 直接对连续动作序列做去噪，一次并行预测 64 步动作（统一兼容单/双臂、关节/末端等接口）；</li><li>训练/数据：在46 个数据集、100 万+ 回合上做模仿学习式去噪训练，另有 ALOHA 双臂精调；</li><li>执行：可直接下发整段，或配 RHC。</li></ul></li><li><strong>Dita</strong>： <ul><li>原位条件化（in-context conditioning）：不再把视觉特征“融合成单向量”，而是把原始视觉 token 序列与被加噪的动作 token一起送入大号 Transformer 去噪器，细粒度对齐“历史观测 ↔ 动作增量”；</li><li>建模内容：显式建模动作增量与环境细节，跨机体/多视角/多动作空间更鲁棒；</li><li>训练/执行：标准扩散/少步采样套路，易与多源数据横向扩展。</li></ul></li><li><strong>DiT-Block Policy</strong>：面向扩散策略设计的<strong>高性能条件噪声网络（Transformer 模块化块）</strong>，以目标文本/本体状态/时间步嵌入为条件，提供可扩展的Goal-conditioned策略学习骨架。</li></ul></li><li>训练：<strong>L2 / score / flow</strong> 目标；推理：<strong>并行采样/缓存/少步采样</strong>。</li></ul></li><li><p><strong>离散扩散</strong></p><ul><li>先把动作<strong>离散化</strong>为 <strong>token/动作块</strong>，在<strong>单体 Transformer</strong> 内做<strong>离散扩散</strong>并用<strong>交叉熵</strong>训练（与 <strong>VLM</strong> 一致），支持<strong>并行解码、二次掩码纠错</strong>；</li><li>代表： <ul><li><strong>Discrete Diffusion VLA</strong>（与离散语言接口原生对齐） <ul><li>单体骨干：把原本自回归动作头改为双向注意力，在同一 Transformer里对离散化动作块执行离散扩散（不另挂外部连续扩散头）；</li><li>训练目标：与 VLM 同一套交叉熵监督（离散复原），因此预训练先验保留好；</li><li>推理技巧：自适应解码顺序（先解“容易的”维度/通道）、二次掩码重整（对高不确定 token 反复复原），并支持并行生成；</li><li>执行：解码完成后去 token 化还原到连续关节/末端控制。</li></ul></li><li><strong>DIVA（Discrete diffusion Vision-Language-Action）</strong><ul><li>动作表示：将动作/子动作编码为离散潜表示，在迭代去噪中逐步复原；</li><li>条件化：视觉与语言通过投影接入，与“被破坏的动作 token”联合输入扩散网络；</li><li>训练/推理：离散扩散范式，强调减小自回归误差累积与时间刚性；</li><li>接口：与 VLM 的token 接口一致，易做统一大模型。</li></ul></li><li><strong>DFMP（Discrete Flow Matching Policy，离散流匹配）</strong><ul><li>动作空间：将连续动作映射到离散空间；</li><li>生成头：使用离散的 flow/score-based生成过程（与离散扩散同宗，目标为匹配离散分布的速度场/得分）；</li><li>训练/推理：迭代复原离散动作，再反量化为连续控制；</li><li>定位：当作“离散扩散的流匹配变体”，保留离散接口优势，同时享受 flow 的收敛/少步潜力。</li></ul></li></ul></li></ul></li><li><p><strong>工程关注点</strong>：<strong>并行/缓存加速、多视角条件、SE(3) 约束、少步采样与蒸馏</strong>；</p></li></ul><h3 id="_2-5-任务规划-会想再去做-并保证完备性" tabindex="-1">2.5 任务规划（<strong>会想再去做，并保证完备性</strong>） <a class="header-anchor" href="#_2-5-任务规划-会想再去做-并保证完备性" aria-label="Permalink to &quot;2.5 任务规划（**会想再去做，并保证完备性**）&quot;">​</a></h3><ul><li><strong>主流范式</strong>： <ol><li><strong>LLM + 可供性/价值约束（SayCan 系）</strong>：LLM 产出候选动作序列，用<strong>可供性/价值</strong>函数打分执行，<strong>安全可控</strong>，适合<strong>长程任务</strong>；</li><li><strong>内在独白/交互式反思（Inner Monologue）</strong>：利用环境反馈形成“<strong>思考 — 行动 — 再思考</strong>”循环，提高<strong>鲁棒性</strong>与<strong>异常处理</strong>；</li><li><strong>显式视觉推理（CoT-VLA / ThinkAct）</strong>：先预测<strong>未来视觉目标帧 / 视觉计划 latent</strong>，再生成<strong>短动作序列</strong>，兼顾<strong>可解释</strong>与<strong>执行稳定</strong>。</li></ol></li><li><strong>完备规划栈要素</strong>： <ul><li><strong>技能库/原语</strong>（pick/place/开关/导航…）与<strong>状态机/行为树接口</strong>；</li><li><strong>可供性/可达性评估</strong>，<strong>失败检测与异常分支</strong>（重试/替代技能）；</li><li><strong>记忆与检索</strong>（任务历史、子目标缓存）、<strong>再规划触发</strong>（阈值/失败/观测突变）；</li><li><strong>安全守护</strong>（动作幅度/碰撞/地域限制）。</li></ul></li></ul><hr><h2 id="_3-方案与架构" tabindex="-1">3. 方案与架构 <a class="header-anchor" href="#_3-方案与架构" aria-label="Permalink to &quot;3. 方案与架构&quot;">​</a></h2><h3 id="_3-1-自回归架构" tabindex="-1">3.1 自回归架构 <a class="header-anchor" href="#_3-1-自回归架构" aria-label="Permalink to &quot;3.1 自回归架构&quot;">​</a></h3><h4 id="_3-1-1-openvla-7b-开源-vla-业界通用强基线-·-2024-09-·-github" tabindex="-1">3.1.1 <strong>OpenVLA</strong>（7B 开源 VLA，业界通用强基线） · 2024.09 · <a href="https://github.com/openvla/openvla" target="_blank" rel="noreferrer">GitHub</a> <a class="header-anchor" href="#_3-1-1-openvla-7b-开源-vla-业界通用强基线-·-2024-09-·-github" aria-label="Permalink to &quot;3.1.1 **OpenVLA**（7B 开源 VLA，业界通用强基线） · 2024.09 · [GitHub](https://github.com/openvla/openvla)&quot;">​</a></h4><p><img src="'+a+'" alt=""></p><ul><li><strong>架构</strong>：视觉端采用 <strong>SigLIP + DINOv2</strong> 融合视觉编码器；经<strong>投影器</strong>送入 <strong>LLaMA-2/Prismatic-7B</strong> 语言主干，<strong>自回归预测离散动作 token</strong>，再<strong>解码</strong>为可执行的<strong>连续控制量</strong>。</li><li><strong>训练方法</strong>：在<strong>预训练 VLM</strong> 基座上<strong>微调</strong>；支持 <strong>LoRA/PEFT</strong>，强调<strong>数据高效迁移</strong>到<strong>新机器人/新任务</strong>。</li><li><strong>训练数据</strong>：以 <strong>Open-X-Embodiment</strong> 汇总数据为主，覆盖 <strong>≈97 万段</strong>真实机器人示范，<strong>跨平台/跨任务</strong>多样化。</li></ul><h4 id="_3-1-2-π0-fast-openpi-·-2025-01-·-github" tabindex="-1">3.1.2 <strong>π₀-FAST（openpi）</strong> · 2025.01 · <a href="https://github.com/Physical-Intelligence/openpi" target="_blank" rel="noreferrer">GitHub</a> <a class="header-anchor" href="#_3-1-2-π0-fast-openpi-·-2025-01-·-github" aria-label="Permalink to &quot;3.1.2 **π₀-FAST（openpi）** · 2025.01 · [GitHub](https://github.com/Physical-Intelligence/openpi)&quot;">​</a></h4><p><img src="'+e+'" alt=""></p><ul><li><strong>架构</strong>：使用 <strong>DCT（离散余弦变换）<strong>将</strong>连续控制离散化</strong>为可学习 <strong>token</strong>；主干用 <strong>VLM + 自回归解码头</strong>，与语言/视觉 <strong>token</strong> 统一接口。</li><li><strong>训练方法</strong>：提供<strong>从零到微调</strong>的开源<strong>训练脚本与配置</strong>。</li><li><strong>训练数据</strong>：openpi 提供 <strong>10k+ 小时</strong>机器人数据上<strong>预训练的基座与专家权重</strong>（含 <strong>DROID/ALOHA/LIBERO</strong> 等），可<strong>直接推理</strong>或<strong>再训练</strong>。</li></ul><h3 id="_3-2-扩散-流匹配架构-diffusion-flow-matching" tabindex="-1">3.2 扩散/流匹配架构（<strong>Diffusion / Flow Matching</strong>） <a class="header-anchor" href="#_3-2-扩散-流匹配架构-diffusion-flow-matching" aria-label="Permalink to &quot;3.2 扩散/流匹配架构（**Diffusion / Flow Matching**）&quot;">​</a></h3><h4 id="_3-2-1-π0-5-openpi-流匹配动作头-开放世界泛化-·-2025-04-·-github" tabindex="-1">3.2.1 <strong>π₀.₅（openpi，流匹配动作头，开放世界泛化）</strong> · 2025.04 · <a href="https://github.com/Physical-Intelligence/openpi" target="_blank" rel="noreferrer">GitHub</a> <a class="header-anchor" href="#_3-2-1-π0-5-openpi-流匹配动作头-开放世界泛化-·-2025-04-·-github" aria-label="Permalink to &quot;3.2.1 **π₀.₅（openpi，流匹配动作头，开放世界泛化）** · 2025.04 · [GitHub](https://github.com/Physical-Intelligence/openpi)&quot;">​</a></h4><p><img src="'+h+'" alt=""></p><ul><li><strong>架构</strong>：采用 <strong>Flow Matching</strong> 作为<strong>动作生成头</strong>（仓库当前仅支持 <strong>π₀.₅</strong> 的 <strong>flow-matching head</strong>），与视觉/语言特征在同一<strong>序列空间</strong>内<strong>条件生成动作</strong>。</li><li><strong>知识隔离（Knowledge Insulation）</strong>： <ul><li><strong>动机</strong>：端到端并入<strong>连续动作专家</strong>会<strong>拖慢训练</strong>且<strong>破坏 VLM 已学知识</strong>；</li><li><strong>训练期</strong>：两路并行 <ul><li>（A）<strong>VLM 主干</strong>：图像/文本/状态 → <strong>预测离散动作 token</strong>（<strong>交叉熵</strong>）；</li><li>（B）<strong>动作专家</strong>：同条件下做<strong>流匹配/扩散去噪</strong>（<strong>L2/flow</strong> 目标），<strong>梯度不回流</strong>主干；</li></ul></li><li><strong>推理期</strong>：<strong>丢弃离散支路</strong>，仅用<strong>连续动作专家</strong>生成<strong>平滑、低延迟</strong>控制。</li></ul></li><li><strong>实现与数据</strong>：提供 <strong>JAX / PyTorch</strong> 实现，支持 <strong>LIBERO / DROID</strong> 的<strong>开源复现与微调</strong>；基座在 <strong>10k+ 小时</strong>数据上预训练，并给出 <strong>LIBERO/DROID</strong> 微调与评测配置。</li></ul><h4 id="_3-2-2-rdt-1b-robotics-diffusion-transformer-·-2025-05-·-github-·-iclr-2025" tabindex="-1">3.2.2 <strong>RDT-1B（Robotics Diffusion Transformer）</strong> · 2025.05 · <a href="https://github.com/thu-ml/RoboticsDiffusionTransformer" target="_blank" rel="noreferrer">GitHub</a> · <em>ICLR 2025</em> <a class="header-anchor" href="#_3-2-2-rdt-1b-robotics-diffusion-transformer-·-2025-05-·-github-·-iclr-2025" aria-label="Permalink to &quot;3.2.2 **RDT-1B（Robotics Diffusion Transformer）** · 2025.05 · [GitHub](https://github.com/thu-ml/RoboticsDiffusionTransformer) · *ICLR 2025*&quot;">​</a></h4><p><img src="'+u+'" alt=""></p><ul><li><strong>架构</strong>：<strong>扩散 Transformer 策略</strong>，支持<strong>多视角 RGB + 语言指令</strong>条件，<strong>一次预测 64 步动作</strong>，兼容<strong>单/双臂、关节/末端</strong>等多执行接口，属“<strong>可扩展轨迹扩散策略</strong>”。</li><li><strong>训练数据</strong>：融合 <strong>46</strong> 个数据集、<strong>100 万+</strong> 多机器人轨迹，并针对 <strong>ALOHA 双臂</strong>额外收集 <strong>6k+</strong> 集，覆盖广泛任务与形态。</li></ul><h3 id="_3-3-商业闭源参考方案" tabindex="-1">3.3 商业闭源参考方案 <a class="header-anchor" href="#_3-3-商业闭源参考方案" aria-label="Permalink to &quot;3.3 商业闭源参考方案&quot;">​</a></h3><h4 id="_3-3-1-seed-gr-3-bytedance-seed-·-2025-07" tabindex="-1">3.3.1 <strong>Seed GR-3（ByteDance Seed）</strong> · 2025.07 <a class="header-anchor" href="#_3-3-1-seed-gr-3-bytedance-seed-·-2025-07" aria-label="Permalink to &quot;3.3.1 **Seed GR-3（ByteDance Seed）** · 2025.07&quot;">​</a></h4><p><img src="'+c+'" alt=""></p><ul><li><strong>架构</strong>：通用 <strong>VLA</strong>，联合<strong>视觉-语言</strong>与<strong>机器人轨迹</strong>，支持<strong>长程与可泛化</strong>操作（含<strong>可变形物体</strong>）。</li><li><strong>训练方法</strong>：<strong>统一训练（V+L+Action）</strong>，下游<strong>少量人示范</strong>即可<strong>高效适配</strong>。</li><li><strong>训练数据</strong>：<strong>视觉-语言语料 + 真实/仿真轨迹</strong>混合。</li><li><strong>可用性</strong>：技术报告与演示公开；<strong>模型未开源</strong>，商业产品化导向明确。</li></ul><h4 id="_3-3-2-gemini-robotics-google-deepmind-·-2025-09" tabindex="-1">3.3.2 <strong>Gemini Robotics（Google DeepMind）</strong> · 2025.09 <a class="header-anchor" href="#_3-3-2-gemini-robotics-google-deepmind-·-2025-09" aria-label="Permalink to &quot;3.3.2 **Gemini Robotics（Google DeepMind）** · 2025.09&quot;">​</a></h4><p><img src="'+f+'" alt=""></p><ul><li><strong>架构</strong>：<strong>Gemini</strong> 系列的<strong>机器人定制版 VLA</strong>（含 <strong>On-Device</strong> 与 <strong>ER 1.5</strong> 路线），强调在<strong>多形态平台</strong>上的<strong>感知-推理-工具使用-行动</strong>一体化；提供“<strong>本地运行</strong>”优化版本以满足<strong>低时延/离线</strong>需求。</li><li><strong>训练方法</strong>：在<strong>通用多模态基座</strong>上做具身定制，支持<strong>少量演示</strong>的<strong>快速自适应</strong>；面向<strong>可信测试者</strong>提供 <strong>SDK/工具链</strong>。</li><li><strong>训练数据</strong>：Google 内部<strong>演示</strong>（如 <strong>ALOHA</strong>）+ <strong>跨平台迁移</strong>记录（<strong>Apollo、Franka</strong> 等），细节未完全披露。</li></ul><h4 id="_3-3-3-isaac-gr00t-n1-nvidia" tabindex="-1">3.3.3 <strong>Isaac GR00T N1（NVIDIA）</strong> <a class="header-anchor" href="#_3-3-3-isaac-gr00t-n1-nvidia" aria-label="Permalink to &quot;3.3.3 **Isaac GR00T N1（NVIDIA）**&quot;">​</a></h4><ul><li><strong>架构</strong>：面向<strong>人形</strong>的<strong>通用机器人基础模型（VLA）</strong>，“<strong>System-1 快速反射动作</strong> + <strong>System-2 深度推理/规划</strong>”；与 <strong>Omniverse</strong> 合成数据、仿真与<strong>数据飞轮</strong>深度耦合。</li><li><strong>训练方法</strong>：<strong>条件扩散/Transformer</strong> 范式，结合<strong>自/合成/仿真</strong>学习与<strong>跨域蒸馏</strong>；适配多家机器人平台。</li><li><strong>训练数据</strong>：<strong>自我采集 + 合作生态</strong>（<strong>第一人称人类视频、真实/仿真轨迹、合成数据</strong>等混合）。</li><li><strong>可用性</strong>：面向生态伙伴与开发者发布<strong>可定制版本</strong>与<strong>数据/工具链</strong>；总体<strong>非完全开源</strong>、以<strong>商用生态</strong>为目标。</li></ul><h2 id="_4-落地展望-要点" tabindex="-1">4. 落地展望（要点） <a class="header-anchor" href="#_4-落地展望-要点" aria-label="Permalink to &quot;4. 落地展望（要点）&quot;">​</a></h2><h3 id="_4-1-基于开源方案的快速落地技术路线-两种备选" tabindex="-1">4.1 基于开源方案的快速落地技术路线（两种备选） <a class="header-anchor" href="#_4-1-基于开源方案的快速落地技术路线-两种备选" aria-label="Permalink to &quot;4.1 基于开源方案的快速落地技术路线（两种备选）&quot;">​</a></h3><ul><li><p><strong>方案 A（效果最优）：流匹配 / 连续扩散路线（π₀.₅ / RDT-1B）</strong></p><ul><li><strong>适用</strong>：需要<strong>细腻接触</strong>、<strong>SE(3) 精确位姿</strong>、<strong>长时间窗</strong>与<strong>复杂约束</strong>（开关/插拔/装配/柔性物体）。</li><li><strong>做法</strong>： <ol><li>以 <strong>π₀.₅</strong> 为基座（<strong>知识隔离</strong>，flow-matching 动作头），先在 <strong>DROID/LIBERO</strong> 微调；</li><li>对目标场景采<strong>少量高质量演示</strong>再适配；</li><li>如需更长轨迹/多视角，接入 <strong>RDT-1B（DiT 一次 64 步）</strong>；</li><li>部署侧启用 <strong>少步采样/并行采样/KV 缓存</strong>，并在控制层加入 <strong>SE(3) 投影 + IK/QP 约束</strong>。</li></ol></li><li><strong>优点/权衡</strong>：动作<strong>连续顺滑</strong>、可处理<strong>多峰分布</strong>与<strong>复杂接触</strong>；对<strong>算力与时延优化</strong>要求更高，工程集成相对更重。</li></ul></li><li><p><strong>方案 B（落地最快）：自回归路线（OpenVLA / π₀-FAST）</strong></p><ul><li><strong>适用</strong>：<strong>多平台通用操控</strong>、语义指令到动作的<strong>快速上线</strong>与<strong>验证</strong>（收纳、拣取、开合、搬运等半结构化任务）。</li><li><strong>做法</strong>： <ol><li>直接加载 <strong>OpenVLA</strong> 权重，按目标物体/场景做<strong>LoRA 微调</strong>；</li><li>若需更平滑控制，用 <strong>π₀-FAST</strong> 做<strong>动作离散化</strong>统一接口，末端加<strong>平滑/IK</strong>；</li><li>数据用 <strong>Open-X-Embodiment + 少量自采演示</strong> 即可跑通；</li><li>以 <strong>policy server</strong> 接入评测/灰度，快速形成<strong>数据飞轮</strong>。</li></ol></li><li><strong>优点/权衡</strong>：<strong>上手最快、脚本与生态成熟</strong>、成本低；在<strong>极致接触精度/长程一致性</strong>上不及方案 A，但可满足大多数<strong>服务与后场作业</strong>场景的首发需求。</li></ul></li></ul><hr><h3 id="_4-2-数据源与数据飞轮" tabindex="-1">4.2 数据源与数据飞轮 <a class="header-anchor" href="#_4-2-数据源与数据飞轮" aria-label="Permalink to &quot;4.2 数据源与数据飞轮&quot;">​</a></h3><ul><li><strong>公共数据直连</strong>：<strong>Open-X-Embodiment、DROID、LIBERO、ALOHA</strong> 等（优先同分布子集）；必要时用统一格式（如 <strong>LeRobot</strong>）做转换。</li><li><strong>自采与轻量示范</strong>：远程示教/VR/第一人称视频；少量高质量演示对<strong>新物体/新场景</strong>适配很有效。</li><li><strong>仿真与合成</strong>：<strong>Isaac/Omniverse/Habitat</strong> 生成可控长尾；<strong>合成文字指令</strong>与<strong>程序化场景</strong>扩大覆盖。</li><li><strong>自动标注与弱监督</strong>：多视角重建、语言描述对齐（LLM/VLM 生成标签）、成功/失败信号回流。</li><li><strong>闭环飞轮</strong>：线上部署产生的轨迹→离线清洗→定期再训练/蒸馏→灰度回放验证→小流量上线。</li></ul><hr><h3 id="_4-3-可支持的业务场景-方向级" tabindex="-1">4.3 可支持的业务场景（方向级） <a class="header-anchor" href="#_4-3-可支持的业务场景-方向级" aria-label="Permalink to &quot;4.3 可支持的业务场景（方向级）&quot;">​</a></h3><ul><li><strong>家庭/办公服务</strong>：清理整理、开关/旋钮操作、物品搬运与分类、简单烹饪辅助、桌面任务（收纳/擦拭）。</li><li><strong>仓储与零售后场</strong>：拣选与再分拣、开箱/封箱、货架补货、条码面向调整、异常品处理。</li><li><strong>轻制造与装配</strong>：取放/插拔、螺栓/卡扣操作、工位上下料、治具操作、产线切换时的小批量柔性任务。</li><li><strong>质检与实验室</strong>：视觉质检辅助手爪触探、移液/开盖/取样等标准化动作序列。</li><li><strong>巡检与运维</strong>：门禁/阀门/按钮操作，简单工具使用（扳手/螺丝刀），异常点位取证。</li></ul>',60)])])}const k=t(d,[["render",p]]);export{D as __pageData,k as default};
