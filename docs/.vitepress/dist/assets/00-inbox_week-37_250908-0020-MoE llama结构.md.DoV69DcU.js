import{_ as o,c as r,o as l,ag as s}from"./chunks/framework.OaOo95RB.js";const c=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"00-inbox/week-37/250908-0020-MoE llama结构.md","filePath":"00-inbox/week-37/250908-0020-MoE llama结构.md"}'),a={name:"00-inbox/week-37/250908-0020-MoE llama结构.md"};function n(i,t,e,g,h,$){return l(),r("div",null,[...t[0]||(t[0]=[s('<h2 id="tl-dr-≤3点" tabindex="-1">TL;DR（≤3点） <a class="header-anchor" href="#tl-dr-≤3点" aria-label="Permalink to &quot;TL;DR（≤3点）&quot;">​</a></h2><ul><li><strong>把 LLaMA 的 MLP（SwiGLU）换成 MoE</strong>：注意力仍是<strong>密集共享</strong>，只在若干层把 FFN 替换为“多专家+路由器”。</li><li><strong>路由 = 轻量线性 + Top-K + 归一权重</strong>；每个 token 只激活 <strong>K 个专家</strong>（常见 K=1 或 2），计算稀疏，参数巨量可见。</li><li>训练稳定三件套：<strong>容量上限（capacity）</strong>、<strong>负载均衡损失（aux loss）</strong>、<strong>All-to-All 专家并行</strong>。</li></ul><hr><h2 id="what-是什么" tabindex="-1">What（是什么） <a class="header-anchor" href="#what-是什么" aria-label="Permalink to &quot;What（是什么）&quot;">​</a></h2><p>以 LLaMA 基础块（RMSNorm → Self-Attn → RMSNorm → MLP/SwiGLU）为底座，把 <strong>MLP</strong> 替换成 <strong>MoE-FFN</strong>。对每个 token 隐状态 $h\\in\\mathbb{R}^d$：</p><ol><li><strong>路由打分（gating）</strong></li></ol><p>$$ s = W_r h \\in \\mathbb{R}^{E},\\quad \\text{选 } \\mathcal{T}=\\text{TopK}(s) $$</p><p>可选温度/噪声：$s \\leftarrow (s+\\text{noise})/\\tau$。</p><ol start="2"><li><strong>权重归一</strong>（只在选中的 $\\mathcal{T}$ 上 softmax）</li></ol><p>$$ g_i(h)=\\frac{\\exp(s_i)}{\\sum_{j\\in \\mathcal{T}}\\exp(s_j)},\\quad i\\in\\mathcal{T} $$</p><ol start="3"><li><strong>专家前馈</strong>（每个专家都是一套独立的 SwiGLU-FFN）</li></ol><p>$$ E_i(h)=W^{(i)}<em>{\\text{down}}!\\left(\\mathrm{SiLU}(W^{(i)}</em>{\\text{up}}h)\\odot(W^{(i)}_{\\text{gate}}h)\\right) $$</p><ol start="4"><li><strong>聚合输出</strong></li></ol><p>$$ \\mathrm{MoE}(h)=\\sum_{i\\in\\mathcal{T}} g_i(h),E_i(h) $$</p><p>（可选：再加一个<strong>共享专家</strong> $E_{\\text{shared}}$ 恒被激活，改善长尾稳健性。）</p><p>注意：<strong>注意力层不变</strong>；MoE 只替换 FFN，并可“隔层”使用（如每 2~4 层一个 MoE-FFN）。</p><hr><h2 id="why-为什么这么做-何时使用" tabindex="-1">Why（为什么这么做/何时使用） <a class="header-anchor" href="#why-为什么这么做-何时使用" aria-label="Permalink to &quot;Why（为什么这么做/何时使用）&quot;">​</a></h2><ul><li><strong>更高“表观参数量”</strong>：E 个专家 × FFN 参数，但<strong>每步只算 K 个</strong>，在相近 FLOPs 下提升模型表达力/迁移性。</li><li><strong>吞吐与成本</strong>：训练/推理仍近似稀疏计算成本；适合大规模预训练与指令跟随扩展。</li><li><strong>可控专家化</strong>：不同专家可隐式学到不同模式/域；在多域数据上更有益。</li></ul><hr><h2 id="how-最小复现配方-≤5步" tabindex="-1">How（最小复现配方，≤5步） <a class="header-anchor" href="#how-最小复现配方-≤5步" aria-label="Permalink to &quot;How（最小复现配方，≤5步）&quot;">​</a></h2><ol><li><strong>在若干 LLaMA 层把 FFN 替换为 MoE-FFN</strong>（保持维度一致；专家数 $E\\in{8,16,32}$，Top-K 常用 1 或 2，K=2 质量更好、通信更重）。</li><li><strong>实现路由与容量</strong>：为每层设容量</li></ol><p>$$ C=\\left\\lceil \\text{capacity_factor}\\cdot \\frac{B\\cdot L\\cdot K}{E}\\right\\rceil $$</p><p>每个专家最多接收 $C$ 个 token；溢出按策略 <strong>drop</strong> 或 <strong>随机回退</strong>。 3) <strong>All-to-All 专家并行</strong>：把不同专家分布到不同 GPU（expert parallel），用 A2A 把被分配的 token 打包发送/回收。 4) <strong>加负载均衡损失</strong>（Switch/GShard 风格）：</p><ul><li>令 $f_i$=分到专家 $i$ 的 token 份额，$p_i$=路由概率质量在专家 $i$ 的份额；</li><li>辅助损失 $L_{\\text{aux}} = E \\cdot \\sum_i f_i , p_i$（或等价的均衡正则/熵正则），系数一般 $10^{-2}!\\sim!10^{-1}$。</li></ul><ol start="5"><li><strong>训练细节</strong>：路由温度/噪声、容量系数（1.0–1.5）、路由 logits 的 z-loss（抑制过饱和）、混合并行（DP+TP+EP），以及 <strong>drop-tokens vs dropless</strong> 路由策略按资源取舍。</li></ol><hr><h2 id="gotchas-坑点与边界" tabindex="-1">Gotchas（坑点与边界） <a class="header-anchor" href="#gotchas-坑点与边界" aria-label="Permalink to &quot;Gotchas（坑点与边界）&quot;">​</a></h2><ul><li><strong>通信成为瓶颈</strong>：Top-2 + 大 E 会放大 All-to-All；用专家分组、流水或“分块 GEMM（megablocks）”缓解。</li><li><strong>负载不均/溢出</strong>：没有 aux loss 或温度太低时，少数专家被“吸满”；监控各专家的 <strong>利用率直方图</strong>。</li><li><strong>数值稳定</strong>：路由 logits 可加微噪声/温度；必要时 clip；K=1（Switch）更稳但上限略低。</li><li><strong>放置策略</strong>：不是所有层都用 MoE。实践里常“<strong>间隔放置</strong>”或“<strong>后段更密</strong>”（高层语义更受益）。</li><li><strong>推理并行</strong>：生产推理多用 <strong>expert sharding</strong>；批很小时通信开销相对更高，需权衡 E 与并行度。</li><li><strong>度量</strong>：除了 PPL/任务指标，<strong>看专家负载熵、容量利用率、溢出率</strong> 才能定位路由是否健康。</li></ul><hr><h3 id="常见超参参考-经验值" tabindex="-1">常见超参参考（经验值） <a class="header-anchor" href="#常见超参参考-经验值" aria-label="Permalink to &quot;常见超参参考（经验值）&quot;">​</a></h3><ul><li>$E=8$ 或 $16$，<strong>Top-K=2</strong>，capacity_factor $=1.25$；</li><li>路由温度 $\\tau\\in[0.5,1.0]$，或加小噪声（Noisy Top-K）；</li><li>$L_{\\text{aux}}$ 系数 $=0.01$ 左右；</li><li>仅 <strong>部分层</strong> 使用 MoE（例如每 2 层一个 MoE-FFN）。</li></ul><hr><p><strong>一句话</strong>：MoE-LLaMA = <strong>“注意力密集 + MoE-FFN 稀疏”</strong>。用一个轻量路由器把 token 分发给少数专家（Top-K），再按门控权重加权求和；想把它跑稳，核心在 <strong>容量与负载均衡</strong>，想把它跑快，核心在 <strong>专家并行的 All-to-All</strong>。</p>',34)])])}const d=o(a,[["render",n]]);export{c as __pageData,d as default};
