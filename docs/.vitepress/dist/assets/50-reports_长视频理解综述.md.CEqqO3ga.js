import{_ as i,c as n,o as e,ag as l,j as t,a as r}from"./chunks/framework.OaOo95RB.js";const p=JSON.parse('{"title":"MA-LMM 中的 Query-Former 与长时记忆模块","description":"","frontmatter":{},"headers":[],"relativePath":"50-reports/长视频理解综述.md","filePath":"50-reports/长视频理解综述.md"}'),a={name:"50-reports/长视频理解综述.md"};function s(g,o,d,h,u,m){return e(),n("div",null,[...o[0]||(o[0]=[l('<h1 id="ma-lmm-中的-query-former-与长时记忆模块" tabindex="-1">MA-LMM 中的 Query-Former 与长时记忆模块 <a class="header-anchor" href="#ma-lmm-中的-query-former-与长时记忆模块" aria-label="Permalink to &quot;MA-LMM 中的 Query-Former 与长时记忆模块&quot;">​</a></h1><h2 id="方法流程" tabindex="-1">方法流程 <a class="header-anchor" href="#方法流程" aria-label="Permalink to &quot;方法流程&quot;">​</a></h2><ol><li><p><strong>视觉特征提取</strong></p><ul><li>视频逐帧输入视觉编码器，提取帧特征并加入时间位置编码。</li><li>特征存入长时记忆模块。</li></ul></li><li><p><strong>长时记忆建模</strong></p><ul><li>使用 Query-Former 作为桥梁，对齐视觉特征与文本空间。</li><li>引入两个记忆库： <ul><li><strong>视觉记忆库（Visual Memory Bank）</strong>：保存所有历史帧特征，用于 cross-attention。</li><li><strong>查询记忆库（Query Memory Bank）</strong>：保存历次查询结果（learned queries），用于 self-attention。</li></ul></li></ul></li><li><p><strong>文本解码</strong></p><ul><li>Q-Former 输出的最终表示输入 LLM，生成回答或描述。</li></ul></li></ol><h2 id="输入输出" tabindex="-1">输入输出 <a class="header-anchor" href="#输入输出" aria-label="Permalink to &quot;输入输出&quot;">​</a></h2><ul><li><strong>输入</strong>：视频帧序列 + 文本提示</li><li><strong>输出</strong>：自然语言回答、视频描述或分类标签</li></ul><h2 id="记忆更新机制" tabindex="-1">记忆更新机制 <a class="header-anchor" href="#记忆更新机制" aria-label="Permalink to &quot;记忆更新机制&quot;">​</a></h2><ol><li><p><strong>视觉记忆库更新</strong></p><ul><li>每新一帧加入库中，作为 cross-attention 的 Key/Value。</li></ul></li><li><p><strong>查询记忆库更新</strong></p><ul><li>每个时间步的查询 (z_t) 累积存入库中，作为 self-attention 的 Key/Value。</li></ul></li><li><p><strong>记忆压缩（MBC）</strong></p><ul><li>计算相邻帧特征余弦相似度，合并最相似的一对特征。</li><li>保留判别性特征并减少冗余，避免显存线性增长。</li></ul></li></ol><h1 id="长视频理解方法对比-ma-lmm-vs-streaming-lvu" tabindex="-1">长视频理解方法对比：MA-LMM vs. Streaming-LVU <a class="header-anchor" href="#长视频理解方法对比-ma-lmm-vs-streaming-lvu" aria-label="Permalink to &quot;长视频理解方法对比：MA-LMM vs. Streaming-LVU&quot;">​</a></h1><table tabindex="0"><thead><tr><th>方法</th><th>对已有记忆库方法的批评</th><th>改进思路</th><th>特点</th></tr></thead><tbody><tr><td><strong>MA-LMM</strong> (Memory-Augmented LMM for Long-Term Video Understanding)</td><td>记忆库在召回历史信息时依赖<strong>显式时间戳</strong>，如果没有时间指引，难以生成全面的回答</td><td>引入 <strong>Query-Former + 双记忆库（视觉记忆库 + 查询记忆库）</strong>，通过语义查询与历史特征交互进行检索</td><td>解决了对时间戳的依赖，支持语义驱动的历史信息回忆</td></tr><tr><td><strong>Streaming-LVU</strong> (Streaming Long Video Understanding with LLMs)</td><td>在长视频场景中，传统记忆库容易出现 <strong>检索效率低</strong>，且依赖时间片段索引，难以进行流式理解</td><td>提出 <strong>Streaming 机制</strong>，动态维护上下文记忆，支持流式输入和压缩更新</td><td>强调连续视频流处理与高效记忆管理，适合超长视频场景</td></tr></tbody></table><h1 id="videostreaming-memory-propagated-streaming-encoding-adaptive-memory-selection" tabindex="-1">VideoStreaming (Memory-Propagated Streaming Encoding + Adaptive Memory Selection) <a class="header-anchor" href="#videostreaming-memory-propagated-streaming-encoding-adaptive-memory-selection" aria-label="Permalink to &quot;VideoStreaming (Memory-Propagated Streaming Encoding + Adaptive Memory Selection)&quot;">​</a></h1><h2 id="方法核心思想" tabindex="-1">方法核心思想 <a class="header-anchor" href="#方法核心思想" aria-label="Permalink to &quot;方法核心思想&quot;">​</a></h2><ul><li><strong>目标</strong>：在长视频理解中保留关键的空间信息与时间动态，同时减少冗余。</li><li><strong>核心机制</strong>： <ol><li><strong>Memory-Propagated Streaming Encoding</strong>：逐段编码视频，并将前一段的记忆传递给后一段，形成连续表示。</li><li><strong>Adaptive Memory Selection</strong>：针对具体问题，自适应地选择相关历史记忆，提升回答的针对性和精确性。</li></ol></li></ul><hr><h2 id="方法流程-1" tabindex="-1">方法流程 <a class="header-anchor" href="#方法流程-1" aria-label="Permalink to &quot;方法流程&quot;">​</a></h2><ol><li><p><strong>视频分段与逐段编码</strong></p><ul><li>将长视频划分为多个短片段（clip）。</li><li>编码当前片段时，会先参考前一个片段的记忆，再与当前特征拼接后输入到 <strong>小型 Decoder-only LM</strong>。</li><li>由于自回归特性，序列信息逐渐累积到最后几个 token，因此使用最后几个 token 作为更新后的记忆，代表当前时间点之前的全部视频信息。</li></ul></li><li><p><strong>固定长度记忆</strong></p><ul><li>模型在编码过程中始终保持固定长度的记忆，用以表示任意长的视频。</li><li>但这会导致早期细节被压缩或丢失。</li></ul></li><li><p><strong>历史记忆保存与问题驱动的记忆选择</strong></p><ul><li>为弥补固定长度记忆的不足，方法会保存 <strong>所有片段的历史记忆</strong>。</li><li>在编码每个片段时，附加一个 <strong>summary token</strong>（片段指示符），用于概括该片段的语义信息。</li><li>在回答具体问题时： <ol><li>将最终迭代的 condensed memory 与问题拼接，输入同一个小型 LM。</li><li>获取问题指示符（question indicator）。</li><li>计算问题指示符与所有片段指示符的相似度。</li><li>选取相似度最高的若干 clip memories，作为与问题最相关的历史信息。</li></ol></li><li>最终将这些 <strong>自适应选择的记忆</strong> 输入 LLM，进行详细问答。</li></ul></li></ol><hr><h2 id="输入与输出" tabindex="-1">输入与输出 <a class="header-anchor" href="#输入与输出" aria-label="Permalink to &quot;输入与输出&quot;">​</a></h2><ul><li><strong>输入</strong>：长视频（切分为多个 clip）+ 问题（question）。</li><li><strong>输出</strong>：对问题的自然语言回答，具备明确的时间指向性（temporal grounding）。</li></ul><hr><h2 id="训练策略" tabindex="-1">训练策略 <a class="header-anchor" href="#训练策略" aria-label="Permalink to &quot;训练策略&quot;">​</a></h2><ul><li><p><strong>两阶段训练</strong>：</p><ol><li><strong>单片段预训练</strong>：通过 prefix 任务，增强小型 LM 的单片段编码能力。</li><li><strong>流式训练</strong>：让其作为 streaming encoder，并与 LLM 联合训练，实现长视频理解。</li></ol></li><li><p><strong>数据构建</strong>：</p><ul><li>将短视频拼接成长视频，并保留原有问题。</li><li>使用 Panda-70M 的长视频及分段描述，构建多轮长视频 QA 数据（带显式时间戳），指导模型学习准确的记忆选择。</li></ul></li></ul><hr><h2 id="方法贡献" tabindex="-1">方法贡献 <a class="header-anchor" href="#方法贡献" aria-label="Permalink to &quot;方法贡献&quot;">​</a></h2><ol><li>分析了长视频理解中的挑战，指出现有方法存在编码效率低的问题。</li><li>提出了两大关键设计： <ul><li><strong>Memory-Propagated Streaming Encoding</strong></li><li><strong>Adaptive Memory Selection</strong></li></ul></li><li>实验表明：VideoStreaming 能在长视频基准测试中实现更精确的时间定位、更优性能和更高推理效率。</li></ol><h2 id="流式编码流程" tabindex="-1">流式编码流程 <a class="header-anchor" href="#流式编码流程" aria-label="Permalink to &quot;流式编码流程&quot;">​</a></h2><p>第一阶段</p><ol><li>将一段视频分割为多个clip(剪辑)。对每一个clip提取均匀采样16帧。通过小语言模型和mlp投影层将每一帧蒸馏为4个token，最终使得每一个每个clip具有64个token。</li></ol><p>第二阶段</p><h1 id="an-exploration-of-video-understanding-in-large-multimodal-models" tabindex="-1">An Exploration of Video Understanding in Large Multimodal Models <a class="header-anchor" href="#an-exploration-of-video-understanding-in-large-multimodal-models" aria-label="Permalink to &quot;An Exploration of Video Understanding in Large Multimodal Models&quot;">​</a></h1><h1 id="apollo-an-exploration-of-video-understanding-in-large-multimodal-models" tabindex="-1">Apollo: An Exploration of Video Understanding in Large Multimodal Models <a class="header-anchor" href="#apollo-an-exploration-of-video-understanding-in-large-multimodal-models" aria-label="Permalink to &quot;Apollo: An Exploration of Video Understanding in Large Multimodal Models&quot;">​</a></h1><h2 id="论文贡献" tabindex="-1">论文贡献 <a class="header-anchor" href="#论文贡献" aria-label="Permalink to &quot;论文贡献&quot;">​</a></h2><ol><li><p><strong>系统性探索设计空间</strong></p><ul><li>全面研究视频大规模多模态模型（video-LMMs）的关键要素：<br> 视频采样、编码器选择、token 重采样、token 融合、训练调度、数据构成等。</li><li>揭示了真正驱动视频理解性能的关键因素，并提供了可操作的设计建议。</li></ul></li><li><p><strong>提出 “Scaling Consistency”</strong></p><ul><li>发现小规模模型（2B–4B 参数以上）和小数据集上的设计选择可以可靠迁移到大模型。</li><li>显著降低研究成本，使研究者可以在低资源条件下高效实验。</li></ul></li><li><p><strong>改进评测方式：ApolloBench</strong></p><ul><li>指出现有基准测试中大量问题无需视频信息即可解答，存在冗余。</li><li>提出 <strong>ApolloBench</strong>：更高效、更紧凑的基准，评测速度快 41 倍，并更能区分视频感知与推理能力。</li></ul></li><li><p><strong>推出 Apollo 模型家族</strong></p><ul><li>基于研究洞察，训练了 <strong>Apollo-1.5B, Apollo-3B, Apollo-7B</strong> 三类模型。</li><li><strong>Apollo-3B 超越大多数 7B 模型</strong>，而 <strong>Apollo-7B 在同类中达到 SOTA</strong>，甚至超过部分 30B 模型。</li><li>展现了通过合理设计与训练，中等规模模型也能实现先进的视频理解能力。</li></ul></li></ol><hr><h2 id="论文结论" tabindex="-1">论文结论 <a class="header-anchor" href="#论文结论" aria-label="Permalink to &quot;论文结论&quot;">​</a></h2><ul><li>本研究填补了视频 LMM 缺乏系统性探索的空白，提供了从 <strong>设计 → 训练 → 评测 → 模型实现</strong> 的完整指南。</li><li><strong>Scaling Consistency</strong> 的提出，使得研究者无需依赖超大模型，也能获得可靠的实验结论，加快研究迭代。</li><li><strong>ApolloBench</strong> 提供了一个更高质量的评测工具，避免了现有基准中的“假视频理解”现象。</li><li><strong>Apollo 模型家族</strong> 的结果证明：通过精心设计，中小规模模型也能匹敌甚至超越更大规模模型。</li><li>整体上，Apollo 工作为视频 LMM 的未来发展提供了 <strong>理论洞察、实证结果与实用工具</strong>，推动该领域向更高效、更可解释的方向前进。</li></ul><h1 id="ovo-bench-在线视频理解基准" tabindex="-1">OVO-Bench: 在线视频理解基准 <a class="header-anchor" href="#ovo-bench-在线视频理解基准" aria-label="Permalink to &quot;OVO-Bench: 在线视频理解基准&quot;">​</a></h1><h2 id="研究背景" tabindex="-1">研究背景 <a class="header-anchor" href="#研究背景" aria-label="Permalink to &quot;研究背景&quot;">​</a></h2><ul><li><strong>时间感知（Temporal Awareness）</strong> 是在线视频大模型（Video-LLMs）的关键能力。</li><li>离线模型：依赖完整视频，做静态、事后分析。</li><li>在线模型：需在视频流过程中，随时间戳动态推理与回答。</li><li><strong>现有基准不足</strong>：未能充分评估“时间感知”能力。</li></ul><hr><h2 id="三大评测场景" tabindex="-1">三大评测场景 <a class="header-anchor" href="#三大评测场景" aria-label="Permalink to &quot;三大评测场景&quot;">​</a></h2><ol><li><strong>Backward tracing（回溯推理）</strong><ul><li>追溯过去事件来回答问题。</li></ul></li><li><strong>Real-time understanding（实时理解）</strong><ul><li>在当前时间戳即时理解并回答。</li></ul></li><li><strong>Forward active responding（前瞻响应）</strong><ul><li>等待未来信息足够时，再准确作答。</li></ul></li></ol><hr><h2 id="数据与任务设计" tabindex="-1">数据与任务设计 <a class="header-anchor" href="#数据与任务设计" aria-label="Permalink to &quot;数据与任务设计&quot;">​</a></h2><ul><li>共 <strong>12 个任务</strong>，<strong>644 个视频</strong>。</li><li>含 <strong>约 2800 条人工精细标注</strong>，均带有 <strong>精确时间戳</strong>。</li><li>结合 <strong>自动生成管线</strong> 与 <strong>人工标注</strong>，保证数据高质量。</li></ul><hr><h2 id="评测方法" tabindex="-1">评测方法 <a class="header-anchor" href="#评测方法" aria-label="Permalink to &quot;评测方法&quot;">​</a></h2><ul><li>设计了系统化评估流程。</li><li>沿视频时间轴动态提问，模拟真实使用场景。</li></ul><hr><h2 id="实验结果" tabindex="-1">实验结果 <a class="header-anchor" href="#实验结果" aria-label="Permalink to &quot;实验结果&quot;">​</a></h2><ul><li>评估了 <strong>11 个现有 Video-LLMs</strong>。</li><li>结果显示： <ul><li>在传统基准上表现良好。</li><li>在 <strong>在线视频理解</strong> 上，显著落后于人类表现。</li></ul></li></ul><hr><h2 id="贡献与意义" tabindex="-1">贡献与意义 <a class="header-anchor" href="#贡献与意义" aria-label="Permalink to &quot;贡献与意义&quot;">​</a></h2><ul><li><strong>填补空白</strong>：首次系统性评估视频 LLM 的时间感知能力。</li><li><strong>推动研究</strong>：促进模型在 <strong>在线动态推理</strong> 方向的进步。</li><li><strong>开源资源</strong>：<a href="https://github.com/JoeLeelyf/OVO-Bench" target="_blank" rel="noreferrer">GitHub - OVO-Bench</a></li></ul><h1 id="temporal-preference-optimization-for-long-form-video-understanding" tabindex="-1">Temporal Preference Optimization for Long-Form Video Understanding <a class="header-anchor" href="#temporal-preference-optimization-for-long-form-video-understanding" aria-label="Permalink to &quot;Temporal Preference Optimization for Long-Form Video Understanding&quot;">​</a></h1><h2 id="核心问题" tabindex="-1">核心问题 <a class="header-anchor" href="#核心问题" aria-label="Permalink to &quot;核心问题&quot;">​</a></h2><ul><li>当前视频大模型（video-LMMs）在长视频理解上表现欠佳，尤其是 <strong>时间定位 (temporal grounding)</strong> 能力不足。</li><li>现有方法依赖大规模合成/标注数据，且缺乏显式的时间优化信号。</li></ul><h2 id="方法论-temporal-preference-optimization-tpo" tabindex="-1">方法论：Temporal Preference Optimization (TPO) <a class="header-anchor" href="#方法论-temporal-preference-optimization-tpo" aria-label="Permalink to &quot;方法论：Temporal Preference Optimization (TPO)&quot;">​</a></h2><p>TPO 是一个 <strong>后训练(post-training)</strong> 框架，结合 <strong>自动偏好数据构造</strong> 与 <strong>Direct Preference Optimization (DPO)</strong>，提升模型的长时序理解能力。</p><h3 id="_1-偏好数据构造-self-training" tabindex="-1">1. 偏好数据构造 (Self-Training) <a class="header-anchor" href="#_1-偏好数据构造-self-training" aria-label="Permalink to &quot;1. 偏好数据构造 (Self-Training)&quot;">​</a></h3><p>TPO 自动生成 <strong>优选 (preferred)</strong> 与 <strong>劣选 (dis-preferred)</strong> 响应对，形成偏好数据：</p><ul><li><strong>Localized Temporal Preference</strong><ul><li>针对视频子片段生成问题。</li><li>r⁺：基于相关片段的回答。</li><li>r⁻：基于去掉该片段的其他部分生成的回答。</li></ul></li><li><strong>Comprehensive Temporal Preference</strong><ul><li>针对全局长时序问题。</li><li>r⁺：基于完整视频的回答。</li><li>r⁻：基于下采样/缺失片段的视频回答。</li></ul></li><li><strong>Post-filtering</strong>：利用 LLM（GPT-4o-mini）过滤掉错误或质量不佳的pair。</li></ul><h3 id="_2-偏好优化-direct-preference-optimization" tabindex="-1">2. 偏好优化 (Direct Preference Optimization) <a class="header-anchor" href="#_2-偏好优化-direct-preference-optimization" aria-label="Permalink to &quot;2. 偏好优化 (Direct Preference Optimization)&quot;">​</a></h3><ul><li><p>使用 DPO 训练 video-LMM，使模型更倾向输出 r⁺ 而非 r⁻。</p></li><li><p>同时结合少量 <strong>SFT Loss</strong>，保证训练稳定性。</p></li><li><p>优化目标：</p><p>$\\pi_\\theta(r^+|V,q) &gt; \\pi_\\theta(r^-|V,q)$</p></li></ul><hr><h2 id="贡献" tabindex="-1">贡献 <a class="header-anchor" href="#贡献" aria-label="Permalink to &quot;贡献&quot;">​</a></h2><ol><li><strong>提出 TPO 框架</strong>：一种轻量、可扩展的后训练方法，显式增强长视频的时间理解能力。</li><li><strong>双粒度时间偏好建模</strong>： <ul><li>Localized（细粒度）+ Comprehensive（全局依赖），实现互补效果。</li></ul></li><li><strong>无需大规模人工标注</strong>：通过自训练自动生成偏好对，降低了数据成本。</li></ol><hr><h2 id="总结" tabindex="-1">总结 <a class="header-anchor" href="#总结" aria-label="Permalink to &quot;总结&quot;">​</a></h2><p>TPO 通过 <strong>自动构造多粒度时间偏好数据 + 偏好优化训练</strong>，显著提升视频大模型的<strong>长时序理解与时间定位能力</strong>，为长视频理解提供了一种高效的<strong>后训练解决方案</strong>。</p><h3 id="方法论总结-temporal-preference-optimization-tpo" tabindex="-1">方法论总结：Temporal Preference Optimization (TPO) <a class="header-anchor" href="#方法论总结-temporal-preference-optimization-tpo" aria-label="Permalink to &quot;方法论总结：Temporal Preference Optimization (TPO)&quot;">​</a></h3><ul><li><strong>核心思想</strong>：通过构造 <strong>优选 (preferred)</strong> 与 <strong>劣选 (dis-preferred)</strong> 响应对，引导模型学习时间定位能力。</li></ul><h4 id="两类时间偏好数据" tabindex="-1">两类时间偏好数据 <a class="header-anchor" href="#两类时间偏好数据" aria-label="Permalink to &quot;两类时间偏好数据&quot;">​</a></h4><ol><li><p><strong>局部时间定位 (Localized Temporal Grounding)</strong></p><ul><li>问题针对特定片段。</li><li>r⁺：来自对应片段的回答。</li><li>r⁻：来自无关片段的回答。</li></ul></li><li><p><strong>全局时间定位 (Comprehensive Temporal Grounding)</strong></p><ul><li>问题涉及更长时序依赖。</li><li>r⁺：基于完整视频的回答。</li><li>r⁻：基于下采样、缺失关键信息的视频回答。</li></ul></li></ol><h4 id="优化方式" tabindex="-1">优化方式 <a class="header-anchor" href="#优化方式" aria-label="Permalink to &quot;优化方式&quot;">​</a></h4><ul><li>利用 <strong>Direct Preference Optimization (DPO)</strong>，让模型更倾向输出 r⁺。</li><li>简单的视频输入变换即可自动生成偏好数据。</li></ul><h4 id="效果" tabindex="-1">效果 <a class="header-anchor" href="#效果" aria-label="Permalink to &quot;效果&quot;">​</a></h4><ul><li>在 <strong>细粒度 (局部)</strong> 与 <strong>长上下文 (全局)</strong> 场景下均能提升时间推理能力。</li><li>为长视频理解提供稳健的后训练解决方案。</li></ul><h1 id="video-xl-extra-long-vision-language-model-for-hour-scale-video-understanding" tabindex="-1">Video-XL: Extra-Long Vision Language Model for Hour-Scale Video Understanding <a class="header-anchor" href="#video-xl-extra-long-vision-language-model-for-hour-scale-video-understanding" aria-label="Permalink to &quot;Video-XL:  Extra-Long Vision Language Model for Hour-Scale Video Understanding&quot;">​</a></h1><ol><li>输入与特征抽取</li></ol><ul><li>输入：长视频（小时级别，可包含 2k+ 帧）</li><li>处理： <ul><li>从视频中抽帧</li><li>每帧送入 <strong>CLIP ViT-L/14</strong>，得到帧级视觉 token（包括 [CLS] 表示）</li></ul></li></ul><hr><ol start="2"><li>语义一致性分段（Semantic Consistency Segmentation）</li></ol><ul><li>目标：避免固定长度切片，做到“语义稳定 → 粗切，语义突变 → 细切”</li><li>方法： <ol><li>使用 CLIP [CLS] embedding 表征每帧</li><li>计算 <strong>Depth Score</strong>： <ul><li>$D_t = 1 - \\cos(f_t, f_{t+1})$</li></ul></li><li>在窗口内找到局部最大值作为 <strong>分段边界</strong></li></ol></li><li>输出：动态长度的片段 $${X_1, X_2, \\ldots, X_i}$$</li></ul><hr><ol start="3"><li>动态压缩率分配</li></ol><ul><li>每个片段 $X_i$$ 分配一个压缩率 $ $\\alpha_i $ ： <ul><li>语义复杂 (Depth Score 高) → 小 $\\alpha$ → 更多 VST</li><li>语义平稳 (Depth Score 低) → 大 $\\alpha$ → 更少 VST</li></ul></li></ul><hr><ol start="4"><li>插入 Visual Summarization Token (VST)</li></ol>',88),t("ul",null,[t("li",null,[r("在片段 $X_i$ 中，每隔 $\\alpha_i$ 个视觉 token 插入一个 VST： "),t("ul",null,[t("li",null,"公式：$ k_i = |X_i| / \\alpha_i $$"),t("li",null,[r("插入后得到："),t("br"),r(" $$X'"),t("em",{i1:""},"i = [x"),r(", ..., x_{i\\alpha_i}, \\langle vs \\rangle_{i1}, ..., x_{i|X_i|}, \\langle vs \\rangle_{ik_i}]$$")])])])],-1),l('<hr><ol start="5"><li>LLM 编码 + KV 压缩</li></ol><ul><li>基础模型：<strong>Qwen2-7B</strong></li><li>编码流程： <ol><li>将片段 $X&#39;_i$ 送入 LLM，所有 token（包括 VST）参与多头自注意力</li><li>片段结束后： <ul><li><strong>丢弃普通视觉 token 的 K,V</strong></li><li><strong>保留 VST 的 K,V</strong> 作为片段的压缩记忆</li></ul></li><li>编码下一个片段 $X_{i+1}$ 时： <ul><li>仅能 attend 到之前所有 VST 的 KV（作为代理）</li></ul></li></ol></li></ul><hr><ol start="6"><li>累积记忆 &amp; 长视频建模</li></ol><ul><li>VST KV-cache 在不同片段间累积，形成“压缩版的全局记忆”</li><li>优点： <ul><li>上下文长度消耗 ~ 与 VST 数量线性相关</li><li>避免原始视觉 token 的二次复杂度 &amp; 巨大显存开销</li></ul></li></ul><hr><ol start="7"><li>训练策略</li></ol><ul><li><strong>指令微调</strong>：统一图像/多图/视频任务</li><li><strong>课程学习</strong>：从小压缩比 (2×/4×) → 大压缩比 (8×/16×)，逐步适应</li><li><strong>合成数据 VICO</strong>：基于视频分段字幕生成跨段 QA，用于训练长视频推理能力</li></ul><hr><ol start="8"><li>整体特点</li></ol><ul><li>基于 LLaVA-like MLLM，而非专门的 Video-Encoder</li><li>创新点：<strong>LLM 内部的 KV 压缩 (VST) + 动态自适应分段</strong></li><li>能在单卡上处理 <strong>小时级视频 (2000+ 帧)</strong>，同时保持细粒度推理能力</li></ul><h1 id="streammem-query-agnostic-kv-cache-memory-for-streaming-video-understanding" tabindex="-1">StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding <a class="header-anchor" href="#streammem-query-agnostic-kv-cache-memory-for-streaming-video-understanding" aria-label="Permalink to &quot;StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding&quot;">​</a></h1><table tabindex="0"><thead><tr><th>模块</th><th>内容</th></tr></thead><tbody><tr><td><strong>输入</strong></td><td>流式视频（逐段到达的帧序列）</td></tr><tr><td><strong>输出</strong></td><td>视频问答 / 推理结果（QA、检索、排序等任务）</td></tr><tr><td><strong>主干网络 (Backbone)</strong></td><td>现有 MLLMs（如 <strong>LLaVA-OneVision, Qwen2-VL, Qwen2.5-VL</strong>），<strong>StreamMem 作为插件，不需再训练</strong></td></tr><tr><td><strong>流程概述</strong></td><td>1. 视频帧流式输入 2. 输入帧压缩（减少冗余） 3. 送入视觉编码器 → 投影到 MLLM 4. 每段计算 KV-cache → 通过 <strong>Saliency Metric</strong> 选择重要 KV 5. <strong>Frame-wise KV merging</strong> 构建帧级 prototype 表示 6. 累积形成 <strong>紧凑 KV memory</strong> 7. QA 阶段利用 KV memory 进行推理</td></tr><tr><td><strong>KV 压缩机制</strong></td><td>- <strong>显著性筛选 (Saliency Metric)</strong>：基于 visual token 与 chat template token 的 cross-attention 分数，选出重要 KV - <strong>输入压缩</strong>：减少重复帧输入 - <strong>KV 合并</strong>：相似 KV 合并为 prototype，保持多样性同时节省内存</td></tr><tr><td><strong>特点</strong></td><td>- <strong>Query-agnostic</strong>：不依赖提前知道问题，压缩时通用保留信息 - <strong>Training-free</strong>：无需微调，可直接接入任意 MLLM - <strong>Bounded Memory</strong>：内存占用随视频长度不增长，避免 OOM</td></tr><tr><td><strong>应用场景</strong></td><td>- 实时流式视频理解 - 内存受限设备上的长视频推理 - 开放世界的连续视频 Agent</td></tr></tbody></table><table tabindex="0"><thead><tr><th>方面</th><th><strong>Video-XL: 动态压缩率分配</strong></th><th><strong>StreamMem: Saliency Metric</strong></th></tr></thead><tbody><tr><td><strong>触发时机</strong></td><td>在 <strong>视频分段阶段</strong> 就决定压缩率</td><td>在 <strong>KV 压缩阶段</strong> 选择哪些 KV 保留</td></tr><tr><td><strong>依据信号</strong></td><td>- <strong>Depth Score</strong>：基于相邻帧的 CLIP [CLS] 余弦相似度差 - 捕捉语义突变（动作切换点）</td><td>- <strong>Cross-attention 分数</strong>：视觉 token 与 chat template token 的注意力强度 - 捕捉哪些视觉 token 对语言理解更有用</td></tr><tr><td><strong>压缩方式</strong></td><td>- 动态设置 <strong>插入 VST 的频率</strong> - 语义复杂 → 更多 VST，保留更细信息 - 语义平稳 → 更少 VST，节省内存</td><td>- 从已有的 KV-cache 中 <strong>选择/合并重要 token</strong> - 保留显著 token 的 KV - 合并冗余 KV 为 prototype</td></tr><tr><td><strong>目标</strong></td><td>- 自适应分配压缩比，保证语义边界处信息密集 - <strong>“结构级别”压缩</strong>（按片段分配 VST 数量）</td><td>- 无需 query，也能在 KV-cache 中挑选关键信息 - <strong>“内容级别”压缩</strong>（按 token 显著性筛选）</td></tr><tr><td><strong>优势</strong></td><td>- 对“语义边界”敏感，能在动作切换点保留更多细节 - 易解释（Depth Score 可视化清晰）</td><td>- 与 query 无关，支持 <strong>流式输入</strong> - 不需重新切片，能在线动态选择 KV</td></tr><tr><td><strong>局限</strong></td><td>- 需要提前做分段（offline） - 偏离线场景，不适合纯 streaming</td><td>- 注意力显著性可能不等于任务显著性 - 可能保留了一些“假重要”的 token</td></tr></tbody></table><h1 id="qwen2-5-omni-end-to-end-multimodal-model" tabindex="-1">Qwen2.5-Omni: End-to-End Multimodal Model <a class="header-anchor" href="#qwen2-5-omni-end-to-end-multimodal-model" aria-label="Permalink to &quot;Qwen2.5-Omni: End-to-End Multimodal Model&quot;">​</a></h1><h2 id="模型特点" tabindex="-1">模型特点 <a class="header-anchor" href="#模型特点" aria-label="Permalink to &quot;模型特点&quot;">​</a></h2><ul><li><strong>输入模态</strong>：文本、图像、音频、视频</li><li><strong>输出模态</strong>：文本 + 语音（自然语音，支持流式生成）</li><li><strong>流式处理</strong>：支持输入和输出的实时流式处理</li></ul><h2 id="核心方法" tabindex="-1">核心方法 <a class="header-anchor" href="#核心方法" aria-label="Permalink to &quot;核心方法&quot;">​</a></h2><ol><li><p><strong>Block-wise Processing</strong></p><ul><li>音频和视频编码器采用分块处理</li><li>编码器负责感知 → LLM 负责长序列建模</li><li>实现高效解耦和跨模态融合</li></ul></li><li><p><strong>TMRoPE (Time-aligned Multimodal RoPE)</strong></p><ul><li>新的时间对齐位置编码</li><li>将音频与视频帧按时间顺序交错排列，实现跨模态时间同步</li></ul></li><li><p><strong>Thinker–Talker 架构</strong></p><ul><li><strong>Thinker</strong>：LLM，负责文本生成</li><li><strong>Talker</strong>：双轨自回归模型，直接基于 Thinker 的隐表示生成音频 token</li><li>两者端到端联合训练与推理，避免模态间干扰</li></ul></li><li><p><strong>Streaming Audio Decoding</strong></p><ul><li>使用滑动窗口 DiT（Diffusion Transformer）</li><li>限制感受野以降低初始延迟，提高流式语音生成体验</li></ul></li></ol><h2 id="实验结果-1" tabindex="-1">实验结果 <a class="header-anchor" href="#实验结果-1" aria-label="Permalink to &quot;实验结果&quot;">​</a></h2><ul><li>与 <strong>Qwen2.5-VL</strong> 相当，优于 <strong>Qwen2-Audio</strong></li><li>在 <strong>Omni-Bench</strong> 等多模态基准上达到 SOTA</li><li>语音指令跟随任务效果接近文本输入能力（MMLU, GSM8K）</li><li>流式语音生成在稳健性与自然度上超越大多数现有方法（包括非流式）</li></ul><h2 id="关键贡献" tabindex="-1">关键贡献 <a class="header-anchor" href="#关键贡献" aria-label="Permalink to &quot;关键贡献&quot;">​</a></h2><ul><li>首个端到端 <strong>流式多模态输入–输出</strong> 模型</li><li>提出 <strong>TMRoPE</strong> 解决跨模态时间同步</li><li>提出 <strong>Thinker–Talker</strong> 架构实现文本与语音的并行生成</li><li>推出高效 <strong>流式语音解码</strong> 方法，降低延迟，提升交互体验</li></ul><p><strong>不同模态的处理方式</strong></p><ul><li><strong>Text 文本</strong><ul><li>只用 1D 的位置 ID（等价于普通的 RoPE）。</li></ul></li><li><strong>Audio 音频</strong><ul><li>每 40ms 对应一个 temporal ID。</li><li>相当于：音频序列直接按时间轴对齐。</li></ul></li><li><strong>Image 图像</strong><ul><li>每个图像 token 的 temporal ID 相同（因为一张图像就是一个时间点）。</li><li>高度、宽度的 ID 则根据在图像中的位置分配（2D RoPE）。</li></ul></li><li><strong>Video 视频</strong><ul><li>视为多张图像序列。</li><li>每帧视频有一个<strong>独立的 temporal ID</strong>（随时间推移递增）。</li><li><strong>高度、宽度依然按图像 token 的 2D 位置分配</strong>。</li><li><strong>帧率不固定时</strong>：temporal ID 会根据实际时间调整，<strong>确保 40ms 对应 1 个 ID，与音频对齐。</strong></li></ul></li></ul><p>在 TMRoPE 中，视频被视作一系列图像，每一帧除了<strong>分配高度和宽度位置 ID</strong> 外，还会根据<strong>时间顺序获得递增的时间 ID</strong>。由于视频帧率并不固定，TMRoPE 会依据帧的实际时间戳<strong>动态调整时间 ID</strong>，确保始终遵循“一个时间 ID 对应 40ms”的规则。这样，<strong>视频帧的时间 ID 能与音频的时间 ID 对齐</strong>，<strong>实现跨模态的精确时序同步</strong>，从而保证模型在多模态输入下既能<strong>捕捉空间结构</strong>，又能保持<strong>时间一致性</strong>。</p><table tabindex="0"><thead><tr><th>阶段</th><th>训练策略</th><th>数据规模 &amp; 类型</th><th>目标</th></tr></thead><tbody><tr><td><strong>阶段一</strong>：模态编码器适配</td><td>- 冻结 LLM- 仅训练视觉/音频编码器（先 adapter 后 encoder）</td><td>- 图文对- 音频-文本对</td><td>- 建立视觉-文本、音频-文本的语义对齐- 为 LLM 融合多模态奠定基础</td></tr><tr><td><strong>阶段二</strong>：多模态联合训练</td><td>- 解冻所有参数，全模型训练</td><td>- 图像/视频相关 <strong>8000 亿 tokens</strong>- 音频相关 <strong>3000 亿 tokens</strong>- 音视频联合 <strong>1000 亿 tokens</strong>- 纯文本数据</td><td>- 跨模态交互与理解- 多任务学习能力- 保持/提升语言能力</td></tr><tr><td><strong>阶段三</strong>：长序列训练</td><td>- 扩展序列长度到 32,768 tokens</td><td>- 长文本- 长音频- 长视频- 长图像序列</td><td>- 增强复杂长序列建模能力- 适应真实场景下的长时依赖</td></tr></tbody></table><h2 id="qwen-vl-视频能力分析" tabindex="-1">Qwen-VL 视频能力分析 <a class="header-anchor" href="#qwen-vl-视频能力分析" aria-label="Permalink to &quot;Qwen-VL 视频能力分析&quot;">​</a></h2><table tabindex="0"><thead><tr><th>方面</th><th>优势</th><th>缺陷</th></tr></thead><tbody><tr><td><strong>时间建模</strong></td><td>- 动态 FPS 采样，避免冗余帧- MRoPE 与绝对时间对齐，可进行秒级事件定位</td><td>- 没有显式记忆机制，无法保持跨片段的长时依赖- 跨场景因果推理能力有限</td></tr><tr><td><strong>Token 处理</strong></td><td>- 14×14 patch 切分 + 2×2 merge + MLP 压缩，显著减少 token 数- 支持更长视频输入（最高 32k tokens）</td><td>- 压缩导致细节丢失（如小目标跟踪、动作细节理解）- 对高精度需求的视频任务可能性能下降</td></tr><tr><td><strong>注意力机制</strong></td><td>- 窗口注意力降低计算复杂度，仅少数层全局 self-attention- 能处理分钟级视频而保持效率</td><td>- 全局一致性弱，长视频中不同片段之间的语义连接不稳固</td></tr><tr><td><strong>长序列支持</strong></td><td>- 最终阶段训练扩展至 32k tokens，覆盖长视频、长音频数据- 在 LongVideoBench、LVBench 等基准超越 GPT-4o</td><td>- 32k tokens 仍有限制，无法端到端覆盖小时级超长视频- 实际需依赖分段/滑动窗口处理</td></tr><tr><td><strong>整体表现</strong></td><td>- 在分钟级长视频（几千帧）中具备强时序理解和事件定位能力</td><td>- 超长视频（&gt;30 分钟至小时级）理解力不足，易片段化、遗忘全局语义</td></tr></tbody></table><h3 id="边界与缺陷" tabindex="-1">边界与缺陷 <a class="header-anchor" href="#边界与缺陷" aria-label="Permalink to &quot;边界与缺陷&quot;">​</a></h3><ul><li><strong>记忆机制缺失</strong>：依赖 <strong>token 压缩 + 窗口 attention</strong>，没有引入 memory bank / 分层时序建模 → 在跨场景、长时依赖视频任务上仍会失效。</li><li><strong>小时级视频 ≠ 全局建模</strong>：虽然可以输入小时级视频，但受限于 <strong>32k token 上限</strong>，无法在端到端条件下完整编码小时级视频（需要分块/滑动窗口）。</li><li><strong>细粒度信息丢失</strong>：2×2 patch merge 等压缩方式减少了时空分辨率 → 对精细动作、细微物体变化不够敏感。</li><li><strong>跨片段推理能力有限</strong>：在需要全局语义一致性、因果链路建模的任务（如电影剧情问答）上存在弱点。</li></ul>',32)])])}const b=i(a,[["render",s]]);export{p as __pageData,b as default};
