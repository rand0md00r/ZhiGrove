import{_ as o,c as l,o as r,ag as a}from"./chunks/framework.CQuhCYrb.js";const p=JSON.parse('{"title":"惊讶度","description":"","frontmatter":{"title":"惊讶度","created":"2025-09-07 17:02","updated":"2025-09-07T00:00:00.000Z","origin":"week-35","type":"knowledge","status":"draft","tags":[],"links":[]},"headers":[],"relativePath":"10-knowledge/250829-0735-惊讶度-overview.md","filePath":"10-knowledge/250829-0735-惊讶度-overview.md"}'),i={name:"10-knowledge/250829-0735-惊讶度-overview.md"};function n(e,t,s,g,$,h){return r(),l("div",null,[...t[0]||(t[0]=[a('<h2 id="tl-dr-≤3点" tabindex="-1">TL;DR（≤3点） <a class="header-anchor" href="#tl-dr-≤3点" aria-label="Permalink to &quot;TL;DR（≤3点）&quot;">​</a></h2><ul><li>定义：<strong>惊讶度 / 自信息</strong> $I(x)=-\\log p(x)$（底 2→bit，底 $e$→nat）。</li><li>关系：<strong>熵 = 平均惊讶度</strong>；NLL 就是“真实样本”的惊讶度；交叉熵是用真分布对模型惊讶度求平均。</li><li>性质：越稀有越“惊”，<strong>可加性</strong>（独立事件相加），序列总惊讶度 = 各 token 惊讶度之和。</li></ul><h2 id="what-是什么" tabindex="-1">What（是什么） <a class="header-anchor" href="#what-是什么" aria-label="Permalink to &quot;What（是什么）&quot;">​</a></h2><ul><li><strong>定义</strong>：$I(x)=-\\log p(x)$。概率越小，$I$ 越大。</li><li><strong>单位</strong>： <ul><li>bit：$I(x)=-\\log_2 p(x)$，可解释为“理想前缀码所需的比特数”。</li><li>nat：$I(x)=-\\ln p(x)$；换算：$1\\text{ nat}= \\log_2 e \\approx 1.4427\\text{ bit}$。</li></ul></li><li><strong>加性</strong>：独立事件 $x,y$，$I(x,y)=I(x)+I(y)$。</li></ul><h2 id="why-为什么这么做-何时使用" tabindex="-1">Why（为什么这么做/何时使用） <a class="header-anchor" href="#why-为什么这么做-何时使用" aria-label="Permalink to &quot;Why（为什么这么做/何时使用）&quot;">​</a></h2><ul><li><strong>建模直觉</strong>：它把“罕见性”变成可加的度量，便于<strong>求和、求期望、做梯度优化</strong>。</li><li><strong>训练指标</strong>：语言建模里，<strong>NLL</strong> 就是对真实 token 的惊讶度；<strong>平均 NLL</strong> 就是<strong>交叉熵</strong>，最小化它等价于最大似然。</li><li><strong>分析用</strong>：定位<strong>高惊讶度 token</strong>（模型最不确定/最关键的分岔点），做对齐、采样控制或误差分析。</li></ul><h2 id="how-最小复现配方-≤5步" tabindex="-1">How（最小复现配方，≤5步） <a class="header-anchor" href="#how-最小复现配方-≤5步" aria-label="Permalink to &quot;How（最小复现配方，≤5步）&quot;">​</a></h2><ol><li>给定某位置 $t$ 的 logits $z_t\\in\\mathbb{R}^V$ 与温度 $T&gt;0$（$\\beta=1/T$），得策略分布<br> $p_t(v)=\\dfrac{e^{\\beta z_{t,v}}}{\\sum_j e^{\\beta z_{t,j}}}$。</li><li>对真实/采样到的 token $y_t$，其<strong>惊讶度</strong>为<br> $I_t(y_t)=-\\log p_t(y_t)$（选定对数底）。</li><li>序列总惊讶度（链式法则）：$I(y_{1:L})=\\sum_{t=1}^L I_t(y_t)$。</li><li><strong>平均惊讶度</strong>（=熵）：$H_t=\\mathbb{E}_{v\\sim p_t}[I_t(v)]=-\\sum_v p_t(v)\\log p_t(v)$。</li><li>数据/批次上取平均：得到<strong>平均 NLL / 交叉熵</strong>；若用 nat，<strong>困惑度</strong>$=\\exp(H)$；若用 bit，困惑度 $=2^H$。</li></ol><h2 id="gotchas-坑点与边界" tabindex="-1">Gotchas（坑点与边界） <a class="header-anchor" href="#gotchas-坑点与边界" aria-label="Permalink to &quot;Gotchas（坑点与边界）&quot;">​</a></h2><ul><li><strong>底数与单位</strong>：别混用 bit 和 nat；换算需乘 $\\log_2 e$ 或 $\\ln 2$。</li><li><strong>$p=0$ 的无穷大</strong>：$I=-\\log 0=+\\infty$；实际需 <strong>加平滑/剪裁</strong>（如 $p\\leftarrow\\max(p,\\varepsilon)$）。</li><li><strong>训练 vs 推理分布</strong>：教师强制下的 $p_t$ 与 top-$k$/top-$p$/温度重采样后的分布不同；你报告的惊讶度需<strong>说明计算口径</strong>。</li><li><strong>平均 vs 个体</strong>：熵（平均惊讶度）是<strong>分布属性</strong>；$-\\log p(y_t)$ 是<strong>样本属性</strong>。混淆二者会误读指标。</li><li><strong>尺度直觉</strong>：概率每 <strong>×1/2</strong>，惊讶度 <strong>+1 bit</strong>；极低概率 token 的 $I$ 会主导序列总值。</li></ul><h2 id="raw-notes" tabindex="-1">Raw Notes <a class="header-anchor" href="#raw-notes" aria-label="Permalink to &quot;Raw Notes&quot;">​</a></h2><p>下面把“惊讶度（surprisal / self-information）”说得又直白又准：</p><h2 id="它是什么" tabindex="-1">它是什么 <a class="header-anchor" href="#它是什么" aria-label="Permalink to &quot;它是什么&quot;">​</a></h2><ul><li><p>一句话：<strong>某件事有多罕见，就有多“惊”</strong>。</p></li><li><p>数学定义：$\\text{惊讶度} = I(x) = -\\log p(x)$ 概率 $p(x)$ 越小，$I(x)$ 越大；常用底数：</p><ul><li>以 2 为底（$\\log_2$）：单位是 <strong>bit</strong>（信息位）</li><li>以 e 为底（$\\ln$）：单位是 <strong>nat</strong></li></ul></li></ul><h2 id="为什么用-log-p-表示" tabindex="-1">为什么用 “$-\\log p$” 表示 <a class="header-anchor" href="#为什么用-log-p-表示" aria-label="Permalink to &quot;为什么用 “$-\\log p$” 表示&quot;">​</a></h2><ul><li><p>直觉1（稀有 = 更有料）：小概率事件带来更多“新信息”，所以数值更大。</p></li><li><p>直觉2（可加性）：独立事件 $x,y$ 同时发生的惊讶度</p><p>$$ I(x,y)=-\\log p(x,y)=-\\log p(x)-\\log p(y)=I(x)+I(y) $$</p><p>——“连续两次巧合”比一次更震惊，数值直接<strong>相加</strong>，很顺手。</p></li></ul><h2 id="一个小表-log-2-单位-bit" tabindex="-1">一个小表（$\\log_2$ 单位：bit） <a class="header-anchor" href="#一个小表-log-2-单位-bit" aria-label="Permalink to &quot;一个小表（$\\log_2$ 单位：bit）&quot;">​</a></h2><table tabindex="0"><thead><tr><th>概率 $p$</th><th style="text-align:right;">惊讶度 $I=-\\log_2 p$</th><th>直觉</th></tr></thead><tbody><tr><td>1（必然）</td><td style="text-align:right;">0 bit</td><td>一点不惊讶</td></tr><tr><td>1/2</td><td style="text-align:right;">1 bit</td><td>抛硬币出正面</td></tr><tr><td>1/4</td><td style="text-align:right;">2 bit</td><td>连续两次正面</td></tr><tr><td>1/8</td><td style="text-align:right;">3 bit</td><td>连续三次正面</td></tr><tr><td>1/100</td><td style="text-align:right;">≈6.64 bit</td><td>百分之一的巧合</td></tr><tr><td>1/1000</td><td style="text-align:right;">≈9.97 bit</td><td>千分之一的大巧合</td></tr></tbody></table><blockquote><p>记忆法：<strong>概率每×1/2，惊讶度 +1 bit</strong>。</p></blockquote><h2 id="和-熵-的关系" tabindex="-1">和“熵”的关系 <a class="header-anchor" href="#和-熵-的关系" aria-label="Permalink to &quot;和“熵”的关系&quot;">​</a></h2><ul><li><p><strong>熵 = 平均惊讶度</strong></p><p>$$ H = \\mathbb{E}_{x\\sim p}[-\\log p(x)] = \\sum_x p(x),I(x) $$</p><p>——一个分布整体“有多不确定”，就是把它每个结果的惊讶度按其概率<strong>求平均</strong>。</p></li></ul><h2 id="和-nll-交叉熵的关系" tabindex="-1">和 NLL/交叉熵的关系 <a class="header-anchor" href="#和-nll-交叉熵的关系" aria-label="Permalink to &quot;和 NLL/交叉熵的关系&quot;">​</a></h2><ul><li>训练里常见的 <strong>负对数似然（NLL）</strong> 就是对“真实发生的那个结果”的惊讶度：$-\\log p(y)$。</li><li><strong>交叉熵</strong>是用“真实分布”对<strong>模型分布的惊讶度</strong>做平均。</li></ul><h2 id="语言模型里的直觉" tabindex="-1">语言模型里的直觉 <a class="header-anchor" href="#语言模型里的直觉" aria-label="Permalink to &quot;语言模型里的直觉&quot;">​</a></h2><ul><li><p>“The cat sat on the ___”</p><ul><li>“mat”的模型概率高 → <strong>惊讶度低</strong>（很顺）</li><li>“unicorn”的概率低 → <strong>惊讶度高</strong>（很怪）</li></ul></li><li><p>在一段生成里：<strong>高惊讶度的 token</strong>=模型最犹豫、最关键的分岔点；<strong>低惊讶度的 token</strong>=模板化续写。</p></li></ul><h2 id="单位怎么理解-bit-的直观" tabindex="-1">单位怎么理解（bit 的直观） <a class="header-anchor" href="#单位怎么理解-bit-的直观" aria-label="Permalink to &quot;单位怎么理解（bit 的直观）&quot;">​</a></h2><ul><li>$I(x)=k$ bit 可以理解为：“平均需要 <strong>k 个是/否</strong> 问题才能把这个结果区分出来”。 例：$p=1/8\\Rightarrow I=3$ bit——需要 3 次二分才能定位到它。</li></ul><hr><p><strong>一句话总结</strong>： 惊讶度 $I(x)=-\\log p(x)$ 是“<strong>稀有度尺子</strong>”：越少见，数值越大；它能相加，平均后就是熵。用它，我们能精确刻画“这一步到底有多让模型犯难”。</p>',29)])])}const b=o(i,[["render",n]]);export{p as __pageData,b as default};
