import{_ as e,c as t,o as i,ag as l}from"./chunks/framework.OaOo95RB.js";const u=JSON.parse('{"title":"OpenUni训练方法","description":"","frontmatter":{"title":"OpenUni训练方法","created":"2025-09-07 17:01","updated":"2025-09-07T00:00:00.000Z","origin":"week-35","type":"knowledge","status":"draft","tags":["OpenUni","train"],"links":[]},"headers":[],"relativePath":"10-knowledge/250826-0737-OpenUni训练方法-overview.md","filePath":"10-knowledge/250826-0737-OpenUni训练方法-overview.md"}'),r={name:"10-knowledge/250826-0737-OpenUni训练方法-overview.md"};function o(n,a,h,s,c,d){return i(),t("div",null,[...a[0]||(a[0]=[l('<h2 id="tl-dr-≤3点" tabindex="-1">TL;DR（≤3点） <a class="header-anchor" href="#tl-dr-≤3点" aria-label="Permalink to &quot;TL;DR（≤3点）&quot;">​</a></h2><ul><li>总结了OpenUni的训练方法；</li><li></li><li></li></ul><h2 id="what-是什么" tabindex="-1">What（是什么） <a class="header-anchor" href="#what-是什么" aria-label="Permalink to &quot;What（是什么）&quot;">​</a></h2><ul><li></li></ul><h2 id="why-为什么这么做-何时使用" tabindex="-1">Why（为什么这么做/何时使用） <a class="header-anchor" href="#why-为什么这么做-何时使用" aria-label="Permalink to &quot;Why（为什么这么做/何时使用）&quot;">​</a></h2><ul><li></li></ul><h2 id="how-最小复现配方-≤5步" tabindex="-1">How（最小复现配方，≤5步） <a class="header-anchor" href="#how-最小复现配方-≤5步" aria-label="Permalink to &quot;How（最小复现配方，≤5步）&quot;">​</a></h2><ul><li></li></ul><h2 id="gotchas-坑点与边界" tabindex="-1">Gotchas（坑点与边界） <a class="header-anchor" href="#gotchas-坑点与边界" aria-label="Permalink to &quot;Gotchas（坑点与边界）&quot;">​</a></h2><ul><li></li></ul><h2 id="raw-notes" tabindex="-1">Raw Notes <a class="header-anchor" href="#raw-notes" aria-label="Permalink to &quot;Raw Notes&quot;">​</a></h2><h1 id="训练是怎么做的" tabindex="-1">训练是怎么做的 <a class="header-anchor" href="#训练是怎么做的" aria-label="Permalink to &quot;训练是怎么做的&quot;">​</a></h1><ul><li><p>整体思路：用最小连接模块把一个冻结的多模态 LLM与一个**扩散/Flow-Matching DiT（SANA）**连接起来。连接模块只含 N=256 个可学习 query 和 6 层 Transformer 的轻量 connector，把 LLM 的条件特征送入扩散模型的 cross-attention 中。LLM 始终冻结，视觉生成能力主要靠 connector+扩散模型学习。 arXiv</p></li><li><p>两阶段训练（Two-stage）：</p><ul><li><p>Stage I（预训练，23M对）：只训练 learnable queries + connector，扩散模型冻结（LLM 也冻结）。目标是先把 LLM 条件正确“翻译”成扩散模型可用的条件信号。超参：AdamW，LR=1e-4，Batch=512，Cosine 调度，Warm-up 1k 步，训练 100k 步，GradClip=1.0，WD=0.05，betas=(0.9,0.95)。 arXiv +1</p></li><li><p>Stage II（高质微调，60K对）：解冻扩散模型并与 connector 一起微调（LLM 仍冻结）。超参：AdamW，LR=1e-5，Batch=256，Cosine 调度，Warm-up 100 步，训练 10k 步，其他同上。 arXiv</p></li></ul></li><li><p>提示模板与 CFG：文本到图像训练时的提示统一成 User: Generate an image &lt;caption&gt;\\n Assistant:，并对 10% 样本把 &lt;caption&gt; 置空以支持推理时的 Classifier-Free Guidance (CFG)。 arXiv</p></li><li><p>模型变体：提供 B-512、L-512、L-1024 三个版本（分别对应 InternVL3-1B/2B 与 SANA-0.6B/1.6B，不同分辨率）。训练配方对三者一致。</p></li></ul><h1 id="数据集是怎么构造的" tabindex="-1">数据集是怎么构造的 <a class="header-anchor" href="#数据集是怎么构造的" aria-label="Permalink to &quot;数据集是怎么构造的&quot;">​</a></h1><ul><li><p>预训练语料（23M 图文对）：来自多个公开集合的大合并集，并统一用 LLM 重新标注（re-caption）。论文点名包含：text-to-image-2M、LAION-Aesthetic-6M、Megalith-10M、RedCaps-5M 等，合计约 2300 万 图文对；这些图像的文本描述全部由 LLM 生成/改写，并计划公开。 arXiv</p></li><li><p>高质量微调集（60K 图文对）：采用 BLIP3-o 发布的 6 万高质指令型图文数据。其构造方法是：用 GPT-4o 生成多样 caption，再使用 DALL·E-3、Midjourney 等模型合成图像，从而得到“指令→图像”的高一致性样本，用于 Stage II 的对齐与质量提升。 arXiv</p></li><li><p>数据使用方式：Stage I 用 23M 合并集先学“桥接”；Stage II 用 60K 高质指令数据做小步微调，增强指令跟随与画质鲁棒性。</p></li></ul>',15)])])}const L=e(r,[["render",o]]);export{u as __pageData,L as default};
