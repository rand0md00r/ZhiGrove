import{_ as a,c as i,o as r,ag as t}from"./chunks/framework.OaOo95RB.js";const m=JSON.parse('{"title":"Qwen2.5-Omni论文阅读","description":"","frontmatter":{"title":"Qwen2.5-Omni论文阅读","created":"2025-09-07 17:02","updated":"2025-09-07T00:00:00.000Z","origin":"week-35","type":"paper","status":"draft","tags":[],"links":[]},"headers":[],"relativePath":"20-papers/250829-0735-Qwen2.5-Omni论文阅读.md","filePath":"20-papers/250829-0735-Qwen2.5-Omni论文阅读.md"}'),n={name:"20-papers/250829-0735-Qwen2.5-Omni论文阅读.md"};function o(l,e,s,d,h,c){return r(),i("div",null,[...e[0]||(e[0]=[t('<h2 id="raw-notes" tabindex="-1">Raw Notes <a class="header-anchor" href="#raw-notes" aria-label="Permalink to &quot;Raw Notes&quot;">​</a></h2><p><a href="https://arxiv.org/pdf/2503.20215" target="_blank" rel="noreferrer">论文连接</a></p><h1 id="_2-architecture" tabindex="-1">2. Architecture <a class="header-anchor" href="#_2-architecture" aria-label="Permalink to &quot;2. Architecture&quot;">​</a></h1><h2 id="_2-1-overview" tabindex="-1">2.1 Overview <a class="header-anchor" href="#_2-1-overview" aria-label="Permalink to &quot;2.1 Overview&quot;">​</a></h2><p>Figure 2 中展示了Qwen2.5-Omni采用的Thinker-Talker 架构。</p><ul><li>Thinker用于处理和理解多模态的输入，包括文本，语音，视频（无音频），生成高纬度的表示，以及对应的文本；</li><li>Talker采用一个流管理（streaming manner）处理高维表征和Thinker输出的文本，并输出成离散的token。</li><li>Thinker是一个Transformer 解码器的结构，并配有编码器用于处理多模态信息的输入；</li><li>Talker 是一个双轨自回归Transformer解码器架构（Dual-track autoregressive Transformer Decoder architecture），借鉴了Mini-Omni模型。</li><li>在训练和推理中，talker直接接收来自于Thinker的高维表征，并共享所有Thinker的历史上下文信息。</li></ul><h2 id="_2-2-perceivation" tabindex="-1">2.2 Perceivation <a class="header-anchor" href="#_2-2-perceivation" aria-label="Permalink to &quot;2.2 Perceivation&quot;">​</a></h2><h3 id="文本、音频、图像、视频-无音频-输入处理" tabindex="-1">文本、音频、图像、视频(无音频)输入处理 <a class="header-anchor" href="#文本、音频、图像、视频-无音频-输入处理" aria-label="Permalink to &quot;文本、音频、图像、视频(无音频)输入处理&quot;">​</a></h3><ul><li>文本：使用Qwen的tokenizer，采用byte-level的byte-pair编码，词表长度151,643；</li><li>音频：包括常规音频和video音频，重新采样到16kHz的频率，并将原始波形转换到128通道的梅尔频谱图，窗口大小为25ms，hop size为10ms；编码器使用的是Qwen2-Audio，每一帧的音频表示对应了大概40ms的片段；</li><li>图像：采用Qwen2.5VL的ViT，675m参数，能够编码图像和video；视觉编码器使用图像和视频混合训练的方案；</li><li>视频：采用动态帧率对视频进行采样，以适应音频采样率的同时尽可能完整的保留商品信息。为了保持一致性，每个图像被视为两个相同对的帧；</li></ul><h3 id="video-and-tmrope" tabindex="-1">Video and TMRoPE <a class="header-anchor" href="#video-and-tmrope" aria-label="Permalink to &quot;Video and TMRoPE&quot;">​</a></h3><ul><li>提出了音频和视频的时间交错算法，以及TMRoPE位置编码；TMRoPE对多模态输入的三维信息进行编码，即具有绝对时间位置的多模态旋转位置嵌入（M-RoPE）；</li><li>TMRoPE将原来的旋转位置编码解构为三部分：时间、高度和宽度；</li><li>对于文本输入，采用相同的位置ID，使M-RoPE退化为1D-RoPE；</li><li>对于音频输入，也采用相同的位置ID，并且引入绝对时间位置编码，一个时间ID对应40ms；</li><li>对于图像输入，suddenly./SuSS</li></ul>',11)])])}const _=a(n,[["render",o]]);export{m as __pageData,_ as default};
