import{_ as n,c as l,o as s,ag as r,j as t,a as o}from"./chunks/framework.OaOo95RB.js";const u=JSON.parse('{"title":"token熵","description":"","frontmatter":{"title":"token熵","created":"2025-09-07 17:02","updated":"2025-09-07T00:00:00.000Z","origin":"week-35","type":"knowledge","status":"draft","tags":["entropy","token","perplexity","temperature","training-signal"],"links":[]},"headers":[],"relativePath":"10-knowledge/250827-0736-token熵-overview.md","filePath":"10-knowledge/250827-0736-token熵-overview.md"}'),e={name:"10-knowledge/250827-0736-token熵-overview.md"};function g(i,a,_,p,h,$){return s(),l("div",null,[...a[0]||(a[0]=[r('<h2 id="tl-dr-≤3点" tabindex="-1">TL;DR（≤3点） <a class="header-anchor" href="#tl-dr-≤3点" aria-label="Permalink to &quot;TL;DR（≤3点）&quot;">​</a></h2><ul><li><strong>定义</strong>：位置 (t) 的 token 熵 (H_t=-\\sum_v p_t(v)\\log p_t(v))，度量<strong>下一个词分布的不确定性</strong>；与样本惊讶度 (-\\log p_t(y_t)) 不同（它是期望惊讶度）。</li><li><strong>等价式</strong>：若 (p_t(v)=\\exp(\\beta z_{t,v})/Z_t)，则 (H_t=\\log Z_t-\\beta,\\mathbb{E}_{p_t}[z])；<strong>温度升高</strong>（(T=1/\\beta)）→ 熵<strong>单调增</strong>。</li><li><strong>用途</strong>：定位<strong>高不确定位置</strong>（筛选/加权）、温度/采样策略调参、与困惑度 (\\mathrm{PPL}=\\exp(H)) 对应做监控与报警。</li></ul><h2 id="what-是什么" tabindex="-1">What（是什么） <a class="header-anchor" href="#what-是什么" aria-label="Permalink to &quot;What（是什么）&quot;">​</a></h2><ul><li><strong>Token 熵</strong>是<strong>Shannon 熵</strong>在“下一词分布”上的实例：衡量模型对下一个 token 的不确定性。</li><li>取自然对数单位为 <strong>nat</strong>，上界为 (\\log|V_t^{\\text{mask}}|)（在有效词表上的均匀分布），下界为 0（one-hot）。</li></ul><h2 id="why-为什么这么做-何时使用" tabindex="-1">Why（为什么这么做/何时使用） <a class="header-anchor" href="#why-为什么这么做-何时使用" aria-label="Permalink to &quot;Why（为什么这么做/何时使用）&quot;">​</a></h2><ul><li><strong>训练观测</strong>：监控 (H_t) 的统计可发现<strong>过度自信/塌缩</strong>或<strong>过平</strong>的问题（校准）。</li><li><strong>数据与目标选择</strong>：用高熵位置做<strong>掩码/加权</strong>，集中算力在“真正不确定”的 token 上（如 Top-K/Top-ρ）。</li><li><strong>采样与温度</strong>：指导推理期的 <strong>T / top-p / top-k</strong> 调参；温度上升应伴随熵上升，否则提示 logits 量纲或掩码异常。</li></ul><h2 id="how-最小复现配方-≤5步" tabindex="-1">How（最小复现配方，≤5步） <a class="header-anchor" href="#how-最小复现配方-≤5步" aria-label="Permalink to &quot;How（最小复现配方，≤5步）&quot;">​</a></h2><ol><li>从模型获得 logits (z_t)，按温度 (T) 计算 (p_t(v)=\\mathrm{softmax}(z_t/T))（训练监控通常取 (T{=}1)）。</li><li>计算 (H_t=-\\sum_v p_t(v)\\log p_t(v))；<strong>仅在有效词表上</strong>求和并重归一（mask）。</li><li>统计与聚合：对 batch/序列取均值、分位数（如 P50/P90），输出曲线与报警阈值。</li><li>选点：设分位阈值 (\\tau_\\rho)（如 80/90 分位）或序列 Top-K，把 ({t:H_t\\ge\\tau}) 作为<strong>高熵位置</strong>。</li><li>融合训练：在策略/监督目标中对这些位置<strong>加权或掩码</strong>；若仅做“路标”，<strong>对 (H_t) detach</strong>，避免将“更不确定”作为优化方向。</li></ol><h2 id="gotchas-坑点与边界" tabindex="-1">Gotchas（坑点与边界） <a class="header-anchor" href="#gotchas-坑点与边界" aria-label="Permalink to &quot;Gotchas（坑点与边界）&quot;">​</a></h2><ul><li><strong>与 NLL 的区别</strong>：(-\\log p_t(y_t)) 是样本惊讶度；(H_t) 是其<strong>期望</strong>。混用会误读。</li><li><strong>温度与单位</strong>：训练监控建议固定 (T{=}1)；推理温度改变采样熵但<strong>不改变</strong>训练时度量口径。注意 (\\log) 基底（nat vs bit）。</li><li><strong>数值稳定</strong>：用 (\\log\\text{-}\\mathrm{sum}\\exp) 计算 (\\log Z)；掩码后要<strong>重归一</strong>。</li><li><strong>标签平滑/正则</strong>：label smoothing 会抬高目标熵；对照时需注明是否启用。</li><li><strong>回传策略</strong>：把熵作为<strong>正则项</strong>回传与把熵作为<strong>选择/加权信号</strong>（detach）在优化语义上不同，勿混搭。</li></ul><h2 id="raw-notes" tabindex="-1">Raw Notes <a class="header-anchor" href="#raw-notes" aria-label="Permalink to &quot;Raw Notes&quot;">​</a></h2><p>很好，这里把“token 熵”从<strong>严格公式</strong>出发讲清楚，并点明与常见量（交叉熵、负对数似然、困惑度）以及温度/采样的关系。整段<strong>不含伪代码</strong>，只给出可直接落地的数学表达。</p><hr><h1 id="_1-场景与符号" tabindex="-1">1) 场景与符号 <a class="header-anchor" href="#_1-场景与符号" aria-label="Permalink to &quot;1) 场景与符号&quot;">​</a></h1>',14),t("ul",null,[t("li",null,[t("p",null,[o("设第 $t$ 个生成位置的"),t("strong",null,"logits 向量"),o("为 $z_t\\in\\mathbb{R}^V$（$V$=词表大小），温度 $T>0$，$\\beta=1/T$。")])]),t("li",null,[t("p",null,[t("strong",null,"策略分布"),o("（对下一个词元的条件分布）：")]),t("p",null,[o("$$ p_t(v);=;\\pi_\\theta(v\\mid \\text{context}"),t("em",{"t,v":""},"t);=;\\frac{\\exp(\\beta, z"),o(")}{Z_t},\\quad Z_t=\\sum_{j=1}^V \\exp(\\beta, z_{t,j}). $$")])])],-1),r('<blockquote><p>这里的“context$<em>t$”就是输入 + 先前已生成的 token 前缀 $y</em>{&lt;t}$。</p></blockquote><hr><h1 id="_2-token-熵的定义与等价式" tabindex="-1">2) Token 熵的<strong>定义</strong>与等价式 <a class="header-anchor" href="#_2-token-熵的定义与等价式" aria-label="Permalink to &quot;2) Token 熵的**定义**与等价式&quot;">​</a></h1><h2 id="_2-1-标准定义-shannon-熵-位置-t-的分布不确定性" tabindex="-1">2.1 标准定义（Shannon 熵，位置 $t$ 的分布不确定性） <a class="header-anchor" href="#_2-1-标准定义-shannon-熵-位置-t-的分布不确定性" aria-label="Permalink to &quot;2.1 标准定义（Shannon 熵，位置 $t$ 的分布不确定性）&quot;">​</a></h2><p>$$ \\boxed{,H_t ;=; -\\sum_{v=1}^{V} p_t(v),\\log p_t(v),} $$</p><ul><li>若用自然对数 $\\log$，单位为 <strong>nat</strong>；若用 $\\log_2$，单位为 <strong>bit</strong>。</li><li><strong>语义</strong>：这是<strong>分布</strong>的属性，不是被采样出的具体 token 的属性。</li></ul><h2 id="_2-2-与-softmax-能量-形式的恒等式" tabindex="-1">2.2 与 softmax“能量”形式的恒等式 <a class="header-anchor" href="#_2-2-与-softmax-能量-形式的恒等式" aria-label="Permalink to &quot;2.2 与 softmax“能量”形式的恒等式&quot;">​</a></h2><p>把 $p_t$ 写成 $p_t(v)=e^{\\beta z_{t,v}}/Z_t$，可得到：</p>',8),t("p",null,[o("$$ \\boxed{,H_t ;=; \\log Z_t ;-; \\beta;\\mathbb{E}"),t("em",{"t,v":""},"{v\\sim p_t}[,z"),o(",],} $$")],-1),r('<ul><li>这条式子常用于分析温度/梯度（见 §4）。</li></ul><hr><h1 id="_3-与负对数似然-交叉熵-困惑度的区别与联系" tabindex="-1">3) 与<strong>负对数似然/交叉熵/困惑度</strong>的区别与联系 <a class="header-anchor" href="#_3-与负对数似然-交叉熵-困惑度的区别与联系" aria-label="Permalink to &quot;3) 与**负对数似然/交叉熵/困惑度**的区别与联系&quot;">​</a></h1><ol><li><strong>样本的自信息</strong>（被抽到的那个 token $y_t$ 的“惊讶度”）：</li></ol><p>$$ I_t(y_t);=;-\\log p_t(y_t). $$</p><ul><li>这是<strong>标量</strong>，依赖于<strong>采样出的具体 token</strong>。</li></ul><ol start="2"><li><strong>位置分布的熵</strong>（我们这里的 $H_t$）：</li></ol><p>$$ H_t ;=; \\mathbb{E}_{v\\sim p_t}[,I_t(v),];=;-\\sum_v p_t(v)\\log p_t(v). $$</p><ul><li>这是<strong>该位置分布的期望惊讶度</strong>，不依赖某个采样结果。</li></ul><ol start="3"><li><strong>交叉熵</strong>（若有“真分布” $q_t$）：</li></ol><p>$$ \\mathrm{CE}(q_t,p_t);=;-\\sum_v q_t(v)\\log p_t(v);=;H(q_t);+;\\mathrm{KL}(q_t\\Vert p_t). $$</p><ul><li>当“真分布”是<strong>一元分布</strong>（one-hot）时，$\\mathrm{CE}=-\\log p_t(y_t)$ 就是<strong>NLL</strong>。</li></ul><ol start="4"><li><strong>困惑度</strong>（perplexity）：</li></ol><p>$$ \\mathrm{PPL}_t ;=; \\exp!\\big(H_t\\big). $$</p><ul><li>这是一种把熵<strong>指数化</strong>后的直观度量；平均到语料上得到数据级 PPL。</li></ul><hr><h1 id="_4-温度与熵的单调性、可导性" tabindex="-1">4) <strong>温度</strong>与熵的单调性、可导性 <a class="header-anchor" href="#_4-温度与熵的单调性、可导性" aria-label="Permalink to &quot;4) **温度**与熵的单调性、可导性&quot;">​</a></h1><p>用 §2.2 的恒等式，写成 $H_t(\\beta)=\\log Z_t-\\beta,\\mathbb{E}_{p_t}[z]$。可得两条关键性质：</p><ol><li><strong>对温度的单调性</strong> 先对 $\\beta$ 求导，再链式法则到 $T$：</li></ol><p>$$ \\frac{\\partial H_t}{\\partial \\beta} ;=; -,\\beta;\\mathrm{Var}<em>{p_t}(z</em>{t,\\cdot});\\le 0 \\quad\\Longrightarrow\\quad \\boxed{;\\frac{\\partial H_t}{\\partial T} ;=;\\frac{\\mathrm{Var}<em>{p_t}(z</em>{t,\\cdot})}{T^{3}};\\ge 0;} $$</p><ul><li>结论：<strong>温度越高（分布越平），熵越大</strong>；温度 $\\to 0$ 时 $p_t$ 退化为 one-hot，熵 $\\to 0$。</li></ul><ol start="2"><li><strong>对 logits 的梯度</strong>（若你在目标里“真地”对熵求导）</li></ol><p>$$ \\boxed{;\\frac{\\partial H_t}{\\partial z_{t,k}} ;=; -,\\beta^2;p_t(k),\\Big(z_{t,k}-\\mathbb{E}_{p_t}[z]\\Big) ;=; -,\\beta,p_t(k),\\Big(\\log p_t(k)+H_t\\Big);} $$</p><ul><li>一些方法（如“<strong>优势熵塑形</strong>”）选择 <strong>detach</strong> 熵，不让这一梯度回传，从而把熵仅当作“加权信号”使用（见应用文献中的做法）。</li></ul><hr><h1 id="_5-上下界与特殊情况" tabindex="-1">5) <strong>上下界</strong>与<strong>特殊情况</strong> <a class="header-anchor" href="#_5-上下界与特殊情况" aria-label="Permalink to &quot;5) **上下界**与**特殊情况**&quot;">​</a></h1><ul><li><p>设有效词表为 $V_t^{\\text{mask}}$（把 PAD/非法/被屏蔽 token 去掉并重归一），则</p><p>$$ \\boxed{,0;\\le;H_t;\\le;\\log |V_t^{\\text{mask}}|,}. $$</p><ul><li><strong>下界取等</strong>（完全确定）：$p_t$ 为 one-hot。</li><li><strong>上界取等</strong>（完全无偏）：$p_t$ 为在 $V_t^{\\text{mask}}$ 上的<strong>均匀分布</strong>。</li></ul></li><li><p>与<strong>均匀分布</strong>的 KL 关系：</p><p>$$ \\mathrm{KL}\\big(p_t;\\Vert;\\mathcal{U}\\big);=;\\log|V_t^{\\text{mask}}|;-;H_t. $$</p><p>—— 熵越大，离均匀越近；熵越小，分布越“尖”。</p></li></ul><hr><h1 id="_6-批量-序列上的聚合与-高熵位置-的选择" tabindex="-1">6) <strong>批量/序列</strong>上的聚合与“高熵位置”的选择 <a class="header-anchor" href="#_6-批量-序列上的聚合与-高熵位置-的选择" aria-label="Permalink to &quot;6) **批量/序列**上的聚合与“高熵位置”的选择&quot;">​</a></h1><ul><li><p><strong>批内 Top-$\\rho$ 分位阈值</strong> 把一个（微）批全部 token 的 $H_t$ 聚在一起，取分位数阈值 $\\tau_\\rho$：</p><p>$$ \\text{mask}<em>t ;=;\\mathbf{1}\\big[,H_t \\ge \\tau</em>\\rho,\\big]. $$</p><p>—— 在很多工作中用它**筛选“高熵少数 token”**进入策略梯度（$\\rho\\approx 10!\\sim!30%$ 常见）。</p></li><li><p><strong>序列 Top-$K$</strong> 对<strong>单条</strong>序列的 ${H_t}_{t=1}^L$ 取 Top-$K$ 位置作为“<strong>分岔锚点</strong>”，再围绕这些锚点做结构化的探索/局部展开。</p></li></ul><hr><h1 id="_7-与训练目标的正确拼接方式-只述公式-不给代码" tabindex="-1">7) 与训练目标的<strong>正确拼接方式</strong>（只述公式，不给代码） <a class="header-anchor" href="#_7-与训练目标的正确拼接方式-只述公式-不给代码" aria-label="Permalink to &quot;7) 与训练目标的**正确拼接方式**（只述公式，不给代码）&quot;">​</a></h1><ul><li><p><strong>掩码式（只训高熵）</strong> 在 token-level 的策略目标（如 PPO/GRPO/DAPO 的 clipped surrogate）里，将<strong>优势/损失</strong>乘上 $\\text{mask}_t$：</p><p>$$ \\max_\\theta;\\mathbb{E}\\big[;\\text{mask}<em>t\\cdot \\min\\big(r_t(\\theta)A_t,;\\mathrm{clip}(r_t(\\theta),1-\\varepsilon</em>{!l},1+\\varepsilon_{!h})A_t\\big);\\big]. $$</p></li><li><p><strong>优势“熵塑形”（不回传熵梯度）</strong></p><p>$$ \\tilde A_t ;=; A_t ;+; \\min!\\Big(\\alpha,H_t^{\\text{detach}},;\\frac{|A_t|}{\\kappa}\\Big), \\qquad \\max_\\theta;\\mathbb{E}\\big[\\min(r_t\\tilde A_t,;\\mathrm{clip}(\\cdot)\\tilde A_t)\\big]. $$</p><p>—— 只改变<strong>步长幅度/优先级</strong>，不改变原有优化方向。</p></li><li><p><strong>结构化探索（熵作锚点评分）</strong> 用 ${H_t}$ 选定锚点，再从这些前缀做<strong>局部 rollouts</strong>估计中间价值 $V(\\text{prefix})$，把它变成<strong>更密集的优势</strong>（可加自适应缩放），再进常规策略更新。</p></li></ul><hr><h1 id="_8-常见容易混淆的点-一口气厘清" tabindex="-1">8) 常见容易混淆的点（一口气厘清） <a class="header-anchor" href="#_8-常见容易混淆的点-一口气厘清" aria-label="Permalink to &quot;8) 常见容易混淆的点（一口气厘清）&quot;">​</a></h1><ol><li><p><strong>“token 熵” vs “负对数似然”</strong> $-\\log p_t(y_t)$ 是<strong>样本</strong>的惊讶度；$H_t$ 是<strong>分布</strong>在该位置的期望惊讶度。两者不是同一量，但 $\\mathbb{E}_{y_t\\sim p_t}[-\\log p_t(y_t)]=H_t$。</p></li><li><p><strong>“加熵正则” vs “用熵做路标”</strong> 在损失里加 $+\\beta\\sum H_t$ 会<strong>反向传导“变得更不确定”</strong>；而“优势熵塑形/高熵掩码”只是<strong>用 $H_t$</strong> 来<strong>决定更新力度或范围</strong>，可以选择 <strong>detach</strong> 避免改变优化目标。</p></li><li><p><strong>温度/采样与熵</strong> 训练时 $H_t$ 通常由 $T=1$ 的分布计算；推理期若调采样温度/Top-p/Top-k，会<strong>改变采样分布的熵</strong>，但<strong>不改变训练时度量的 $H_t$</strong>（除非你也显式改训练温度）。</p></li></ol><hr><blockquote><p>结论（一句话）： <strong>token 熵</strong>就是<strong>该位置 softmax 分布</strong>的不确定性 $H_t=-\\sum p\\log p$；用 softmax 形式可写成 $H_t=\\log Z_t-\\beta,\\mathbb{E}_p[z]$。它<strong>不等同</strong>于某个采样 token 的 $-\\log p$，而是<strong>其期望</strong>。在训练中，熵既可<strong>作为选择/加权信号</strong>（不回传），也可（少见地）<strong>作为正则项</strong>回传——两者数学与优化含义不同，务必区分。</p></blockquote>',38)])])}const b=n(e,[["render",g]]);export{u as __pageData,b as default};
