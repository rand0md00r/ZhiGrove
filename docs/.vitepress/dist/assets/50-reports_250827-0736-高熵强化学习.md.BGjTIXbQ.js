import{_ as n,c as a,o as i,ag as o,j as t,a as r}from"./chunks/framework.OaOo95RB.js";const v=JSON.parse('{"title":"高熵强化学习","description":"","frontmatter":{"title":"高熵强化学习","created":"2025-09-07 17:02","updated":"2025-09-07T00:00:00.000Z","origin":"week-35","type":"report","status":"draft","tags":[],"links":[]},"headers":[],"relativePath":"50-reports/250827-0736-高熵强化学习.md","filePath":"50-reports/250827-0736-高熵强化学习.md"}'),l={name:"50-reports/250827-0736-高熵强化学习.md"};function s(g,e,p,h,d,f){return i(),a("div",null,[...e[0]||(e[0]=[o('<h2 id="raw-notes" tabindex="-1">Raw Notes <a class="header-anchor" href="#raw-notes" aria-label="Permalink to &quot;Raw Notes&quot;">​</a></h2><p>下面是一份围绕“三篇论文 + 相关工作”的精炼调研报告，重点交代<strong>token 熵的精确定义与实现细节</strong>，并将各路线的目标函数、训练管线与优缺点对照起来，便于直接落地到你的强化学习后训练流程中。</p><h1 id="一、核心概念-token-熵是什么、为何要在-rl-后训练中关注它" tabindex="-1">一、核心概念：token 熵是什么、为何要在 RL 后训练中关注它 <a class="header-anchor" href="#一、核心概念-token-熵是什么、为何要在-rl-后训练中关注它" aria-label="Permalink to &quot;一、核心概念：token 熵是什么、为何要在 RL 后训练中关注它&quot;">​</a></h1><p><strong>token 级生成熵</strong>（generation entropy）刻画模型在<strong>当前位置</strong>对下一个词元的犹豫程度。对输入 $q$ 与已生成前缀 $o_{&lt;t}$，当前策略 $\\pi_\\theta$ 的词表分布为</p><p>$$ p_t = \\pi_\\theta(\\cdot \\mid q, o_{&lt;t})=\\mathrm{Softmax}(z_t), $$</p><p>其<strong>token 熵</strong>定义为</p><p>$$ H_t ;=; -\\sum_{j=1}^{V} p_{t,j},\\log p_{t,j}. $$</p><p>该定义直接来自论文(等式(1))，强调“<strong>熵属于位置 $t$ 的分布</strong>而不是被采样出来的具体 token 本身”。实现上就是对当前前向得到的 logits $z_t$ 做一次 softmax + 向量级的 $-(p\\cdot\\log p)$ 规约。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</p><p>关注 token 熵的动机：在 CoT 推理中，<strong>大多数 token 是低熵的“续写/拼写”</strong>，而<strong>少数高熵 token 是“分岔点/承上启下”</strong>，决定思路转折与步骤衔接。RL 的收益几乎都发生在这些“高熵分岔点”上。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</p><h1 id="二、三篇论文的要点与它们-怎样用到熵" tabindex="-1">二、三篇论文的要点与它们“怎样用到熵” <a class="header-anchor" href="#二、三篇论文的要点与它们-怎样用到熵" aria-label="Permalink to &quot;二、三篇论文的要点与它们“怎样用到熵”&quot;">​</a></h1><h2 id="_1-《beyond-the-80-20-rule-high-entropy-minority-tokens-》-高熵少数派" tabindex="-1">1) 《Beyond the 80/20 Rule: High-Entropy Minority Tokens…》(高熵少数派) <a class="header-anchor" href="#_1-《beyond-the-80-20-rule-high-entropy-minority-tokens-》-高熵少数派" aria-label="Permalink to &quot;1) 《Beyond the 80/20 Rule: High-Entropy Minority Tokens…》(高熵少数派)&quot;">​</a></h2><p><strong>思想</strong>：只在<strong>高熵 token</strong>处更新策略梯度，<strong>屏蔽底部 80% 低熵 token</strong> 的梯度——“用 20% token 训练也不掉点，甚至更好”。 <strong>目标函数</strong>（DAPO 框架下的改动，批内 Top-ρ 选取）： 对一个训练批 $B$，求最大化</p>',12),t("p",null,[r("$$ J^{\\text{HighEnt}}"),t("em",{i:""},"B(\\theta) =\\mathbb{E}!\\left[ \\frac{1}{\\sum_i |o_i|}\\sum"),r("\\sum_{t=1}^{|o_i|} \\mathbf{1}[H^i_t \\ge \\tau^{B}"),t("em",null,"\\rho]\\cdot \\min!\\big(r^i_t(\\theta)\\hat A^i_t,,\\mathrm{clip}(r^i_t(\\theta), 1-\\epsilon"),r("{\\text{low}}, 1+\\epsilon_{\\text{high}})\\hat A^i_t\\big) \\right], $$")],-1),o('<p>其中 $\\tau^{B}<em>\\rho$ 是<strong>在该（微）批所有 token 的熵上取 Top-ρ 分位的阈值</strong>，仅保留满足 $H^i_t \\ge \\tau^{B}</em>\\rho$ 的 token 进入损失与反传；$\\epsilon_{\\text{high}}$ 采用 <strong>Clip-Higher</strong>（上界放宽，如 0.28）以鼓励探索。实现上只需在构造优势时加一层 <strong>indicator mask</strong>。文中常用 $\\rho=20%$。该做法在 Qwen3-32B/14B 上显著提升，在 8B 上持平。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</p><p><strong>关键实现细节（必须点）</strong></p><ul><li><strong>熵的计算</strong>：用<strong>当前策略</strong>的前向 logits 计算 $H_t$，不需要温度缩放（默认 $T=1$）。（见等式(1)与 5.1 节）(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>分位阈值</strong>：在**（微）批维度**上把所有 token 的 $H_t$ 拼起来求分位数 $\\tau^{B}_\\rho$，得到布尔 mask（True 表示 Top-ρ 的高熵 token）。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>只改动 PG 分量</strong>：把 $\\mathbf{1}[\\cdot]$ 乘到优势上即可，其余（如 clip-higher、动态采样、overlong 奖励等）与 DAPO 配方一致；<strong>不引入 KL 或额外 entropy bonus</strong>。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>经验观察</strong>：$\\rho$ 在 10–50% 区间相对鲁棒，但用 100% 会恶化（因为把大量低熵续写 token 也纳入 PG，等价于“稀释”了有效学习信号）。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</li></ul><h2 id="_2-《reasoning-with-exploration-an-entropy-perspective》" tabindex="-1">2) 《Reasoning with Exploration: An Entropy Perspective》 <a class="header-anchor" href="#_2-《reasoning-with-exploration-an-entropy-perspective》" aria-label="Permalink to &quot;2) 《Reasoning with Exploration: An Entropy Perspective》&quot;">​</a></h2><p><strong>思想</strong>：不是“掩码掉谁”，而是<strong>把熵直接注入优势</strong>，鼓励处于高不确定处的<strong>更深/更长</strong>探索；与一般“熵正则（增大不确定性）”不同，本工作通过<strong>优势塑形</strong>来促使模型在关键处更有把握。 <strong>做法（“一行代码”）</strong>： 计算常规优势 $\\text{adv}$（PPO 或 GRPO），再加一项<strong>截断且</strong> <strong>detached</strong> 的熵项：</p><p>$$ \\tilde{\\text{adv}}_t = \\text{adv}_t ;+; \\min!\\big(\\alpha\\cdot H_t^{\\text{detach}},, |\\text{adv}_t|/\\kappa\\big), $$</p><p>再用 $\\tilde{\\text{adv}}$ 进入标准 PPO/GRPO 的 clipped-surrogate；这样<strong>不改变梯度方向</strong>（熵项不反传），只是放大高熵位置的步长，并且用 $\\kappa$ 防止过度放大/翻转符号。论文明确给出 PyTorch 伪“一行”插入点（veRL 框架的 dp_actor）。(<a href="https://www.ar5iv.org/pdf/2506.14758v2" title="[2506.14758] Reasoning with Exploration: An Entropy Perspective" target="_blank" rel="noreferrer">ar5iv</a>)</p><p><strong>关键实现细节（必须点）</strong></p><ul><li><strong>$\\alpha,\\kappa$ 两个超参</strong>：$\\alpha$ 控制熵项强度；$\\kappa$ 用于截断，保证 $\\alpha H_t \\le |\\text{adv}_t|/\\kappa$ 时不会反向或主导更新。(<a href="https://www.ar5iv.org/pdf/2506.14758v2" title="[2506.14758] Reasoning with Exploration: An Entropy Perspective" target="_blank" rel="noreferrer">ar5iv</a>)</li><li><strong>为何要 detach</strong>：避免把“增大熵”作为优化目标；这里是<strong>用熵做路标</strong>而非 regularizer。随着训练置信度提升，$H_t$ 下降，熵加成会<strong>自衰减</strong>，从而避免后期过探索。(<a href="https://www.ar5iv.org/pdf/2506.14758v2" title="[2506.14758] Reasoning with Exploration: An Entropy Perspective" target="_blank" rel="noreferrer">ar5iv</a>)</li><li><strong>与 GRPO/PPO 的兼容</strong>：仅替换优势，剩下的剪切比、KL（若有）等策略不变；理论与实现都保持最小侵入。(<a href="https://www.ar5iv.org/pdf/2506.14758v2" title="[2506.14758] Reasoning with Exploration: An Entropy Perspective" target="_blank" rel="noreferrer">ar5iv</a>)</li></ul><h2 id="_3-《fr3e-first-return-entropy-eliciting-explore》" tabindex="-1">3) 《FR3E: First Return, Entropy-Eliciting Explore》 <a class="header-anchor" href="#_3-《fr3e-first-return-entropy-eliciting-explore》" aria-label="Permalink to &quot;3) 《FR3E: First Return, Entropy-Eliciting Explore》&quot;">​</a></h2><p><strong>思想</strong>：把高熵位置当作<strong>锚点</strong>，按“<strong>先回到正确轨迹（First Return）—再从高熵处</strong>做<strong>定向展开</strong>（Explore）”的两阶段结构化探索，构造<strong>局部中间反馈</strong>，改进 credit assignment 与探索稳定性。(<a href="https://arxiv.org/html/2507.07017v1" title="First Return, Entropy-Eliciting Explore" target="_blank" rel="noreferrer">arXiv</a>)</p><p><strong>做法</strong>：</p><ol><li>用当前策略生成<strong>基准轨迹</strong>，沿途计算 $H_t$；选择<strong>全局 Top-K 高熵位置</strong>作为<strong>分块边界</strong>，形成“语义块/中间状态”。</li><li>从这些锚点做<strong>局部 rollouts</strong>，用可验证的结果给出<strong>经验价值</strong> $V(\\text{prefix})$，再配以<strong>自适应优势调制系数</strong> $\\gamma\\big(\\Delta V\\big)$ 来稳住学习（进步小则放大、进步大则收敛时缩小）。(<a href="https://arxiv.org/html/2507.07017v1" title="First Return, Entropy-Eliciting Explore" target="_blank" rel="noreferrer">arXiv</a>)</li></ol><p><strong>关键实现细节（必须点）</strong></p><ul><li><strong>熵的用途</strong>：只用于<strong>定位高不确定决策点</strong>（Top-K）及<strong>分块</strong>；损失里不直接加熵项。等式中给出分布 $p_t$ 与熵 $H_t$ 的标准定义与<strong>Top-K 选点</strong>规则。(<a href="https://arxiv.org/html/2507.07017v1" title="First Return, Entropy-Eliciting Explore" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>计算开销</strong>：相较普通 RLVR，多了<strong>从若干锚点起的局部扩展</strong>与评估；但这些扩展是“定向”的，采样效率高于盲目的全局探索。(<a href="https://arxiv.org/html/2507.07017v1" title="First Return, Entropy-Eliciting Explore" target="_blank" rel="noreferrer">arXiv</a>)</li></ul><h1 id="三、与三篇论文相关的其它路线-聚焦-熵-探索-credit-assignment-配方" tabindex="-1">三、与三篇论文相关的其它路线（聚焦“熵/探索/credit assignment/配方”） <a class="header-anchor" href="#三、与三篇论文相关的其它路线-聚焦-熵-探索-credit-assignment-配方" aria-label="Permalink to &quot;三、与三篇论文相关的其它路线（聚焦“熵/探索/credit assignment/配方”）&quot;">​</a></h1><ul><li><strong>Clip-Higher 与 DAPO 配方</strong>：上界放宽（如 $1+\\epsilon_{\\text{high}}=1.28$）能在不破坏稳定性的前提下<strong>更敢于把低概率“探索 token”抬起来</strong>，是近来 RLVR 成功的关键配方之一（本文 1) 也沿用）。DAPO 公开报告详述了动机与实现。(<a href="https://arxiv.org/pdf/2503.14476?utm_source=chatgpt.com" title="DAPO: An Open-Source LLM Reinforcement Learning ..." target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>GRPO 及其实现要点</strong>：不训练 critic，用同一问题的多样本平均奖励作基线，token-level PPO 损失+可选 KL；veRL 文档提供了工程接口。(<a href="https://verl.readthedocs.io/en/latest/algo/grpo.html?utm_source=chatgpt.com" title="Group Relative Policy Optimization (GRPO) - verl documentation" target="_blank" rel="noreferrer">Verl</a>)</li><li><strong>熵可控的 DPO（H-DPO）</strong>：在<strong>偏好优化</strong>（无显式 RL）里，通过修改 DPO 的<strong>反向 KL 正则</strong>来<strong>控制策略熵/锐度</strong>，与上面“在 RL 中用熵”形成互补。实现改动仅在损失计算处，实验显示对数学任务的 pass@k 有提升。(<a href="https://arxiv.org/abs/2411.07595" title="[2411.07595] Entropy Controllable Direct Preference Optimization" target="_blank" rel="noreferrer">arXiv</a>, <a href="https://ar5iv.org/pdf/2411.07595" title="[2411.07595] Entropy Controllable Direct Preference Optimization" target="_blank" rel="noreferrer">ar5iv</a>)</li><li><strong>ETPO（Entropy-Regularized Token-level Policy Optimization）</strong>：把<strong>软 Bellman</strong>与<strong>策略更新</strong>都下沉到<strong>token 粒度</strong>，从理论到算法完整地把“熵正则”引入到 token 级 RL。适合交互式环境（如代码代理），而非典型的离线 RLVR，但在“<strong>token 级 credit assignment + 熵</strong>”上与三篇论文一脉相承。(<a href="https://arxiv.org/html/2402.06700v1" title="Entropy-Regularized Token-Level Policy Optimization for Large Language Models" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>过程奖励与中间反馈</strong>（与 FR3E 目标相近）：VinePPO（无价值网络、MC 估计中间价值）、PRIME（只用结果标签学<strong>隐式过程奖励</strong>，得到稠密过程信号）、S-GRPO（串行组+衰减奖励，引导“更早更好”的思考退出），都是<strong>加强中间监督/credit assignment</strong>的代表，与“熵选点/结构化探索”可以组合。(<a href="https://arxiv.org/html/2410.01679v2?utm_source=chatgpt.com" title="VinePPO: Refining Credit Assignment in RL Training of LLMs" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>拒绝采样基线（RAFT / Reinforce-Rej）</strong>：近期工作显示“仅用正样本做再训练”的简单基线已能逼近乃至超过部分 RL 配方，提示我们应谨慎评估熵驱动探索的<strong>相对收益</strong>。(<a href="https://arxiv.org/html/2504.11343v1?utm_source=chatgpt.com" title="A Minimalist Approach to LLM Reasoning: from Rejection ..." target="_blank" rel="noreferrer">arXiv</a>)</li></ul><h1 id="四、token-熵——工程侧-怎么做才对" tabindex="-1">四、token 熵——工程侧“怎么做才对” <a class="header-anchor" href="#四、token-熵——工程侧-怎么做才对" aria-label="Permalink to &quot;四、token 熵——工程侧“怎么做才对”&quot;">​</a></h1><blockquote><p>下面把“算 $H_t$”“怎么选 Top-ρ/Top-K”“放到损失/优势里”的实现细节一次讲清。</p></blockquote><h2 id="a-计算-h-t-一行向量规约" tabindex="-1">A. 计算 $H_t$：一行向量规约 <a class="header-anchor" href="#a-计算-h-t-一行向量规约" aria-label="Permalink to &quot;A. 计算 $H_t$：一行向量规约&quot;">​</a></h2><ul><li><strong>前向</strong>得到 logits $z_t$（你本来为计算 log-prob/比值 $r_t$ 就会算）。</li><li><strong>Softmax</strong> 得 $p_t$，常规训练温度 $T=1$（除非你显式做温度缩放）。</li><li><strong>规约</strong>：<code>H_t = -(p_t * (p_t.log())).sum(dim=-1)</code>；注意对 <strong>padding/EOS</strong> 做 mask，不把它们纳入统计/更新。该定义与三篇论文一致（见 1) 的式(1)，2) 的式(4)，3) 的式(5)）。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>, <a href="https://www.ar5iv.org/pdf/2506.14758v2" title="[2506.14758] Reasoning with Exploration: An Entropy Perspective" target="_blank" rel="noreferrer">ar5iv</a>)</li></ul><h2 id="b-top-ρ-批内阈值-top-k-序列级-选点" tabindex="-1">B. Top-ρ（批内阈值）/Top-K（序列级）选点 <a class="header-anchor" href="#b-top-ρ-批内阈值-top-k-序列级-选点" aria-label="Permalink to &quot;B. Top-ρ（批内阈值）/Top-K（序列级）选点&quot;">​</a></h2><ul><li><strong>批内 Top-ρ（高熵少数派）</strong>：把<strong>一个（微）批</strong>中所有 token 的 $H_t$ 拉平成一维，做 <code>quantile(1-ρ)</code> 得 $\\tau^B_\\rho$，得到布尔 mask：<code>mask = (H &gt;= tau)；adv *= mask</code>。论文明确写作 $\\tau^B_\\rho$，是<strong>在批级别</strong>求阈值而非逐序列。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>序列级 Top-K（FR3E）</strong>：对<strong>单条轨迹</strong>取全局 Top-K 的位置（“高不确定决策点”）作为<strong>锚点/分块边界</strong>，后续仅在这些锚点上做局部 rollouts。(<a href="https://arxiv.org/html/2507.07017v1" title="First Return, Entropy-Eliciting Explore" target="_blank" rel="noreferrer">arXiv</a>)</li></ul><h2 id="c-把熵-接-进优化" tabindex="-1">C. 把熵“接”进优化 <a class="header-anchor" href="#c-把熵-接-进优化" aria-label="Permalink to &quot;C. 把熵“接”进优化&quot;">​</a></h2><ul><li><p><strong>“掩码式”接法（高熵少数派）</strong>：在 token-level PPO/DAPO 损失里，把优势乘上 <code>mask</code>，实现“<strong>只在高熵处学</strong>”。<strong>其余配方不变</strong>（clip-higher/采样/overlong 奖励等保持一致；论文实验中不使用 KL/entropy bonus）。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</p></li><li><p><strong>“优势塑形”接法（熵视角）</strong>：</p><p>$$ \\tilde{\\text{adv}}=\\text{adv}+\\min(\\alpha\\cdot H^{\\text{detach}},, |\\text{adv}|/\\kappa), $$</p><p>其中 <code>H.detach()</code> <strong>不回传梯度</strong>，只改变步长大小与优先级；<code>min</code> 截断确保不会翻转 $\\text{adv}$ 的符号。对应 veRL 的实现点就是<strong>在计算完 adv 后加一行</strong>再入损失。(<a href="https://www.ar5iv.org/pdf/2506.14758v2" title="[2506.14758] Reasoning with Exploration: An Entropy Perspective" target="_blank" rel="noreferrer">ar5iv</a>)</p></li><li><p><strong>“结构化探索”接法（FR3E）</strong>：熵只用于<strong>定位锚点</strong>；真正进入损失的是锚点处展开得到的<strong>经验价值/优势</strong>，配上<strong>自适应缩放因子</strong>控制学习强度，进而稳定训练而不过度早收敛。(<a href="https://arxiv.org/html/2507.07017v1" title="First Return, Entropy-Eliciting Explore" target="_blank" rel="noreferrer">arXiv</a>)</p></li></ul><h2 id="d-与常见正则-配方的交互" tabindex="-1">D. 与常见正则/配方的交互 <a class="header-anchor" href="#d-与常见正则-配方的交互" aria-label="Permalink to &quot;D. 与常见正则/配方的交互&quot;">​</a></h2><ul><li><strong>不要把“熵优势塑形”与“显式熵正则”混为一谈</strong>：前者是<strong>不回传</strong>的路标信号，后者会直接驱动“变得更不确定”。论文指出后者在 CoT 中会<strong>伤害低熵多数 token 的确定性</strong>，不如 clip-higher 这类“更关注高熵少数”的做法。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>与 KL 正则</strong>：1) 中实验不加 KL 亦可稳定；2) 若保留 KL，建议先只引入<strong>优势塑形</strong>或<strong>Top-ρ 掩码</strong>中的一个，逐步网格 $\\alpha,\\kappa,\\rho$；3) 与 clip-higher 的配合通常更自然（高比值 token 往往也是高熵 token）。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</li></ul><h1 id="五、对比与实践建议" tabindex="-1">五、对比与实践建议 <a class="header-anchor" href="#五、对比与实践建议" aria-label="Permalink to &quot;五、对比与实践建议&quot;">​</a></h1><table tabindex="0"><thead><tr><th>路线</th><th>用熵做什么</th><th>代价</th><th>何时更合适</th><th>主要风险/调参点</th></tr></thead><tbody><tr><td><strong>高熵少数派（Top-ρ）</strong></td><td>只在高熵 token 反传</td><td>极低（仅一层 mask）</td><td>你已用 DAPO/GRPO，想<strong>减噪提效</strong></td><td>ρ 过大→“稀释”；过小→样本不足。建议 10–30% 起试。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</td></tr><tr><td><strong>熵优势塑形</strong></td><td>放大高熵处的优势（detach + 截断）</td><td>极低（一行）</td><td>想保留<strong>全部 token</strong>训练又突出“分岔点”</td><td>$\\alpha,\\kappa$ 需网格；$\\kappa$ 太小会过放大。(<a href="https://www.ar5iv.org/pdf/2506.14758v2" title="[2506.14758] Reasoning with Exploration: An Entropy Perspective" target="_blank" rel="noreferrer">ar5iv</a>)</td></tr><tr><td><strong>FR3E</strong></td><td>高熵定位锚点 + 定向 rollouts</td><td>中等（局部展开）</td><td>更关注<strong>中间反馈/credit assignment</strong></td><td>K 过大开销上升；需要设计锚点数量与展开步数。(<a href="https://arxiv.org/html/2507.07017v1" title="First Return, Entropy-Eliciting Explore" target="_blank" rel="noreferrer">arXiv</a>)</td></tr><tr><td><strong>H-DPO</strong></td><td>在 DPO 中直接控熵/锐度</td><td>低（改损失）</td><td>偏好优化场景或想少调 RL 环节</td><td>与 RLVR 不同范式；正则强度需小心。(<a href="https://ar5iv.org/pdf/2411.07595" title="[2411.07595] Entropy Controllable Direct Preference Optimization" target="_blank" rel="noreferrer">ar5iv</a>)</td></tr><tr><td><strong>ETPO</strong></td><td>token-级软 Bellman + 熵正则</td><td>较高（算法更重）</td><td>交互式/在线环境</td><td>工程复杂、与 RLVR 评测口径不同。(<a href="https://arxiv.org/html/2402.06700v1" title="Entropy-Regularized Token-Level Policy Optimization for Large Language Models" target="_blank" rel="noreferrer">arXiv</a>)</td></tr></tbody></table><h1 id="六、最小可落地清单-工程角度" tabindex="-1">六、最小可落地清单（工程角度） <a class="header-anchor" href="#六、最小可落地清单-工程角度" aria-label="Permalink to &quot;六、最小可落地清单（工程角度）&quot;">​</a></h1><ol><li><strong>度量熵</strong>：在生成/训练前向里，取 <code>p = softmax(logits)</code>，<code>H = -(p * log(p)).sum(-1)</code>；屏蔽 PAD/EOS。三篇论文都使用该实现。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>, <a href="https://www.ar5iv.org/pdf/2506.14758v2" title="[2506.14758] Reasoning with Exploration: An Entropy Perspective" target="_blank" rel="noreferrer">ar5iv</a>)</li><li><strong>批内 Top-ρ 掩码</strong>（若采用高熵少数派）：把批内所有 token 的 <code>H</code> 拼起来做 <code>quantile</code> 得阈值 $\\tau^B_\\rho$，生成 <code>mask</code> 乘到优势或损失上。ρ=0.2 常作为强基线。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>优势塑形“一行”</strong>（若采用熵视角）：<code>adv += min(alpha * H.detach(), adv.abs()/kappa)</code>；从 $\\alpha\\in[0.05,0.2]$、$\\kappa\\in[2,8]$ 小步扫描。(<a href="https://www.ar5iv.org/pdf/2506.14758v2" title="[2506.14758] Reasoning with Exploration: An Entropy Perspective" target="_blank" rel="noreferrer">ar5iv</a>)</li><li><strong>结构化探索（FR3E）</strong>：对每条样本<strong>先</strong>生成一条“基准正确轨迹”（或较优轨迹），按 Top-K $H_t$ 定锚、分块；再从各锚点做若干次<strong>局部</strong> rollouts 评估并计算经验价值，最后以自适应优势调制进入策略更新。(<a href="https://arxiv.org/html/2507.07017v1" title="First Return, Entropy-Eliciting Explore" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>与配方的搭配</strong>：Clip-Higher（如 $\\epsilon_{\\text{high}}\\approx0.28$）+ token-level PPO/GRPO 是当前社区默认强基线；在此之上引入 <strong>(2) 或 (3)</strong> 的熵机制，通常更稳。(<a href="https://arxiv.org/pdf/2503.14476?utm_source=chatgpt.com" title="DAPO: An Open-Source LLM Reinforcement Learning ..." target="_blank" rel="noreferrer">arXiv</a>, <a href="https://verl.readthedocs.io/en/latest/algo/grpo.html?utm_source=chatgpt.com" title="Group Relative Policy Optimization (GRPO) - verl documentation" target="_blank" rel="noreferrer">Verl</a>)</li></ol><hr><h1 id="参考与延伸阅读-精选" tabindex="-1">参考与延伸阅读（精选） <a class="header-anchor" href="#参考与延伸阅读-精选" aria-label="Permalink to &quot;参考与延伸阅读（精选）&quot;">​</a></h1><ul><li><strong>高熵少数派（Top-ρ 掩码）</strong>：熵定义、Top-ρ 阈值与目标式(6)，及“不加 KL/entropy bonus”的配方与结果。(<a href="https://arxiv.org/pdf/2506.01939" title="Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>熵优势塑形</strong>：熵定义(式(4))、优势塑形(式(5)(6))、“一行实现”与理论动机。(<a href="https://www.ar5iv.org/pdf/2506.14758v2" title="[2506.14758] Reasoning with Exploration: An Entropy Perspective" target="_blank" rel="noreferrer">ar5iv</a>)</li><li><strong>FR3E</strong>：熵计算与 Top-K 选点、分块/中间状态构造、锚点局部 rollouts 与自适应优势调制。(<a href="https://arxiv.org/html/2507.07017v1" title="First Return, Entropy-Eliciting Explore" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>Clip-Higher / DAPO</strong>：为何放宽上剪切能鼓励探索 token；DAPO 全配方与公开报告。(<a href="https://arxiv.org/pdf/2503.14476?utm_source=chatgpt.com" title="DAPO: An Open-Source LLM Reinforcement Learning ..." target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>GRPO 实现简介</strong>（veRL 文档）：组采样、群内相对优势与无 critic 策略。(<a href="https://verl.readthedocs.io/en/latest/algo/grpo.html?utm_source=chatgpt.com" title="Group Relative Policy Optimization (GRPO) - verl documentation" target="_blank" rel="noreferrer">Verl</a>)</li><li><strong>H-DPO</strong>：在偏好优化中显式<strong>控制策略熵</strong>（非 RLVR）。(<a href="https://arxiv.org/abs/2411.07595" title="[2411.07595] Entropy Controllable Direct Preference Optimization" target="_blank" rel="noreferrer">arXiv</a>, <a href="https://ar5iv.org/pdf/2411.07595" title="[2411.07595] Entropy Controllable Direct Preference Optimization" target="_blank" rel="noreferrer">ar5iv</a>)</li><li><strong>ETPO</strong>：token-级软 Bellman + 熵正则的理论-实践框架。(<a href="https://arxiv.org/html/2402.06700v1" title="Entropy-Regularized Token-Level Policy Optimization for Large Language Models" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>VinePPO / PRIME / S-GRPO</strong>：改进 credit assignment 与中间奖励的代表工作，可与“熵锚点/Top-ρ/优势塑形”互补。(<a href="https://arxiv.org/html/2410.01679v2?utm_source=chatgpt.com" title="VinePPO: Refining Credit Assignment in RL Training of LLMs" target="_blank" rel="noreferrer">arXiv</a>)</li><li><strong>拒绝采样基线（RAFT / Reinforce-Rej）</strong>：提醒评测要对齐与做充足消融。(<a href="https://arxiv.org/html/2504.11343v1?utm_source=chatgpt.com" title="A Minimalist Approach to LLM Reasoning: from Rejection ..." target="_blank" rel="noreferrer">arXiv</a>)</li></ul><blockquote><p>如果你要把这些机制接到现有 veRL/GRPO++ 训练脚本里： <strong>Top-ρ</strong> 属于“优势前的掩码”；<strong>熵优势塑形</strong>属于“计算完优势后一行相加（detach+截断）”；<strong>FR3E</strong> 需在采样器里加入“按高熵锚点做局部展开与评估”的流程，随后把得到的“锚点价值/优势”写回策略更新。以上三者均不要求你改动模型架构，只是调整<strong>优势/采样/反传范围</strong>与<strong>采样策略</strong>。</p></blockquote>',35)])])}const k=n(l,[["render",s]]);export{v as __pageData,k as default};
