import{_ as r,c as o,o as a,ag as l}from"./chunks/framework.CQuhCYrb.js";const _=JSON.parse('{"title":"困惑度","description":"","frontmatter":{"title":"困惑度","created":"2025-09-07 17:02","updated":"2025-09-07T00:00:00.000Z","origin":"week-35","type":"knowledge","status":"draft","tags":["信息论","困惑度","perplexity","语言模型","交叉熵"],"links":[]},"headers":[],"relativePath":"10-knowledge/250829-0735-困惑度-overview.md","filePath":"10-knowledge/250829-0735-困惑度-overview.md"}'),n={name:"10-knowledge/250829-0735-困惑度-overview.md"};function s(e,t,i,g,h,$){return a(),o("div",null,[...t[0]||(t[0]=[l('<h2 id="tl-dr-≤3点" tabindex="-1">TL;DR（≤3点） <a class="header-anchor" href="#tl-dr-≤3点" aria-label="Permalink to &quot;TL;DR（≤3点）&quot;">​</a></h2><ul><li>定义：<strong>困惑度（PPL） = 指标分布的平均负对数似然的指数</strong>；用自然对数时 $\\mathrm{PPL}=e^{\\bar{H}}$，用 $\\log_2$ 时 $\\mathrm{PPL}=2^{\\bar{H}_{\\text{bits}}}$。</li><li>直觉：<strong>每一步等效面对的“等可能选项个数”</strong>；越低越好，理想下界为 1。</li><li>区分：<strong>数据级 PPL</strong>（评测真实序列） vs <strong>单步 PPL</strong>（$e^{H_t}$，预测分布熵的指数化）。</li></ul><h2 id="what-是什么" tabindex="-1">What（是什么） <a class="header-anchor" href="#what-是什么" aria-label="Permalink to &quot;What（是什么）&quot;">​</a></h2><ul><li>对标注序列 $y_{1:T}$，模型给出条件概率 $p(y_t\\mid y_{&lt;t})$。<br><strong>平均负对数似然（交叉熵）</strong>：$\\displaystyle \\bar{H}=-\\frac{1}{T}\\sum_{t=1}^T \\log p(y_t\\mid y_{&lt;t})$<br><strong>困惑度</strong>（自然对数）：$\\displaystyle \\boxed{\\mathrm{PPL}=e^{\\bar{H}}}$</li><li>等价式（几何平均逆概率）： $$ \\mathrm{PPL} =\\exp!\\Big(-\\tfrac{1}{T}\\sum_{t}\\log p(y_t\\mid y_{&lt;t})\\Big) =\\Big(\\prod_{t=1}^{T}\\tfrac{1}{p(y_t\\mid y_{&lt;t})}\\Big)^{!1/T}. $$</li><li><strong>单步困惑度</strong>：若 $p_t(\\cdot)$ 是第 $t$ 步的预测分布，Shannon 熵 $H_t=-\\sum_v p_t(v)\\log p_t(v)$，则<br> $\\displaystyle \\mathrm{PPL}_t=\\exp(H_t)$。</li></ul><h2 id="why-为什么这么做-何时使用" tabindex="-1">Why（为什么这么做/何时使用） <a class="header-anchor" href="#why-为什么这么做-何时使用" aria-label="Permalink to &quot;Why（为什么这么做/何时使用）&quot;">​</a></h2><ul><li><strong>可解释的尺度</strong>：把“平均惊讶度”指数化为“等效分支数”，一眼看出模型不确定性。</li><li><strong>训练/评测</strong>：同一数据与分词口径下比较模型、做早停与退火监控、回归验证（越低越好）。</li><li><strong>误差定位</strong>：联动 token 级惊讶度/熵，定位高困惑度的片段与上下文盲点。</li></ul><h2 id="how-最小复现配方-≤5步" tabindex="-1">How（最小复现配方，≤5步） <a class="header-anchor" href="#how-最小复现配方-≤5步" aria-label="Permalink to &quot;How（最小复现配方，≤5步）&quot;">​</a></h2><ol><li>在评测集上 <strong>教师强制</strong> 前向，拿到每个真标签的 $\\log p(y_t\\mid y_{&lt;t})$（注意屏蔽 PAD）。</li><li>求 <strong>总 NLL</strong>：$L=-\\sum_t \\log p(\\cdot)$（建议以 <strong>nat</strong> 聚合，数值更稳）。</li><li>求 <strong>有效 token 数</strong> $T_{\\text{valid}}$（排除 PAD/BOS 可选/只保留需要计分的 token）。</li><li>得平均交叉熵：$\\bar{H}=L/T_{\\text{valid}}$。</li><li>取指数：$\\mathrm{PPL}=e^{\\bar{H}}$（若步骤 2 用 $\\log_2$，则 $\\mathrm{PPL}=2^{\\bar{H}}$）。</li></ol><h2 id="gotchas-坑点与边界" tabindex="-1">Gotchas（坑点与边界） <a class="header-anchor" href="#gotchas-坑点与边界" aria-label="Permalink to &quot;Gotchas（坑点与边界）&quot;">​</a></h2><ul><li><strong>可比性</strong>：必须 <strong>同数据集、同切分、同分词/词表、同计分口径</strong>（是否含 BOS/EOS、是否区分大小写等）下比较。</li><li><strong>样本口径</strong>：<strong>数据级 PPL</strong> 基于真实序列的 NLL；<strong>单步 PPL</strong> 来自预测分布的熵。它们相关但不等价。</li><li><strong>与主观质量</strong>：PPL 低 ≠ 一定更符合人类偏好；解码策略（温度、top-p/k、约束）会影响可读性但不改变评测 PPL 的定义。</li><li><strong>数值稳定</strong>：极小概率导致 NLL 爆大；用稳定的 <strong>log-softmax / log-sum-exp</strong>，对 $p=0$ 的标签需平滑/剪裁。</li><li><strong>长度与填充</strong>：必须按 <strong>有效 token</strong> 归一；不要对含大量 PAD 的样本直接取均值。</li><li></li></ul><hr><h2 id="raw-notes" tabindex="-1">Raw Notes <a class="header-anchor" href="#raw-notes" aria-label="Permalink to &quot;Raw Notes&quot;">​</a></h2><p>好的！用最直白的话来解释<strong>困惑度（Perplexity, PPL）</strong>：</p><h2 id="它在量什么" tabindex="-1">它在量什么？ <a class="header-anchor" href="#它在量什么" aria-label="Permalink to &quot;它在量什么？&quot;">​</a></h2><p>困惑度就是在问：<strong>模型在每一步平均要面对“多少个等可能的选择”</strong>。</p><ul><li>如果模型很确定，几乎总押中正确词，<strong>困惑度小</strong>（接近 1）。</li><li>如果模型很拿不准，像在多个选项里瞎猜，<strong>困惑度大</strong>。</li></ul><h2 id="和公式怎么对应" tabindex="-1">和公式怎么对应？ <a class="header-anchor" href="#和公式怎么对应" aria-label="Permalink to &quot;和公式怎么对应？&quot;">​</a></h2><p>对一段真实标注的序列 $y_{1:T}$，语言模型给出的条件概率是 $p(y_t\\mid y_{&lt;t})$。</p><ul><li><strong>平均负对数似然（交叉熵）</strong>：$\\displaystyle \\bar{H}=-\\frac{1}{T}\\sum_{t=1}^T \\log p(y_t\\mid y_{&lt;t})$</li><li><strong>困惑度</strong>：$\\displaystyle \\mathrm{PPL}=\\exp(\\bar{H})$（自然对数）。 用二进制对数时：$\\mathrm{PPL}=2^{\\bar{H}_{\\text{bits}}}$。</li></ul><p>直觉：$\\bar{H}$ 是“平均惊讶度”，<strong>把惊讶度指数化</strong>就得到“等效分支数”=困惑度。</p><h2 id="一眼懂的例子" tabindex="-1">一眼懂的例子 <a class="header-anchor" href="#一眼懂的例子" aria-label="Permalink to &quot;一眼懂的例子&quot;">​</a></h2><ul><li>如果模型每步给真实词的概率都是 <strong>0.25</strong>： $-\\log 0.25=\\ln 4$，$\\mathrm{PPL}=e^{\\ln 4}=4$。 含义：像在 <strong>4 选 1</strong> 中选择。</li><li>如果每步概率 <strong>0.1</strong>：$\\mathrm{PPL}=10$（像 10 选 1）。</li><li>如果每步都接近 <strong>1.0</strong>：$\\mathrm{PPL}\\to 1$。</li></ul><h2 id="与-token-熵-的关系" tabindex="-1">与“token 熵”的关系 <a class="header-anchor" href="#与-token-熵-的关系" aria-label="Permalink to &quot;与“token 熵”的关系&quot;">​</a></h2><ul><li><strong>单步困惑度</strong>：$\\mathrm{PPL}_t=\\exp(H_t)$，其中 $H_t=-\\sum_v p_t(v)\\log p_t(v)$ 是<strong>该步分布的熵</strong>（不确定性）。</li><li><strong>数据集困惑度</strong>：对真实序列的<strong>平均 NLL</strong>做指数；和上面的“分布熵”不是同一件事，但两者都刻画“不确定性”。</li></ul><h2 id="使用要点与注意" tabindex="-1">使用要点与注意 <a class="header-anchor" href="#使用要点与注意" aria-label="Permalink to &quot;使用要点与注意&quot;">​</a></h2><ul><li><strong>越低越好</strong>：训练/评测常用“越低越好”的标尺。</li><li><strong>可比性条件</strong>：必须在<strong>同一数据集、同一切分、同一分词/词表</strong>下比较；否则没意义。</li><li><strong>不等于人类偏好</strong>：PPL 低不一定更“有用”或更“好读”；格式约束、解码策略（温度/Top-p）等也影响主观质量。</li><li><strong>单位差异</strong>：用自然对数（nat）或 $\\log_2$（bit）都会被最终的 $\\exp$ 或 $2^{(\\cdot)}$ 抹平，记住你用的是哪种即可。</li></ul><p>一句话总结： <strong>困惑度 = 模型在每一步“等效面对的选择个数”</strong>。它是$\\exp($平均负对数似然$)$，数字越小，说明模型越不“困惑”。</p>',27)])])}const P=r(n,[["render",s]]);export{_ as __pageData,P as default};
