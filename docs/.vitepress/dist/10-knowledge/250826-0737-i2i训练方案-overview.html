<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>i2i训练方案 | ZhiGrove</title>
    <meta name="description" content="Wang Yaqi's Knowledge Base">
    <meta name="generator" content="VitePress v1.6.4">
    <link rel="preload stylesheet" href="/assets/style.BhX5jokk.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.GWSF0Fav.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/theme.DQSJWTrN.js">
    <link rel="modulepreload" href="/assets/chunks/framework.CQuhCYrb.js">
    <link rel="modulepreload" href="/assets/10-knowledge_250826-0737-i2i训练方案-overview.md.ndnUB9Hh.lean.js">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-5d98c3a5><!--[--><!--]--><!--[--><span tabindex="-1" data-v-0b0ada53></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0b0ada53>Skip to content</a><!--]--><!----><header class="VPNav" data-v-5d98c3a5 data-v-ae24b3ad><div class="VPNavBar" data-v-ae24b3ad data-v-6aa21345><div class="wrapper" data-v-6aa21345><div class="container" data-v-6aa21345><div class="title" data-v-6aa21345><div class="VPNavBarTitle has-sidebar" data-v-6aa21345 data-v-1168a8e4><a class="title" href="/" data-v-1168a8e4><!--[--><!--]--><!----><span data-v-1168a8e4>ZhiGrove</span><!--[--><!--]--></a></div></div><div class="content" data-v-6aa21345><div class="content-body" data-v-6aa21345><!--[--><!--]--><div class="VPNavBarSearch search" data-v-6aa21345><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-6aa21345 data-v-dc692963><span id="main-nav-aria-label" class="visually-hidden" data-v-dc692963> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>首页</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/00-inbox/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>收件箱</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/10-knowledge/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>知识库</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/20-papers/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>论文</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/30-ideas/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>灵感</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/40-experiments/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>实验</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/50-reports/" tabindex="0" data-v-dc692963 data-v-e56f3d57><!--[--><span data-v-e56f3d57>报告</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-6aa21345 data-v-6c893767><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-6c893767 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-6aa21345 data-v-0394ad82 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/wangyaqi/ZhiGrove" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-6aa21345 data-v-bb2aa2f0 data-v-cf11d7a2><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-cf11d7a2><span class="vpi-more-horizontal icon" data-v-cf11d7a2></span></button><div class="menu" data-v-cf11d7a2><div class="VPMenu" data-v-cf11d7a2 data-v-b98bc113><!----><!--[--><!--[--><!----><div class="group" data-v-bb2aa2f0><div class="item appearance" data-v-bb2aa2f0><p class="label" data-v-bb2aa2f0>Appearance</p><div class="appearance-action" data-v-bb2aa2f0><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-bb2aa2f0 data-v-5337faa4 data-v-1d5665e3><span class="check" data-v-1d5665e3><span class="icon" data-v-1d5665e3><!--[--><span class="vpi-sun sun" data-v-5337faa4></span><span class="vpi-moon moon" data-v-5337faa4></span><!--]--></span></span></button></div></div></div><div class="group" data-v-bb2aa2f0><div class="item social-links" data-v-bb2aa2f0><div class="VPSocialLinks social-links-list" data-v-bb2aa2f0 data-v-7bc22406><!--[--><a class="VPSocialLink no-icon" href="https://github.com/wangyaqi/ZhiGrove" aria-label="github" target="_blank" rel="noopener" data-v-7bc22406 data-v-bd121fe5><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-6aa21345 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><div class="divider" data-v-6aa21345><div class="divider-line" data-v-6aa21345></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-5d98c3a5 data-v-a6f0e41e><div class="container" data-v-a6f0e41e><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-a6f0e41e><span class="vpi-align-left menu-icon" data-v-a6f0e41e></span><span class="menu-text" data-v-a6f0e41e>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-a6f0e41e data-v-8a42e2b4><button data-v-8a42e2b4>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-5d98c3a5 data-v-319d5ca6><div class="curtain" data-v-319d5ca6></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-319d5ca6><span class="visually-hidden" id="sidebar-aria-label" data-v-319d5ca6> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-c40bc020><section class="VPSidebarItem level-0 has-active" data-v-c40bc020 data-v-b3fd67f8><div class="item" role="button" tabindex="0" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><h2 class="text" data-v-b3fd67f8>Knowledge</h2><!----></div><div class="items" data-v-b3fd67f8><!--[--><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/10-knowledge/" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>README</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/10-knowledge/250822-0737-%E9%87%8D%E5%90%AF%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E5%8D%8A%E5%B0%8F%E6%97%B6%E5%86%85%E4%B8%8D%E5%BC%80%E5%A7%8B%E8%AE%AD-overview.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>250822-0737-重启训练一个半小时内不开始训-overview</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/10-knowledge/250826-0737-OpenUni%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95-overview.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>250826-0737-OpenUni训练方法-overview</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/10-knowledge/250826-0737-i2i%E8%AE%AD%E7%BB%83%E6%96%B9%E6%A1%88-overview.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>250826-0737-i2i训练方案-overview</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/10-knowledge/250827-0736-token%E7%86%B5-overview.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>250827-0736-token熵-overview</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/10-knowledge/250827-0736-%E6%89%A9%E6%95%A3%E8%BF%87%E7%A8%8B%E4%B8%AD%E6%89%93%E5%8D%B0%E6%95%B0%E5%80%BC-overview.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>250827-0736-扩散过程中打印数值-overview</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/10-knowledge/250828-0735-fm_transformers%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0-overview.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>250828-0735-fm_transformers模型参数-overview</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/10-knowledge/250829-0735-%E5%9B%B0%E6%83%91%E5%BA%A6-overview.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>250829-0735-困惑度-overview</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/10-knowledge/250829-0735-%E6%83%8A%E8%AE%B6%E5%BA%A6-overview.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>250829-0735-惊讶度-overview</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/10-knowledge/250830-0735-pipeline%E6%A3%80%E6%9F%A5-overview.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>250830-0735-pipeline检查-overview</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/10-knowledge/250903-0735-macos%E5%BF%AB%E6%8D%B7%E6%93%8D%E4%BD%9C.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>250903-0735-macos快捷操作</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/10-knowledge/250906-0735-VICReg%E7%9B%91%E7%9D%A3.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>250906-0735-VICReg监督</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/10-knowledge/250906-0735-%E6%8A%95%E5%BD%B1%E5%B1%82%E6%96%B9%E6%A1%88.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>250906-0735-投影层方案</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/10-knowledge/250907-0735-%E7%89%B9%E5%BE%81%E8%A7%A3%E8%80%A6%E6%96%B9%E6%A1%88GPT%E5%8F%82%E8%80%83.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>250907-0735-特征解耦方案GPT参考</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-b3fd67f8 data-v-b3fd67f8><div class="item" data-v-b3fd67f8><div class="indicator" data-v-b3fd67f8></div><a class="VPLink link link" href="/10-knowledge/example-knowledge.html" data-v-b3fd67f8><!--[--><p class="text" data-v-b3fd67f8>example-knowledge</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-5d98c3a5 data-v-1428d186><div class="VPDoc has-sidebar has-aside" data-v-1428d186 data-v-39a288b8><!--[--><!--]--><div class="container" data-v-39a288b8><div class="aside" data-v-39a288b8><div class="aside-curtain" data-v-39a288b8></div><div class="aside-container" data-v-39a288b8><div class="aside-content" data-v-39a288b8><div class="VPDocAside" data-v-39a288b8 data-v-3f215769><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3f215769 data-v-a5bbad30><div class="content" data-v-a5bbad30><div class="outline-marker" data-v-a5bbad30></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-a5bbad30>On this page</div><ul class="VPDocOutlineItem root" data-v-a5bbad30 data-v-b933a997><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-39a288b8><div class="content-container" data-v-39a288b8><!--[--><!--]--><main class="main" data-v-39a288b8><div style="position:relative;" class="vp-doc _10-knowledge_250826-0737-i2i%E8%AE%AD%E7%BB%83%E6%96%B9%E6%A1%88-overview" data-v-39a288b8><div><h2 id="tl-dr-≤3点" tabindex="-1">TL;DR（≤3点） <a class="header-anchor" href="#tl-dr-≤3点" aria-label="Permalink to &quot;TL;DR（≤3点）&quot;">​</a></h2><ul><li>OpenUni没有额外的i2i训练数据，只采用了“保持图片不变”的方法进行训练；</li><li>CrossFlow没有开源i2i的训练数据和代码；</li><li></li></ul><h2 id="what-是什么" tabindex="-1">What（是什么） <a class="header-anchor" href="#what-是什么" aria-label="Permalink to &quot;What（是什么）&quot;">​</a></h2><ul><li></li></ul><h2 id="why-为什么这么做-何时使用" tabindex="-1">Why（为什么这么做/何时使用） <a class="header-anchor" href="#why-为什么这么做-何时使用" aria-label="Permalink to &quot;Why（为什么这么做/何时使用）&quot;">​</a></h2><ul><li></li></ul><h2 id="how-最小复现配方-≤5步" tabindex="-1">How（最小复现配方，≤5步） <a class="header-anchor" href="#how-最小复现配方-≤5步" aria-label="Permalink to &quot;How（最小复现配方，≤5步）&quot;">​</a></h2><ul><li></li></ul><h2 id="gotchas-坑点与边界" tabindex="-1">Gotchas（坑点与边界） <a class="header-anchor" href="#gotchas-坑点与边界" aria-label="Permalink to &quot;Gotchas（坑点与边界）&quot;">​</a></h2><ul><li></li></ul><h2 id="raw-notes" tabindex="-1">Raw Notes <a class="header-anchor" href="#raw-notes" aria-label="Permalink to &quot;Raw Notes&quot;">​</a></h2><h1 id="metaquery" tabindex="-1">MetaQuery <a class="header-anchor" href="#metaquery" aria-label="Permalink to &quot;MetaQuery&quot;">​</a></h1><h2 id="_1-训练方法-training-method" tabindex="-1">1）训练方法（Training Method） <a class="header-anchor" href="#_1-训练方法-training-method" aria-label="Permalink to &quot;1）训练方法（Training Method）&quot;">​</a></h2><ul><li><p>总体范式：用一组可学习的 MetaQueries 直接喂给冻结的多模态 LLM（MLLM），取其输出隐表征作为条件，经一个可训练的 Connector对齐到扩散式生成模型（DiT）的条件输入空间；全流程只用标准去噪目标（diffusion denoising objective）在图文配对数据上训练。可选再加入图像重建目标与去噪目标混合训练以获得重建/编辑能力。</p></li><li><p>模块是否训练：默认冻结 MLLM；训练 MetaQuery（可学习查询）+ Connector；扩散解码器（DiT）既可冻结也可微调，论文给出 ablation：只冻 LLM 时已可达高水准；进一步微调 DiT能继续提升画质。</p></li><li><p>Connector 设计：两种结构对比——</p><ul><li><p>Proj‑Enc：先投到 DiT 条件维度再过 Transformer Encoder；</p></li><li><p>Enc‑Proj：先在与 MLLM 隐维相同的维度用 Transformer Encoder 对齐，再投到 DiT 条件维度。 实验更推荐 Enc‑Proj（更省参、效果更好）。Connector 采用双向注意力的 Transformer Encoder。</p></li></ul></li><li><p>训练目标对比：只用 T2I 去噪目标最佳；纯重建目标最差；混合（T2I+重建）可在不明显伤害 T2I 的前提下获得重建/编辑能力。</p></li></ul><h2 id="_2-训练参数-training-hyperparameters" tabindex="-1">2）训练参数（Training Hyperparameters） <a class="header-anchor" href="#_2-训练参数-training-hyperparameters" aria-label="Permalink to &quot;2）训练参数（Training Hyperparameters）&quot;">​</a></h2><h3 id="预训练-pre‐training" tabindex="-1">预训练（Pre‑training） <a class="header-anchor" href="#预训练-pre‐training" aria-label="Permalink to &quot;预训练（Pre‑training）&quot;">​</a></h3><ul><li>数据规模：25M 公开图文对。</li><li>轮数：8 epochs（3.2 概述段落曾写 4 epochs，以 4 节为准）。</li><li>全局 batch size：4096。</li><li>学习率与调度：初始 1e‑4，cosine decay；4000 步 warm‑up；最终衰减到 1e‑5。</li><li>是否冻结：MLLM 冻结；训练 MetaQueries + Connector；DiT 可冻或微调（微调更好）。</li><li>分辨率 / 生成头：文中对齐实验多用 Sana‑0.6B@512 分辨率；COCO FID 评测使用 Stable Diffusion v1.5。</li></ul><h3 id="指令微调-instruction-tuning" tabindex="-1">指令微调（Instruction Tuning） <a class="header-anchor" href="#指令微调-instruction-tuning" aria-label="Permalink to &quot;指令微调（Instruction Tuning）&quot;">​</a></h3><ul><li>数据规模：构建 2.4M “成对图像 + 指令” 样本（见下文数据结构）。</li><li>轮数：3 epochs。</li><li>batch size：2048。</li><li>学习率调度：与预训练相同（cosine + 4k warm‑up）.</li></ul><h3 id="架构-token-侧关键超参" tabindex="-1">架构/Token 侧关键超参 <a class="header-anchor" href="#架构-token-侧关键超参" aria-label="Permalink to &quot;架构/Token 侧关键超参&quot;">​</a></h3><ul><li>MetaQuery 数量（#tokens）：系统性标度实验给出 64 已较好，更大量能进一步提升对齐；论文在模型家族实验中统一设为 256 tokens（博客页明确载明）。</li><li>Connector 层数与维度：推荐 Enc‑Proj，24 层；示例维度 896（Enc‑Proj） vs 2304（Proj‑Enc）；24 层 Enc‑Proj 参数量约 316M。</li><li>背骨与生成头组合：MLLM 采用 LLaVA‑OneVision‑0.5B / Qwen2.5‑VL‑3B / Qwen2.5‑VL‑7B；生成头测试 SD‑v1.5 与 Sana‑1.6B。</li></ul><h2 id="_3-数据结构-data-structure-dataset-construction" tabindex="-1">3）数据结构（Data Structure / Dataset Construction） <a class="header-anchor" href="#_3-数据结构-data-structure-dataset-construction" aria-label="Permalink to &quot;3）数据结构（Data Structure / Dataset Construction）&quot;">​</a></h2><h3 id="a-预训练数据-统一建模的基础对齐" tabindex="-1">A. 预训练数据（统一建模的基础对齐） <a class="header-anchor" href="#a-预训练数据-统一建模的基础对齐" aria-label="Permalink to &quot;A. 预训练数据（统一建模的基础对齐）&quot;">​</a></h3><ul><li>标准 (image, caption) 图文对，规模 25M；用于把冻结的 MLLM 条件通过 Connector 对齐到扩散解码器，仅用去噪目标训练。</li></ul><h3 id="b-指令微调数据-编辑-主体驱动等进阶能力" tabindex="-1">B. 指令微调数据（编辑/主体驱动等进阶能力） <a class="header-anchor" href="#b-指令微调数据-编辑-主体驱动等进阶能力" aria-label="Permalink to &quot;B. 指令微调数据（编辑/主体驱动等进阶能力）&quot;">​</a></h3><ul><li><p>来源：从 mmc4 fewer‑faces 子集拿到“图像 + caption”，用 SigLIP 进行按 caption 相似度聚类（每簇 ≤6，阈值 0.5）；每簇中与其他图像平均相似度最低者设为 target，其余为 source；得到 2.4M 组 (sources, target)。随后用 Qwen2.5‑VL‑3B 为每对生成开放式指令。</p></li><li><p>样本格式： (source_images: 1..N, target_image, instruction_text) —— instruction 要同时描述一条与 sources 的笼统相似点（如“同款上衣/相似斧头/相似建筑”）以及 target 独有的全部差异；不得包含足以单独重建 target 的具体细节（避免泄露/投机），鼓励简洁。</p></li><li><p>用途：在保持 MLLM 冻结的前提下，对 MetaQueries + Connector +（可选）DiT 进行指令微调，获得图像编辑、主体驱动、“视觉联想”“Logo 设计”等能力。</p></li></ul><h3 id="补充" tabindex="-1">补充 <a class="header-anchor" href="#补充" aria-label="Permalink to &quot;补充&quot;">​</a></h3><ul><li>用 可学习查询（MetaQueries） 不仅在画质与对齐上可与“最后一层嵌入”相当/更好，更关键是保留了 MLLM 的“在上下文学习/推理/知识迁移”能力，在需要世界知识与推理的生成上显著更强。</li></ul><h1 id="openuni" tabindex="-1">OpenUni <a class="header-anchor" href="#openuni" aria-label="Permalink to &quot;OpenUni&quot;">​</a></h1><h2 id="训练框架概览" tabindex="-1">训练框架概览 <a class="header-anchor" href="#训练框架概览" aria-label="Permalink to &quot;训练框架概览&quot;">​</a></h2><ul><li>模型结构 OpenUni 建立在**冻结的多模态大语言模型（InternVL3）**与 **扩散模型（SANA DiT）**之间，通过 可学习查询（256 tokens） + 轻量连接器（6 层 Transformer） 实现语义桥接，从而统一理解与图像生成功能。</li></ul><h2 id="一、stage-1-预训练-pre-training" tabindex="-1">一、Stage 1：预训练（Pre-training） <a class="header-anchor" href="#一、stage-1-预训练-pre-training" aria-label="Permalink to &quot;一、Stage 1：预训练（Pre-training）&quot;">​</a></h2><ul><li>目标：仅训练 learnable queries + lightweight connector（LLM 与扩散模型权重均冻结），让连接模块学习将 LLM 输出映射为图像生成条件</li><li>数据来源：共计约 23M 图像-文本对，来自多个公开数据集（text‑to‑image‑2M、LAION‑Aesthetic‑6M、Megalith‑10M、RedCaps‑5M），所有 caption 由 LLM 重写生成。 | 超参数 | 数值 | | ------------- | ------------------ | | 容器 Batch Size | 512 | | 优化器 | AdamW | | 学习率 | $1 \times 10^{-4}$ | | 权重衰减 | 0.05 | | 梯度裁剪 | 1.0 | | Betas | (0.9, 0.95) | | 学习率调度 | Cosine decay | | Warm‑up Steps | 1,000 | | 总训练步数 | 100,000 steps |</li></ul><h2 id="二、stage-2-高质量微调-high-quality-fine-tuning" tabindex="-1">二、Stage 2：高质量微调（High-Quality Fine-tuning） <a class="header-anchor" href="#二、stage-2-高质量微调-high-quality-fine-tuning" aria-label="Permalink to &quot;二、Stage 2：高质量微调（High-Quality Fine-tuning）&quot;">​</a></h2><ul><li>目标：解冻扩散模型，让 connector 和 diffusion 模型一起进一步优化，以提升生成质量、对指令的响应度及鲁棒性。</li><li>数据来源：使用 BLIP3‑o 提供的 60,000 条高质量 instruction-image 样本，这些样本基于 GPT‑4o + DALL‑E3 / Midjourney 生成。</li><li>训练超参数： | 超参数 | 数值 | | ------------- | ------------------ | | Batch Size | 256 | | 优化器 | AdamW | | 学习率 | $1 \times 10^{-5}$ | | 权重衰减 | 0.05 | | 梯度裁剪 | 1.0 | | Betas | (0.9, 0.95) | | 学习率调度 | Cosine decay | | Warm‑up Steps | 100 | | 总训练步数 | 10,000 steps |</li></ul><h1 id="crossflow" tabindex="-1">CrossFlow <a class="header-anchor" href="#crossflow" aria-label="Permalink to &quot;CrossFlow&quot;">​</a></h1><h2 id="_1-方法概览-training-approach" tabindex="-1">1. 方法概览（Training Approach） <a class="header-anchor" href="#_1-方法概览-training-approach" aria-label="Permalink to &quot;1. 方法概览（Training Approach）&quot;">​</a></h2><ul><li><p>核心创新 CrossFlow 完全打破传统扩散/流匹配模型必须从随机噪声开始的限制。它将源模态（如文本、低分辨率图像）分布直接映射到目标模态（如高分辨率图像、图像描述等），无需噪声输入或条件机制（如 cross-attention）。</p></li><li><p>关键技术突破</p><ul><li>Variational Encoder：用于将源模态编码成与目标模态相同维度与空间结构，解决模态间数据形状不一致的问题，同时引入正则化效果。</li><li>Classifer-Free Guidance（CFG）：通过在训练中引入二值指示变量，实现无需条件架构也能控制生成质量的引导机制。</li></ul></li><li><p>架构简洁高效 使用最普通的 Transformer（无 cross-attention）、统一处理输入与输出编码的 Token，展现跨模态生成的普适性。无需为特定任务额外设计结构。</p></li></ul><h2 id="_2-应用范例及任务覆盖-use-cases-performance" tabindex="-1">2. 应用范例及任务覆盖（Use Cases &amp; Performance） <a class="header-anchor" href="#_2-应用范例及任务覆盖-use-cases-performance" aria-label="Permalink to &quot;2. 应用范例及任务覆盖（Use Cases &amp; Performance）&quot;">​</a></h2><p>CrossFlow 在多个任务上与主流方法表现旗鼓相当或更优：</p><ul><li><p>主推任务：文本转图像生成（Text-to-Image）。</p><ul><li>CrossFlow 在该任务中，即使不采用条件机制，也略微优于常规 flow matching 模型，同时在大规模训练与模型扩展下更具优势。</li></ul></li><li><p>扩展任务：</p><ul><li>图像描述（Image captioning）</li><li>单目深度估计（Monocular depth estimation）</li><li>图像超分辨率（Super-resolution） 在这些任务中，CrossFlow 均与或优于现有专用架构方法，体现其通用架构的潜力。</li></ul></li><li><p>Latent Arithmetic 得益于 Variational Encoder 编码出的源分布具有语义结构，可在潜空间中进行有意思的编辑运算，实现对输出模态的语义控制。</p></li></ul><h2 id="_3-精炼训练策略-training-scheme" tabindex="-1">3. 精炼训练策略（Training Scheme） <a class="header-anchor" href="#_3-精炼训练策略-training-scheme" aria-label="Permalink to &quot;3. 精炼训练策略（Training Scheme）&quot;">​</a></h2><h2 id="_4-训练参数" tabindex="-1">4. 训练参数 <a class="header-anchor" href="#_4-训练参数" aria-label="Permalink to &quot;4. 训练参数&quot;">​</a></h2><table tabindex="0"><thead><tr><th>任务类别</th><th>模型规模</th><th>Epoch / Steps</th><th>Batch Size</th><th>Learning Rate</th><th>Warm-up</th><th>备注</th></tr></thead><tbody><tr><td>图像描述</td><td>351M</td><td>100 epochs</td><td>256</td><td>2 × 10⁻⁴</td><td>5 epochs</td><td>–</td></tr><tr><td>深度估计</td><td>527M</td><td>50 epochs</td><td>64</td><td>1 × 10⁻⁴ → 1 × 10⁻⁸</td><td>cosine annealing</td><td>–</td></tr><tr><td>图像超分辨率</td><td>505M</td><td>1,000,000 steps</td><td>512</td><td>1 × 10⁻⁴</td><td>5,000 steps</td><td>–</td></tr><tr><td>文本→图像（T2I）</td><td>~950M</td><td>~300K steps</td><td>未指出</td><td>未指出</td><td>未指出</td><td>同步 baseline，性能略优</td></tr></tbody></table><ul><li>Image Captioning</li></ul><p>“We train a 351M model for 100 epochs with a batch size of 256 and a learning rate of 2e-4 with 5 warm-up epochs.”【CVPR2025 补充材料】</p><ul><li>Depth Estimation</li></ul><p>“We train a 527M model for 50 epochs with a batch size of 64 and a learning rate decayed from 1e-4 to 1e-8 with cosine annealing.”</p><ul><li>Super-Resolution</li></ul><p>“We train a 505M model for 1M steps with a batch size of 512, a learning rate of 1e-4, and 5k warm-up steps.”</p><ul><li>Text-to-Image</li></ul><p>“We train a ~0.95B model for 300K steps with the same training budget as baseline (FID 10.13 vs 10.79).”</p><h1 id="图像编辑-图生图-数据训练表" tabindex="-1">图像编辑/图生图 - 数据训练表 <a class="header-anchor" href="#图像编辑-图生图-数据训练表" aria-label="Permalink to &quot;图像编辑/图生图 - 数据训练表&quot;">​</a></h1><table tabindex="0"><thead><tr><th>用途</th><th>数据/项目</th><th style="text-align:right;">开源</th><th>规模/形式</th><th>许可</th><th>获取/备注</th></tr></thead><tbody><tr><td>指令编辑 (Instruction-based)</td><td><strong>UltraEdit</strong></td><td style="text-align:right;">✅</td><td>≈ <strong>4M</strong> 编辑样本（含自由编辑与区域编辑）</td><td><strong>CC-BY-4.0</strong></td><td>代码、模型与<strong>数据集</strong>均提供，HF 数据集条目：<em>BleachNick/UltraEdit</em>。(<a href="https://github.com/HaozheZhao/UltraEdit" title="GitHub - HaozheZhao/UltraEdit" target="_blank" rel="noreferrer">GitHub</a>, <a href="https://huggingface.co/datasets/BleachNick/UltraEdit" title="BleachNick/UltraEdit · Datasets at Hugging Face" target="_blank" rel="noreferrer">Hugging Face</a>)</td></tr><tr><td>指令编辑</td><td><strong>MagicBrush</strong></td><td style="text-align:right;">✅</td><td><strong>10K</strong> 三元组（源图、指令、目标图），含单/多轮、带/不带掩码</td><td><strong>CC-BY-4.0</strong></td><td>GitHub 与 HF 提供<strong>训练/验证集下载</strong>；测试集需单独压缩包下载。(<a href="https://github.com/OSU-NLP-Group/MagicBrush" title="GitHub - OSU-NLP-Group/MagicBrush: [NeurIPS&#39;23] &quot;MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing&quot;." target="_blank" rel="noreferrer">GitHub</a>, <a href="https://huggingface.co/datasets/osunlp/MagicBrush" title="osunlp/MagicBrush · Datasets at Hugging Face" target="_blank" rel="noreferrer">Hugging Face</a>)</td></tr><tr><td>指令编辑</td><td><strong>HQ-Edit</strong></td><td style="text-align:right;">✅</td><td><strong>197,350</strong> 次编辑，高分辨率，含（正向/反向）编辑文本</td><td><strong>CC-BY-NC-4.0</strong></td><td>HF 数据集条目与项目页均公开（<strong>非商用</strong>）。(<a href="https://huggingface.co/datasets/UCSC-VLAA/HQ-Edit" title="UCSC-VLAA/HQ-Edit · Datasets at Hugging Face" target="_blank" rel="noreferrer">Hugging Face</a>)</td></tr><tr><td>指令编辑（合成）</td><td><strong>InstructPix2Pix 生成数据</strong></td><td style="text-align:right;">✅</td><td><strong>451,990</strong>（随机采样）/ <strong>313,010</strong>（CLIP 过滤）对，附下载脚本</td><td><em>未明示</em>（随项目/源数据）</td><td>官方仓库提供两版数据规模与下载方法（已做 NSFW 过滤）。(<a href="https://github.com/timothybrooks/instruct-pix2pix" title="GitHub - timothybrooks/instruct-pix2pix" target="_blank" rel="noreferrer">GitHub</a>)</td></tr><tr><td>聚合编辑集</td><td><strong>GPT-Image-Edit-1.5M</strong>（编辑基准 V2）</td><td style="text-align:right;">✅</td><td><strong>1.5M</strong>，聚合自 UltraEdit、HQ-Edit、OmniEdit、Complex-Edit 等</td><td>依<strong>来源</strong>而异</td><td>提供统一格式与子集说明；使用时需遵守各源集许可。(<a href="https://huggingface.co/datasets/UCSC-VLAA/GPT-Image-Edit-1.5M?utm_source=chatgpt.com" title="UCSC-VLAA/GPT-Image-Edit-1.5M · Datasets at ..." target="_blank" rel="noreferrer">Hugging Face</a>)</td></tr><tr><td>结构/控制条件</td><td><strong>ControlNet（示例集 fill50k）</strong></td><td style="text-align:right;">✅</td><td><strong>fill50k</strong> 示范数据包（教学/验证）</td><td><em>未明示</em></td><td>官方未发布完整训练语料；推荐<strong>用检测器在公有图集上自建条件</strong>（姿态、边缘、深度等）。(<a href="https://github.com/lllyasviel/ControlNet?utm_source=chatgpt.com" title="lllyasviel/ControlNet: Let us control diffusion models!" target="_blank" rel="noreferrer">GitHub</a>, <a href="https://huggingface.co/blog/train-your-controlnet?utm_source=chatgpt.com" title="Train your ControlNet with diffusers" target="_blank" rel="noreferrer">Hugging Face</a>, <a href="https://mmagic.readthedocs.io/en/latest/autoapi/mmagic/datasets/controlnet_dataset/index.html?utm_source=chatgpt.com" title="mmagic.datasets.controlnet_dataset" target="_blank" rel="noreferrer">MMagic</a>)</td></tr><tr><td>结构/控制条件</td><td><strong>T2I-Adapter</strong></td><td style="text-align:right;">✅（代码/权重）</td><td><em>无官方固定数据集</em></td><td>—</td><td>官方建议<strong>自备数据</strong>（如 COCO/LAION 派生并生成控制信号）进行训练。(<a href="https://github.com/TencentARC/T2I-Adapter?utm_source=chatgpt.com" title="TencentARC/T2I-Adapter" target="_blank" rel="noreferrer">GitHub</a>, <a href="https://arxiv.org/abs/2302.08453?utm_source=chatgpt.com" title="T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models" target="_blank" rel="noreferrer">arXiv</a>)</td></tr><tr><td>图像提示/风格参照</td><td><strong>IP-Adapter</strong></td><td style="text-align:right;">✅（代码/权重）</td><td><em>无官方固定数据集</em></td><td><strong>Apache-2.0</strong>（仓库）</td><td>提供<strong>训练脚本</strong>与数据 JSON 规范，需自建（参考图像 ↔ 文本/目标）样本。(<a href="https://github.com/tencent-ailab/IP-Adapter" title="GitHub - tencent-ailab/IP-Adapter: The image prompt adapter is designed to enable a pretrained text-to-image diffusion model to generate images with image prompt." target="_blank" rel="noreferrer">GitHub</a>)</td></tr><tr><td>示例参照编辑</td><td><strong>Paint-by-Example</strong></td><td style="text-align:right;">✅（代码/权重）</td><td><em>方法型</em>（基于自监督裁剪/遮挡构造对）</td><td>—</td><td>论文/代码公开，训练对通常从通用图集<strong>自生成</strong>（非独立发布数据）。(<a href="https://github.com/Fantasy-Studio/Paint-by-Example?utm_source=chatgpt.com" title="Paint by Example: Exemplar-based Image Editing with ..." target="_blank" rel="noreferrer">GitHub</a>, <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Paint_by_Example_Exemplar-Based_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.pdf?utm_source=chatgpt.com" title="Exemplar-Based Image Editing With Diffusion Models" target="_blank" rel="noreferrer">CVF开放获取</a>)</td></tr><tr><td>通用 I2I 任务（复原/修补等）</td><td><strong>Palette</strong></td><td style="text-align:right;">论文/项目页公开</td><td><em>任务可复现</em>（在 ImageNet/COCO 等上<strong>合成退化/掩码</strong>）</td><td>—</td><td>无独立数据发布；按论文流程<strong>自建合成任务</strong>（上色、补全、去 JPEG 等）。(<a href="https://arxiv.org/abs/2111.05826?utm_source=chatgpt.com" title="Palette: Image-to-Image Diffusion Models" target="_blank" rel="noreferrer">arXiv</a>, <a href="https://iterative-refinement.github.io/palette/" title="Palette: Image-to-Image Diffusion Models" target="_blank" rel="noreferrer">SR3</a>)</td></tr></tbody></table></div></div></main><footer class="VPDocFooter" data-v-39a288b8 data-v-e257564d><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-e257564d><span class="visually-hidden" id="doc-footer-aria-label" data-v-e257564d>Pager</span><div class="pager" data-v-e257564d><a class="VPLink link pager-link prev" href="/10-knowledge/250826-0737-OpenUni%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95-overview.html" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Previous page</span><span class="title" data-v-e257564d>250826-0737-OpenUni训练方法-overview</span><!--]--></a></div><div class="pager" data-v-e257564d><a class="VPLink link pager-link next" href="/10-knowledge/250827-0736-token%E7%86%B5-overview.html" data-v-e257564d><!--[--><span class="desc" data-v-e257564d>Next page</span><span class="title" data-v-e257564d>250827-0736-token熵-overview</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-5d98c3a5 data-v-e315a0ad><div class="container" data-v-e315a0ad><p class="message" data-v-e315a0ad>Released under the MIT License.</p><p class="copyright" data-v-e315a0ad>Copyright © 2025 Wang Yaqi</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"00-inbox_index.md\":\"BxMK2y0K\",\"00-inbox_readme.md\":\"RyqrPFdx\",\"00-inbox_week-34_triage.md\":\"DVk_gK-E\",\"00-inbox_week-35_triage.md\":\"CJ7x_I7h\",\"00-inbox_week-36_triage.md\":\"Bw37u8Cj\",\"00-inbox_week-37_250908-0020-clip损失.md\":\"D6cejIoN\",\"00-inbox_week-37_250908-0020-moe llama结构.md\":\"_N3i4Vl_\",\"00-inbox_week-37_250908-0020-siglip损失.md\":\"DuDR7RnK\",\"00-inbox_week-37_250909-0023-dino语义监督.md\":\"Dx3ylhPa\",\"00-inbox_week-37_250910-0024-clip-siglip-vicreg损失区别与适用场景.md\":\"C2M3PCzT\",\"00-inbox_week-37_250910-0024-训练loss调试思路.md\":\"CGjZKtrV\",\"00-inbox_week-38_250918-0024-0916talk.md\":\"CAnJpmsX\",\"00-inbox_week-38_250920-0022-check_code.md\":\"B8OVOP6a\",\"00-inbox_week-39_250919-0024-0919-代码框架问题排查.md\":\"BPGxg1yy\",\"00-inbox_week-39_250924-0024-0922-ocr-todolist.md\":\"B57-S-36\",\"00-inbox_week-39_250925-0025-0923-复现crossflow.md\":\"cxp65mul\",\"00-inbox_week-39_250925-0025-语义调音.md\":\"Cmh-y7Ll\",\"00-inbox_week-41_251010-0025-0922-ocr-todolist.md\":\"CoLfL63b\",\"00-inbox_week-41_251010-0025-0923-复现crossflow.md\":\"DKn9e9Z3\",\"00-inbox_week-41_251010-0025-dots_ocr服务部署 _ 调用.md\":\"CsnpHxq_\",\"00-inbox_week-41_251010-0025-laion400m训练数据处理.md\":\"BkdfG2q1\",\"00-inbox_week-41_251010-0025-语义调音.md\":\"C8I2YM0Z\",\"00-inbox_week-45_251027-1431-vla-vla调研分享.md\":\"DiWBICAy\",\"00-inbox_week-45_251104-1431-ocr任务-docx等格式文档检测.md\":\"ClznCRj-\",\"00-inbox_week-45_251104-1431-vla-扩散生成调研.md\":\"Dc_BUAti\",\"00-inbox_week-45_251104-1431-vla-综述论文节选翻译-0925发布.md\":\"DlcaK2ts\",\"00-inbox_week-46_251111-0027-k线记忆卡.md\":\"CjGZKdiV\",\"00-inbox_week-50_251113-1650-自动交易策略实现log.md\":\"C0_jSP06\",\"00-inbox_week-50_251211-1650-deepseekv3_2.md\":\"CW71BT6n\",\"00-inbox_week-50_251211-1650-jit论文-视频要点.md\":\"wGtIpzah\",\"00-inbox_week-50_251211-1650-rustdesk安装及配置.md\":\"b3esiVId\",\"00-inbox_week-50_251211-1650-trm应用于vla.md\":\"-yGNCErp\",\"00-inbox_week-50_251211-1650-trm应用大纲.md\":\"CnrnC4gM\",\"00-inbox_week-50_251211-1650-trm论文精读笔记.md\":\"BpdLK_Tu\",\"00-inbox_week-50_251211-1650-vae原理.md\":\"BYNqrid7\",\"00-inbox_week-50_251211-1650-图像生成基座模型调研.md\":\"DGL06d4n\",\"00-inbox_week-50_251211-1650-场景理解分类功能.md\":\"CpvDMYk-\",\"10-knowledge_250822-0737-重启训练一个半小时内不开始训-overview.md\":\"Chmk5W5x\",\"10-knowledge_250826-0737-i2i训练方案-overview.md\":\"ndnUB9Hh\",\"10-knowledge_250826-0737-openuni训练方法-overview.md\":\"Bz71xLqy\",\"10-knowledge_250827-0736-token熵-overview.md\":\"B1Mv5o5q\",\"10-knowledge_250827-0736-扩散过程中打印数值-overview.md\":\"2CZHDGmA\",\"10-knowledge_250828-0735-fm_transformers模型参数-overview.md\":\"BZZWEWfO\",\"10-knowledge_250829-0735-困惑度-overview.md\":\"BQF8-SBk\",\"10-knowledge_250829-0735-惊讶度-overview.md\":\"DlUdKycc\",\"10-knowledge_250830-0735-pipeline检查-overview.md\":\"DfRDzeKl\",\"10-knowledge_250903-0735-macos快捷操作.md\":\"DGlbZob2\",\"10-knowledge_250906-0735-vicreg监督.md\":\"DRzQqJ3b\",\"10-knowledge_250906-0735-投影层方案.md\":\"7iWiO02D\",\"10-knowledge_250907-0735-特征解耦方案gpt参考.md\":\"UwEg4l-R\",\"10-knowledge_example-knowledge.md\":\"D90jZDvZ\",\"10-knowledge_index.md\":\"DX2HZJ0R\",\"10-knowledge_readme.md\":\"DEZ7OggO\",\"20-papers_2025_2025-08-20-.md\":\"OPzruvQ2\",\"20-papers_2025_2025-08-20-vtla-preference-learning.md\":\"Dz6vUCR_\",\"20-papers_250829-0735-qwen2.5-omni论文阅读.md\":\"BUa0mMWP\",\"20-papers_index.md\":\"CgQW4AZg\",\"20-papers_vla_all_papers.md\":\"5XYncC_w\",\"30-ideas_2025_9月_250826-0737-与弘扬讨论-overview.md\":\"BNUaUdAg\",\"30-ideas_2025_9月_250826-0737-现有模型结构可改进点-overview.md\":\"BUFGBCsU\",\"30-ideas_2025_9月_250827-0736-0826-与弘扬沟通-overview.md\":\"BJckJ-Fw\",\"30-ideas_2025_9月_250829-0735-中间层模型方案梳理-overview.md\":\"BB7Rf6Gc\",\"30-ideas_2025_9月_250829-0735-论文理论包装方案.md\":\"DK30ycFI\",\"30-ideas_2025_9月_250904-0735-多轮对话拓展idea-overview.md\":\"B_AZd7pL\",\"30-ideas_2025_9月_250906-0735-0905_今日晨思.md\":\"DeW5zvzz\",\"30-ideas_2025_9月_250907-0735-梦梦的建议.md\":\"b2uAz91C\",\"30-ideas_2025_9月_week36-论文实验idea汇总.md\":\"DO5mY2KW\",\"30-ideas_index.md\":\"BLsaRW7n\",\"40-experiments_250826-0736-0826测评日志.md\":\"Ceca8mbE\",\"40-experiments_250902-0735-0901实验分析.md\":\"l-G-GhLH\",\"40-experiments_250904-0735-exp_card_0903_transencodder.md\":\"NlMuTFDx\",\"40-experiments_250904-0735-实验日志模板.md\":\"B9zK1IYR\",\"40-experiments_exp_card_0908_dinov2.md\":\"3Vfhs2r5\",\"40-experiments_index.md\":\"sKDrNhTt\",\"50-reports_250820-0754-ocr调研.md\":\"CJ40cd-y\",\"50-reports_250827-0736-高熵强化学习.md\":\"DC-sefpp\",\"50-reports_250901-0735-resume.md\":\"DtNSBA5Y\",\"50-reports_250903-0735-座舱vla端云协同方案.md\":\"C4iR_FDg\",\"50-reports_index.md\":\"BaF-7d1_\",\"50-reports_resume_250901-0735-resume_network.md\":\"CxyKg_Il\",\"50-reports_座舱vla端云协同方案.md\":\"DuVATegy\",\"50-reports_文本扩散模型调研.md\":\"DEi7WSBa\",\"50-reports_训练与部署资源申请书.md\":\"DUY8Bcs6\",\"50-reports_近期发布模型调研-0908.md\":\"Q46srq98\",\"50-reports_长视频理解综述.md\":\"BseMc13e\",\"50-reports_高熵rl - 数据构建增益.md\":\"DU4AIY7y\",\"index.md\":\"CEbDhJUx\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"ZhiGrove\",\"description\":\"Wang Yaqi's Knowledge Base\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"收件箱\",\"link\":\"/00-inbox/\"},{\"text\":\"知识库\",\"link\":\"/10-knowledge/\"},{\"text\":\"论文\",\"link\":\"/20-papers/\"},{\"text\":\"灵感\",\"link\":\"/30-ideas/\"},{\"text\":\"实验\",\"link\":\"/40-experiments/\"},{\"text\":\"报告\",\"link\":\"/50-reports/\"}],\"sidebar\":{\"/00-inbox/\":[{\"text\":\"Inbox\",\"items\":[{\"text\":\"README\",\"link\":\"/00-inbox/\"},{\"text\":\"week-34\",\"collapsed\":true,\"items\":[{\"text\":\"TRIAGE\",\"link\":\"/00-inbox/week-34/TRIAGE\"}]},{\"text\":\"week-35\",\"collapsed\":true,\"items\":[{\"text\":\"TRIAGE\",\"link\":\"/00-inbox/week-35/TRIAGE\"}]},{\"text\":\"week-36\",\"collapsed\":true,\"items\":[{\"text\":\"TRIAGE\",\"link\":\"/00-inbox/week-36/TRIAGE\"}]},{\"text\":\"week-37\",\"collapsed\":true,\"items\":[{\"text\":\"250908-0020-MoE llama结构\",\"link\":\"/00-inbox/week-37/250908-0020-MoE llama结构\"},{\"text\":\"250908-0020-clip损失\",\"link\":\"/00-inbox/week-37/250908-0020-clip损失\"},{\"text\":\"250908-0020-siglip损失\",\"link\":\"/00-inbox/week-37/250908-0020-siglip损失\"},{\"text\":\"250909-0023-dino语义监督\",\"link\":\"/00-inbox/week-37/250909-0023-dino语义监督\"},{\"text\":\"250910-0024-clip-siglip-vicreg损失区别与适用场景\",\"link\":\"/00-inbox/week-37/250910-0024-clip-siglip-vicreg损失区别与适用场景\"},{\"text\":\"250910-0024-训练loss调试思路\",\"link\":\"/00-inbox/week-37/250910-0024-训练loss调试思路\"}]},{\"text\":\"week-38\",\"collapsed\":true,\"items\":[{\"text\":\"250918-0024-0916talk\",\"link\":\"/00-inbox/week-38/250918-0024-0916talk\"},{\"text\":\"250920-0022-check_code\",\"link\":\"/00-inbox/week-38/250920-0022-check_code\"}]},{\"text\":\"week-39\",\"collapsed\":true,\"items\":[{\"text\":\"250919-0024-0919-代码框架问题排查\",\"link\":\"/00-inbox/week-39/250919-0024-0919-代码框架问题排查\"},{\"text\":\"250924-0024-0922-ocr-TODOlist\",\"link\":\"/00-inbox/week-39/250924-0024-0922-ocr-TODOlist\"},{\"text\":\"250925-0025-0923-复现crossflow\",\"link\":\"/00-inbox/week-39/250925-0025-0923-复现crossflow\"},{\"text\":\"250925-0025-语义调音\",\"link\":\"/00-inbox/week-39/250925-0025-语义调音\"}]},{\"text\":\"week-41\",\"collapsed\":true,\"items\":[{\"text\":\"251010-0025-0922-ocr-TODOlist\",\"link\":\"/00-inbox/week-41/251010-0025-0922-ocr-TODOlist\"},{\"text\":\"251010-0025-0923-复现crossflow\",\"link\":\"/00-inbox/week-41/251010-0025-0923-复现crossflow\"},{\"text\":\"251010-0025-dots_ocr服务部署 & 调用\",\"link\":\"/00-inbox/week-41/251010-0025-dots_ocr服务部署 & 调用\"},{\"text\":\"251010-0025-laion400m训练数据处理\",\"link\":\"/00-inbox/week-41/251010-0025-laion400m训练数据处理\"},{\"text\":\"251010-0025-语义调音\",\"link\":\"/00-inbox/week-41/251010-0025-语义调音\"}]},{\"text\":\"week-45\",\"collapsed\":true,\"items\":[{\"text\":\"251027-1431-VLA-VLA调研分享\",\"link\":\"/00-inbox/week-45/251027-1431-VLA-VLA调研分享\"},{\"text\":\"251104-1431-OCR任务-docx等格式文档检测\",\"link\":\"/00-inbox/week-45/251104-1431-OCR任务-docx等格式文档检测\"},{\"text\":\"251104-1431-VLA-扩散生成调研\",\"link\":\"/00-inbox/week-45/251104-1431-VLA-扩散生成调研\"},{\"text\":\"251104-1431-VLA-综述论文节选翻译-0925发布\",\"link\":\"/00-inbox/week-45/251104-1431-VLA-综述论文节选翻译-0925发布\"}]},{\"text\":\"week-46\",\"collapsed\":true,\"items\":[{\"text\":\"251111-0027-k线记忆卡\",\"link\":\"/00-inbox/week-46/251111-0027-k线记忆卡\"}]},{\"text\":\"week-50\",\"collapsed\":true,\"items\":[{\"text\":\"251113-1650-自动交易策略实现log\",\"link\":\"/00-inbox/week-50/251113-1650-自动交易策略实现log\"},{\"text\":\"251211-1650-JiT论文-视频要点\",\"link\":\"/00-inbox/week-50/251211-1650-JiT论文-视频要点\"},{\"text\":\"251211-1650-TRM应用于VLA\",\"link\":\"/00-inbox/week-50/251211-1650-TRM应用于VLA\"},{\"text\":\"251211-1650-TRM应用大纲\",\"link\":\"/00-inbox/week-50/251211-1650-TRM应用大纲\"},{\"text\":\"251211-1650-TRM论文精读笔记\",\"link\":\"/00-inbox/week-50/251211-1650-TRM论文精读笔记\"},{\"text\":\"251211-1650-VAE原理\",\"link\":\"/00-inbox/week-50/251211-1650-VAE原理\"},{\"text\":\"251211-1650-deepseekV3_2\",\"link\":\"/00-inbox/week-50/251211-1650-deepseekV3_2\"},{\"text\":\"251211-1650-rustdesk安装及配置\",\"link\":\"/00-inbox/week-50/251211-1650-rustdesk安装及配置\"},{\"text\":\"251211-1650-图像生成基座模型调研\",\"link\":\"/00-inbox/week-50/251211-1650-图像生成基座模型调研\"},{\"text\":\"251211-1650-场景理解分类功能\",\"link\":\"/00-inbox/week-50/251211-1650-场景理解分类功能\"}]}]}],\"/10-knowledge/\":[{\"text\":\"Knowledge\",\"items\":[{\"text\":\"README\",\"link\":\"/10-knowledge/\"},{\"text\":\"250822-0737-重启训练一个半小时内不开始训-overview\",\"link\":\"/10-knowledge/250822-0737-重启训练一个半小时内不开始训-overview\"},{\"text\":\"250826-0737-OpenUni训练方法-overview\",\"link\":\"/10-knowledge/250826-0737-OpenUni训练方法-overview\"},{\"text\":\"250826-0737-i2i训练方案-overview\",\"link\":\"/10-knowledge/250826-0737-i2i训练方案-overview\"},{\"text\":\"250827-0736-token熵-overview\",\"link\":\"/10-knowledge/250827-0736-token熵-overview\"},{\"text\":\"250827-0736-扩散过程中打印数值-overview\",\"link\":\"/10-knowledge/250827-0736-扩散过程中打印数值-overview\"},{\"text\":\"250828-0735-fm_transformers模型参数-overview\",\"link\":\"/10-knowledge/250828-0735-fm_transformers模型参数-overview\"},{\"text\":\"250829-0735-困惑度-overview\",\"link\":\"/10-knowledge/250829-0735-困惑度-overview\"},{\"text\":\"250829-0735-惊讶度-overview\",\"link\":\"/10-knowledge/250829-0735-惊讶度-overview\"},{\"text\":\"250830-0735-pipeline检查-overview\",\"link\":\"/10-knowledge/250830-0735-pipeline检查-overview\"},{\"text\":\"250903-0735-macos快捷操作\",\"link\":\"/10-knowledge/250903-0735-macos快捷操作\"},{\"text\":\"250906-0735-VICReg监督\",\"link\":\"/10-knowledge/250906-0735-VICReg监督\"},{\"text\":\"250906-0735-投影层方案\",\"link\":\"/10-knowledge/250906-0735-投影层方案\"},{\"text\":\"250907-0735-特征解耦方案GPT参考\",\"link\":\"/10-knowledge/250907-0735-特征解耦方案GPT参考\"},{\"text\":\"example-knowledge\",\"link\":\"/10-knowledge/example-knowledge\"}]}],\"/20-papers/\":[{\"text\":\"Papers\",\"items\":[{\"text\":\"README\",\"link\":\"/20-papers/\"},{\"text\":\"250829-0735-Qwen2.5-Omni论文阅读\",\"link\":\"/20-papers/250829-0735-Qwen2.5-Omni论文阅读\"},{\"text\":\"2025\",\"collapsed\":true,\"items\":[{\"text\":\"2025-08-20-\",\"link\":\"/20-papers/2025/2025-08-20-\"},{\"text\":\"2025-08-20-vtla-preference-learning\",\"link\":\"/20-papers/2025/2025-08-20-vtla-preference-learning\"}]},{\"text\":\"vla\",\"collapsed\":true,\"items\":[{\"text\":\"all_papers\",\"link\":\"/20-papers/vla/all_papers\"}]}]}],\"/30-ideas/\":[{\"text\":\"Ideas\",\"items\":[{\"text\":\"README\",\"link\":\"/30-ideas/\"},{\"text\":\"2025\",\"collapsed\":true,\"items\":[{\"text\":\"9月\",\"collapsed\":true,\"items\":[{\"text\":\"250826-0737-与弘扬讨论-overview\",\"link\":\"/30-ideas/2025/9月/250826-0737-与弘扬讨论-overview\"},{\"text\":\"250826-0737-现有模型结构可改进点-overview\",\"link\":\"/30-ideas/2025/9月/250826-0737-现有模型结构可改进点-overview\"},{\"text\":\"250827-0736-0826-与弘扬沟通-overview\",\"link\":\"/30-ideas/2025/9月/250827-0736-0826-与弘扬沟通-overview\"},{\"text\":\"250829-0735-中间层模型方案梳理-overview\",\"link\":\"/30-ideas/2025/9月/250829-0735-中间层模型方案梳理-overview\"},{\"text\":\"250829-0735-论文理论包装方案\",\"link\":\"/30-ideas/2025/9月/250829-0735-论文理论包装方案\"},{\"text\":\"250904-0735-多轮对话拓展idea-overview\",\"link\":\"/30-ideas/2025/9月/250904-0735-多轮对话拓展idea-overview\"},{\"text\":\"250906-0735-0905_今日晨思\",\"link\":\"/30-ideas/2025/9月/250906-0735-0905_今日晨思\"},{\"text\":\"250907-0735-梦梦的建议\",\"link\":\"/30-ideas/2025/9月/250907-0735-梦梦的建议\"},{\"text\":\"week36-论文实验idea汇总\",\"link\":\"/30-ideas/2025/9月/week36-论文实验idea汇总\"}]}]}]}],\"/40-experiments/\":[{\"text\":\"Experiments\",\"items\":[{\"text\":\"README\",\"link\":\"/40-experiments/\"},{\"text\":\"250826-0736-0826测评日志\",\"link\":\"/40-experiments/250826-0736-0826测评日志\"},{\"text\":\"250902-0735-0901实验分析\",\"link\":\"/40-experiments/250902-0735-0901实验分析\"},{\"text\":\"250904-0735-exp_card_0903_transencodder\",\"link\":\"/40-experiments/250904-0735-exp_card_0903_transencodder\"},{\"text\":\"250904-0735-实验日志模板\",\"link\":\"/40-experiments/250904-0735-实验日志模板\"},{\"text\":\"exp_card_0908_dinov2\",\"link\":\"/40-experiments/exp_card_0908_dinov2\"}]}],\"/50-reports/\":[{\"text\":\"Reports\",\"items\":[{\"text\":\"README\",\"link\":\"/50-reports/\"},{\"text\":\"250820-0754-OCR调研\",\"link\":\"/50-reports/250820-0754-OCR调研\"},{\"text\":\"250827-0736-高熵强化学习\",\"link\":\"/50-reports/250827-0736-高熵强化学习\"},{\"text\":\"250901-0735-resume\",\"link\":\"/50-reports/250901-0735-resume\"},{\"text\":\"250903-0735-座舱VLA端云协同方案\",\"link\":\"/50-reports/250903-0735-座舱VLA端云协同方案\"},{\"text\":\"座舱VLA端云协同方案\",\"link\":\"/50-reports/座舱VLA端云协同方案\"},{\"text\":\"文本扩散模型调研\",\"link\":\"/50-reports/文本扩散模型调研\"},{\"text\":\"训练与部署资源申请书\",\"link\":\"/50-reports/训练与部署资源申请书\"},{\"text\":\"近期发布模型调研-0908\",\"link\":\"/50-reports/近期发布模型调研-0908\"},{\"text\":\"长视频理解综述\",\"link\":\"/50-reports/长视频理解综述\"},{\"text\":\"高熵RL - 数据构建增益\",\"link\":\"/50-reports/高熵RL - 数据构建增益\"},{\"text\":\"resume\",\"collapsed\":true,\"items\":[{\"text\":\"250901-0735-resume_network\",\"link\":\"/50-reports/resume/250901-0735-resume_network\"}]}]}]},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/wangyaqi/ZhiGrove\"}],\"search\":{\"provider\":\"local\"},\"footer\":{\"message\":\"Released under the MIT License.\",\"copyright\":\"Copyright © 2025 Wang Yaqi\"}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>