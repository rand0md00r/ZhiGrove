# idea 汇总 20205年9月7日 18:36

## 待实践方案：

### 分块稀疏Query + MoE 传输
- source： 30-ideas/[250829-0735]中间层模型方案梳理.md
- 方案描述：
    - 分块稀疏Query，将vl query作为前缀，输入到一个MoE llama里，连接到高斯空间；
    - 实验进展：未验证；

- 坑：分块重排方案可能导致特征混乱；


### 语义监督方案：
- source：30-ideas/[250827-0736]0826-与弘扬沟通.md

1. VICReg + dino v2
- 特点：无负样本的表示学习——用“不变性 + 方差约束 + 协方差去相关”避免坍塌、提升表征质量。
- deno v2

2. siglip
- 特点：把图文匹配当 独立二分类（BCE with logits）——“每一对都是一个 0/1 判断”。


3. 使用VAE latent做语义监督标签
- 为什么CrossFlow没有使用label做对比损失？
> 因为 VAE 的图像 latent 不具备良好的“语义几何”，而 CLIP 空间天生是语义对齐的。CrossFlow 需要把“文本分布 → 图像分布”的流匹配学好，就得先把文本侧编码进一个语义结构良好、与图像语义一致的空间来当源分布锚点；直接在 VAE 图像 latent 上做（按 label 的）对比/重构，语义信号弱，效果差。
    - 1. 重构≠语义：作者直接试过用 VAE 式的重构损失训练 Text-VE，重构误差虽然低，但“语义概念捕获得不好，生成效果变差”；改用对比损失（CLIP 风格）显著更好，其中文本-图像对比又略优于文本-文本对比。论文在 §4.4.1 与消融里明确给出这一点。
    - 2. 空间几何不匹配：图像 VAE latent（如 4×H×W）更多承载低层像素与风格信息，分布各向异性强、量纲复杂；把它拿来按类别/标签做对比，会把同类样本捏得过近、与实例级语义错位，还可能伤到生成质量。CLIP 空间则是单位归一化、接近各向同性、与自然语言实例级对齐的嵌入，做 InfoNCE/BCE 更稳定。
    - 3. 


## 其他方向idea（暂不实践）：
1. 超长视频理解，做时空建模记忆实现。   理解单个图片 -> 理解短视频 -> 理解流式视频 -> 持续action；
2. 后面我们还可以做多轮对话那种  存储metaquery的KV矩阵做矩阵 然后利用ODE的可逆性   这个可能也是后面能拓展的一点
3. 

## 验证不行的方案：

1. clip语义监督
- 特点：基于 InfoNCE 的行/列 softmax 对比学习——“从一堆候选里选对的那个”。
- 多模态输入无法处理（但是当前为纯文本输出，为什么也不行？）