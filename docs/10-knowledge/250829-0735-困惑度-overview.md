---
title: 困惑度
created: 2025-09-07 17:02
updated: 2025-09-07
origin: week-35
type: knowledge
status: draft
tags: [信息论, 困惑度, perplexity, 语言模型, 交叉熵]
links: []
---

## TL;DR（≤3点）
- 定义：**困惑度（PPL） = 指标分布的平均负对数似然的指数**；用自然对数时 $\mathrm{PPL}=e^{\bar{H}}$，用 $\log_2$ 时 $\mathrm{PPL}=2^{\bar{H}_{\text{bits}}}$。
- 直觉：**每一步等效面对的“等可能选项个数”**；越低越好，理想下界为 1。
- 区分：**数据级 PPL**（评测真实序列） vs **单步 PPL**（$e^{H_t}$，预测分布熵的指数化）。


## What（是什么）
- 对标注序列 $y_{1:T}$，模型给出条件概率 $p(y_t\mid y_{<t})$。  
  **平均负对数似然（交叉熵）**：$\displaystyle \bar{H}=-\frac{1}{T}\sum_{t=1}^T \log p(y_t\mid y_{<t})$  
  **困惑度**（自然对数）：$\displaystyle \boxed{\mathrm{PPL}=e^{\bar{H}}}$
- 等价式（几何平均逆概率）：
  $$
  \mathrm{PPL}
  =\exp\!\Big(-\tfrac{1}{T}\sum_{t}\log p(y_t\mid y_{<t})\Big)
  =\Big(\prod_{t=1}^{T}\tfrac{1}{p(y_t\mid y_{<t})}\Big)^{\!1/T}.
  $$
- **单步困惑度**：若 $p_t(\cdot)$ 是第 $t$ 步的预测分布，Shannon 熵 $H_t=-\sum_v p_t(v)\log p_t(v)$，则  
  $\displaystyle \mathrm{PPL}_t=\exp(H_t)$。


## Why（为什么这么做/何时使用）
- **可解释的尺度**：把“平均惊讶度”指数化为“等效分支数”，一眼看出模型不确定性。
- **训练/评测**：同一数据与分词口径下比较模型、做早停与退火监控、回归验证（越低越好）。
- **误差定位**：联动 token 级惊讶度/熵，定位高困惑度的片段与上下文盲点。


## How（最小复现配方，≤5步）
1. 在评测集上 **教师强制** 前向，拿到每个真标签的 $\log p(y_t\mid y_{<t})$（注意屏蔽 PAD）。
2. 求 **总 NLL**：$L=-\sum_t \log p(\cdot)$（建议以 **nat** 聚合，数值更稳）。
3. 求 **有效 token 数** $T_{\text{valid}}$（排除 PAD/BOS 可选/只保留需要计分的 token）。
4. 得平均交叉熵：$\bar{H}=L/T_{\text{valid}}$。
5. 取指数：$\mathrm{PPL}=e^{\bar{H}}$（若步骤 2 用 $\log_2$，则 $\mathrm{PPL}=2^{\bar{H}}$）。


## Gotchas（坑点与边界）
- **可比性**：必须 **同数据集、同切分、同分词/词表、同计分口径**（是否含 BOS/EOS、是否区分大小写等）下比较。
- **样本口径**：**数据级 PPL** 基于真实序列的 NLL；**单步 PPL** 来自预测分布的熵。它们相关但不等价。
- **与主观质量**：PPL 低 ≠ 一定更符合人类偏好；解码策略（温度、top-p/k、约束）会影响可读性但不改变评测 PPL 的定义。
- **数值稳定**：极小概率导致 NLL 爆大；用稳定的 **log-softmax / log-sum-exp**，对 $p=0$ 的标签需平滑/剪裁。
- **长度与填充**：必须按 **有效 token** 归一；不要对含大量 PAD 的样本直接取均值。
- 

---


## Raw Notes

好的！用最直白的话来解释**困惑度（Perplexity, PPL）**：

## 它在量什么？

困惑度就是在问：**模型在每一步平均要面对“多少个等可能的选择”**。

* 如果模型很确定，几乎总押中正确词，**困惑度小**（接近 1）。
* 如果模型很拿不准，像在多个选项里瞎猜，**困惑度大**。

## 和公式怎么对应？

对一段真实标注的序列 $y_{1:T}$，语言模型给出的条件概率是 $p(y_t\mid y_{<t})$。

* **平均负对数似然（交叉熵）**：$\displaystyle \bar{H}=-\frac{1}{T}\sum_{t=1}^T \log p(y_t\mid y_{<t})$
* **困惑度**：$\displaystyle \mathrm{PPL}=\exp(\bar{H})$（自然对数）。
  用二进制对数时：$\mathrm{PPL}=2^{\bar{H}_{\text{bits}}}$。

直觉：$\bar{H}$ 是“平均惊讶度”，**把惊讶度指数化**就得到“等效分支数”=困惑度。

## 一眼懂的例子

* 如果模型每步给真实词的概率都是 **0.25**：
  $-\log 0.25=\ln 4$，$\mathrm{PPL}=e^{\ln 4}=4$。
  含义：像在 **4 选 1** 中选择。
* 如果每步概率 **0.1**：$\mathrm{PPL}=10$（像 10 选 1）。
* 如果每步都接近 **1.0**：$\mathrm{PPL}\to 1$。

## 与“token 熵”的关系

* **单步困惑度**：$\mathrm{PPL}_t=\exp(H_t)$，其中 $H_t=-\sum_v p_t(v)\log p_t(v)$ 是**该步分布的熵**（不确定性）。
* **数据集困惑度**：对真实序列的**平均 NLL**做指数；和上面的“分布熵”不是同一件事，但两者都刻画“不确定性”。

## 使用要点与注意

* **越低越好**：训练/评测常用“越低越好”的标尺。
* **可比性条件**：必须在**同一数据集、同一切分、同一分词/词表**下比较；否则没意义。
* **不等于人类偏好**：PPL 低不一定更“有用”或更“好读”；格式约束、解码策略（温度/Top-p）等也影响主观质量。
* **单位差异**：用自然对数（nat）或 $\log_2$（bit）都会被最终的 $\exp$ 或 $2^{(\cdot)}$ 抹平，记住你用的是哪种即可。

一句话总结：
**困惑度 = 模型在每一步“等效面对的选择个数”**。它是$\exp($平均负对数似然$)$，数字越小，说明模型越不“困惑”。
